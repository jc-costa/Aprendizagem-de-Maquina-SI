{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar o entendimento do banco de dados, exclui-se a coluna 13 que não é necessária e renomeia-se as colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Dataset_spine.csv\")\r\n",
    "data = data.drop(['Unnamed: 13'], axis=1)\r\n",
    "data.rename(columns = {\r\n",
    "    \"Col1\" : \"pelvic_incidence\", \r\n",
    "    \"Col2\" : \"pelvic_tilt\",\r\n",
    "    \"Col3\" : \"lumbar_lordosis_angle\",\r\n",
    "    \"Col4\" : \"sacral_slope\", \r\n",
    "    \"Col5\" : \"pelvic_radius\",\r\n",
    "    \"Col6\" : \"degree_spondylolisthesis\", \r\n",
    "    \"Col7\" : \"pelvic_slope\",\r\n",
    "    \"Col8\" : \"direct_tilt\",\r\n",
    "    \"Col9\" : \"thoracic_slope\", \r\n",
    "    \"Col10\" :\"cervical_tilt\", \r\n",
    "    \"Col11\" : \"sacrum_angle\",\r\n",
    "    \"Col12\" : \"scoliosis_slope\", \r\n",
    "    \"Class_att\" : \"class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "0           63.027817    22.552586              39.609117     40.475232   \n",
       "1           39.056951    10.060991              25.015378     28.995960   \n",
       "2           68.832021    22.218482              50.092194     46.613539   \n",
       "3           69.297008    24.652878              44.311238     44.644130   \n",
       "4           49.712859     9.652075              28.317406     40.060784   \n",
       "..                ...          ...                    ...           ...   \n",
       "305         47.903565    13.616688              36.000000     34.286877   \n",
       "306         53.936748    20.721496              29.220534     33.215251   \n",
       "307         61.446597    22.694968              46.170347     38.751628   \n",
       "308         45.252792     8.693157              41.583126     36.559635   \n",
       "309         33.841641     5.073991              36.641233     28.767649   \n",
       "\n",
       "     pelvic_radius  degree_spondylolisthesis  pelvic_slope  direct_tilt  \\\n",
       "0        98.672917                 -0.254400      0.744503      12.5661   \n",
       "1       114.405425                  4.564259      0.415186      12.8874   \n",
       "2       105.985135                 -3.530317      0.474889      26.8343   \n",
       "3       101.868495                 11.211523      0.369345      23.5603   \n",
       "4       108.168725                  7.918501      0.543360      35.4940   \n",
       "..             ...                       ...           ...          ...   \n",
       "305     117.449062                 -4.245395      0.129744       7.8433   \n",
       "306     114.365845                 -0.421010      0.047913      19.1986   \n",
       "307     125.670725                 -2.707880      0.081070      16.2059   \n",
       "308     118.545842                  0.214750      0.159251      14.7334   \n",
       "309     123.945244                 -0.199249      0.674504      19.3825   \n",
       "\n",
       "     thoracic_slope  cervical_tilt  sacrum_angle  scoliosis_slope     class  \n",
       "0           14.5386       15.30468    -28.658501          43.5123  Abnormal  \n",
       "1           17.5323       16.78486    -25.530607          16.1102  Abnormal  \n",
       "2           17.4861       16.65897    -29.031888          19.2221  Abnormal  \n",
       "3           12.7074       11.42447    -30.470246          18.8329  Abnormal  \n",
       "4           15.9546        8.87237    -16.378376          24.9171  Abnormal  \n",
       "..              ...            ...           ...              ...       ...  \n",
       "305         14.7484        8.51707    -15.728927          11.5472    Normal  \n",
       "306         18.1972        7.08745      6.013843          43.8693    Normal  \n",
       "307         13.5565        8.89572      3.564463          18.4151    Normal  \n",
       "308         16.0928        9.75922      5.767308          33.7192    Normal  \n",
       "309         17.6963       13.72929      1.783007          40.6049    Normal  \n",
       "\n",
       "[310 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pelvic_incidence</th>\n      <th>pelvic_tilt</th>\n      <th>lumbar_lordosis_angle</th>\n      <th>sacral_slope</th>\n      <th>pelvic_radius</th>\n      <th>degree_spondylolisthesis</th>\n      <th>pelvic_slope</th>\n      <th>direct_tilt</th>\n      <th>thoracic_slope</th>\n      <th>cervical_tilt</th>\n      <th>sacrum_angle</th>\n      <th>scoliosis_slope</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63.027817</td>\n      <td>22.552586</td>\n      <td>39.609117</td>\n      <td>40.475232</td>\n      <td>98.672917</td>\n      <td>-0.254400</td>\n      <td>0.744503</td>\n      <td>12.5661</td>\n      <td>14.5386</td>\n      <td>15.30468</td>\n      <td>-28.658501</td>\n      <td>43.5123</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39.056951</td>\n      <td>10.060991</td>\n      <td>25.015378</td>\n      <td>28.995960</td>\n      <td>114.405425</td>\n      <td>4.564259</td>\n      <td>0.415186</td>\n      <td>12.8874</td>\n      <td>17.5323</td>\n      <td>16.78486</td>\n      <td>-25.530607</td>\n      <td>16.1102</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>68.832021</td>\n      <td>22.218482</td>\n      <td>50.092194</td>\n      <td>46.613539</td>\n      <td>105.985135</td>\n      <td>-3.530317</td>\n      <td>0.474889</td>\n      <td>26.8343</td>\n      <td>17.4861</td>\n      <td>16.65897</td>\n      <td>-29.031888</td>\n      <td>19.2221</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>69.297008</td>\n      <td>24.652878</td>\n      <td>44.311238</td>\n      <td>44.644130</td>\n      <td>101.868495</td>\n      <td>11.211523</td>\n      <td>0.369345</td>\n      <td>23.5603</td>\n      <td>12.7074</td>\n      <td>11.42447</td>\n      <td>-30.470246</td>\n      <td>18.8329</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49.712859</td>\n      <td>9.652075</td>\n      <td>28.317406</td>\n      <td>40.060784</td>\n      <td>108.168725</td>\n      <td>7.918501</td>\n      <td>0.543360</td>\n      <td>35.4940</td>\n      <td>15.9546</td>\n      <td>8.87237</td>\n      <td>-16.378376</td>\n      <td>24.9171</td>\n      <td>Abnormal</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>47.903565</td>\n      <td>13.616688</td>\n      <td>36.000000</td>\n      <td>34.286877</td>\n      <td>117.449062</td>\n      <td>-4.245395</td>\n      <td>0.129744</td>\n      <td>7.8433</td>\n      <td>14.7484</td>\n      <td>8.51707</td>\n      <td>-15.728927</td>\n      <td>11.5472</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>306</th>\n      <td>53.936748</td>\n      <td>20.721496</td>\n      <td>29.220534</td>\n      <td>33.215251</td>\n      <td>114.365845</td>\n      <td>-0.421010</td>\n      <td>0.047913</td>\n      <td>19.1986</td>\n      <td>18.1972</td>\n      <td>7.08745</td>\n      <td>6.013843</td>\n      <td>43.8693</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>307</th>\n      <td>61.446597</td>\n      <td>22.694968</td>\n      <td>46.170347</td>\n      <td>38.751628</td>\n      <td>125.670725</td>\n      <td>-2.707880</td>\n      <td>0.081070</td>\n      <td>16.2059</td>\n      <td>13.5565</td>\n      <td>8.89572</td>\n      <td>3.564463</td>\n      <td>18.4151</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>308</th>\n      <td>45.252792</td>\n      <td>8.693157</td>\n      <td>41.583126</td>\n      <td>36.559635</td>\n      <td>118.545842</td>\n      <td>0.214750</td>\n      <td>0.159251</td>\n      <td>14.7334</td>\n      <td>16.0928</td>\n      <td>9.75922</td>\n      <td>5.767308</td>\n      <td>33.7192</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>33.841641</td>\n      <td>5.073991</td>\n      <td>36.641233</td>\n      <td>28.767649</td>\n      <td>123.945244</td>\n      <td>-0.199249</td>\n      <td>0.674504</td>\n      <td>19.3825</td>\n      <td>17.6963</td>\n      <td>13.72929</td>\n      <td>1.783007</td>\n      <td>40.6049</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>310 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "y = data['class']\r\n",
    "x = data.drop(['class'], axis = 1)\r\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se apenas 20% dos dados do banco de dados para usar como treinamento e o restante será alocado para uso no teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se três camadas de unidades de processamento intermediário, a função sigmoidal como função de ativação e 500 como númrero máximo de interações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.64699710\n",
      "Iteration 2, loss = 0.64178186\n",
      "Iteration 3, loss = 0.63778467\n",
      "Iteration 4, loss = 0.63550049\n",
      "Iteration 5, loss = 0.63354912\n",
      "Iteration 6, loss = 0.63184340\n",
      "Iteration 7, loss = 0.63065593\n",
      "Iteration 8, loss = 0.62907951\n",
      "Iteration 9, loss = 0.62756442\n",
      "Iteration 10, loss = 0.62573880\n",
      "Iteration 11, loss = 0.62380375\n",
      "Iteration 12, loss = 0.62154187\n",
      "Iteration 13, loss = 0.61931788\n",
      "Iteration 14, loss = 0.61665258\n",
      "Iteration 15, loss = 0.61375022\n",
      "Iteration 16, loss = 0.61085673\n",
      "Iteration 17, loss = 0.60773244\n",
      "Iteration 18, loss = 0.60399598\n",
      "Iteration 19, loss = 0.59951720\n",
      "Iteration 20, loss = 0.59434223\n",
      "Iteration 21, loss = 0.58813329\n",
      "Iteration 22, loss = 0.58217381\n",
      "Iteration 23, loss = 0.57602792\n",
      "Iteration 24, loss = 0.56878620\n",
      "Iteration 25, loss = 0.56105643\n",
      "Iteration 26, loss = 0.55279656\n",
      "Iteration 27, loss = 0.54479204\n",
      "Iteration 28, loss = 0.53567061\n",
      "Iteration 29, loss = 0.52626177\n",
      "Iteration 30, loss = 0.51759395\n",
      "Iteration 31, loss = 0.50849511\n",
      "Iteration 32, loss = 0.49877042\n",
      "Iteration 33, loss = 0.49079236\n",
      "Iteration 34, loss = 0.48267256\n",
      "Iteration 35, loss = 0.47408682\n",
      "Iteration 36, loss = 0.46559063\n",
      "Iteration 37, loss = 0.45766057\n",
      "Iteration 38, loss = 0.44948514\n",
      "Iteration 39, loss = 0.44238572\n",
      "Iteration 40, loss = 0.43702578\n",
      "Iteration 41, loss = 0.43196796\n",
      "Iteration 42, loss = 0.42478124\n",
      "Iteration 43, loss = 0.41859180\n",
      "Iteration 44, loss = 0.41363341\n",
      "Iteration 45, loss = 0.40927624\n",
      "Iteration 46, loss = 0.40405530\n",
      "Iteration 47, loss = 0.40019234\n",
      "Iteration 48, loss = 0.39594084\n",
      "Iteration 49, loss = 0.39179582\n",
      "Iteration 50, loss = 0.38802776\n",
      "Iteration 51, loss = 0.38437351\n",
      "Iteration 52, loss = 0.38193176\n",
      "Iteration 53, loss = 0.37792769\n",
      "Iteration 54, loss = 0.37452595\n",
      "Iteration 55, loss = 0.37279817\n",
      "Iteration 56, loss = 0.37127401\n",
      "Iteration 57, loss = 0.36826991\n",
      "Iteration 58, loss = 0.36419292\n",
      "Iteration 59, loss = 0.36115357\n",
      "Iteration 60, loss = 0.35782583\n",
      "Iteration 61, loss = 0.35545849\n",
      "Iteration 62, loss = 0.35294844\n",
      "Iteration 63, loss = 0.35107136\n",
      "Iteration 64, loss = 0.34712682\n",
      "Iteration 65, loss = 0.34758777\n",
      "Iteration 66, loss = 0.34537064\n",
      "Iteration 67, loss = 0.33999710\n",
      "Iteration 68, loss = 0.33879099\n",
      "Iteration 69, loss = 0.33818018\n",
      "Iteration 70, loss = 0.33354463\n",
      "Iteration 71, loss = 0.33292385\n",
      "Iteration 72, loss = 0.33524424\n",
      "Iteration 73, loss = 0.32799442\n",
      "Iteration 74, loss = 0.32757823\n",
      "Iteration 75, loss = 0.32611956\n",
      "Iteration 76, loss = 0.32116031\n",
      "Iteration 77, loss = 0.32275155\n",
      "Iteration 78, loss = 0.32055675\n",
      "Iteration 79, loss = 0.31558219\n",
      "Iteration 80, loss = 0.31675372\n",
      "Iteration 81, loss = 0.31376971\n",
      "Iteration 82, loss = 0.30847143\n",
      "Iteration 83, loss = 0.31753825\n",
      "Iteration 84, loss = 0.31160867\n",
      "Iteration 85, loss = 0.30583011\n",
      "Iteration 86, loss = 0.30666358\n",
      "Iteration 87, loss = 0.30237626\n",
      "Iteration 88, loss = 0.29814873\n",
      "Iteration 89, loss = 0.30514419\n",
      "Iteration 90, loss = 0.30775100\n",
      "Iteration 91, loss = 0.29693683\n",
      "Iteration 92, loss = 0.29533361\n",
      "Iteration 93, loss = 0.29731344\n",
      "Iteration 94, loss = 0.29436712\n",
      "Iteration 95, loss = 0.28953060\n",
      "Iteration 96, loss = 0.28868308\n",
      "Iteration 97, loss = 0.29034118\n",
      "Iteration 98, loss = 0.28581745\n",
      "Iteration 99, loss = 0.28587602\n",
      "Iteration 100, loss = 0.28475216\n",
      "Iteration 101, loss = 0.28053796\n",
      "Iteration 102, loss = 0.28372154\n",
      "Iteration 103, loss = 0.27829373\n",
      "Iteration 104, loss = 0.27642433\n",
      "Iteration 105, loss = 0.29213007\n",
      "Iteration 106, loss = 0.30104348\n",
      "Iteration 107, loss = 0.28616454\n",
      "Iteration 108, loss = 0.27125432\n",
      "Iteration 109, loss = 0.28334280\n",
      "Iteration 110, loss = 0.28364845\n",
      "Iteration 111, loss = 0.27234517\n",
      "Iteration 112, loss = 0.26892194\n",
      "Iteration 113, loss = 0.26995750\n",
      "Iteration 114, loss = 0.26709117\n",
      "Iteration 115, loss = 0.26539239\n",
      "Iteration 116, loss = 0.26358374\n",
      "Iteration 117, loss = 0.26256107\n",
      "Iteration 118, loss = 0.26122995\n",
      "Iteration 119, loss = 0.26086074\n",
      "Iteration 120, loss = 0.25934023\n",
      "Iteration 121, loss = 0.25764393\n",
      "Iteration 122, loss = 0.25657370\n",
      "Iteration 123, loss = 0.25514046\n",
      "Iteration 124, loss = 0.25532976\n",
      "Iteration 125, loss = 0.25655989\n",
      "Iteration 126, loss = 0.25523253\n",
      "Iteration 127, loss = 0.25469523\n",
      "Iteration 128, loss = 0.25245739\n",
      "Iteration 129, loss = 0.25016205\n",
      "Iteration 130, loss = 0.24852917\n",
      "Iteration 131, loss = 0.24710990\n",
      "Iteration 132, loss = 0.24683131\n",
      "Iteration 133, loss = 0.24545871\n",
      "Iteration 134, loss = 0.24597220\n",
      "Iteration 135, loss = 0.24481001\n",
      "Iteration 136, loss = 0.24199649\n",
      "Iteration 137, loss = 0.24658371\n",
      "Iteration 138, loss = 0.24507141\n",
      "Iteration 139, loss = 0.23868282\n",
      "Iteration 140, loss = 0.24355670\n",
      "Iteration 141, loss = 0.23880273\n",
      "Iteration 142, loss = 0.23985506\n",
      "Iteration 143, loss = 0.24050698\n",
      "Iteration 144, loss = 0.23648598\n",
      "Iteration 145, loss = 0.23704775\n",
      "Iteration 146, loss = 0.23800253\n",
      "Iteration 147, loss = 0.23536641\n",
      "Iteration 148, loss = 0.23180790\n",
      "Iteration 149, loss = 0.23648561\n",
      "Iteration 150, loss = 0.24047690\n",
      "Iteration 151, loss = 0.22887455\n",
      "Iteration 152, loss = 0.24021424\n",
      "Iteration 153, loss = 0.23720978\n",
      "Iteration 154, loss = 0.22815818\n",
      "Iteration 155, loss = 0.23304232\n",
      "Iteration 156, loss = 0.22954892\n",
      "Iteration 157, loss = 0.22470505\n",
      "Iteration 158, loss = 0.22595796\n",
      "Iteration 159, loss = 0.22557058\n",
      "Iteration 160, loss = 0.22249979\n",
      "Iteration 161, loss = 0.22291141\n",
      "Iteration 162, loss = 0.22520359\n",
      "Iteration 163, loss = 0.22152053\n",
      "Iteration 164, loss = 0.22240555\n",
      "Iteration 165, loss = 0.21918356\n",
      "Iteration 166, loss = 0.22021325\n",
      "Iteration 167, loss = 0.21950229\n",
      "Iteration 168, loss = 0.21715877\n",
      "Iteration 169, loss = 0.21568332\n",
      "Iteration 170, loss = 0.21483070\n",
      "Iteration 171, loss = 0.21313074\n",
      "Iteration 172, loss = 0.21334271\n",
      "Iteration 173, loss = 0.21282682\n",
      "Iteration 174, loss = 0.21122780\n",
      "Iteration 175, loss = 0.21240677\n",
      "Iteration 176, loss = 0.20962332\n",
      "Iteration 177, loss = 0.21144870\n",
      "Iteration 178, loss = 0.21173123\n",
      "Iteration 179, loss = 0.20787954\n",
      "Iteration 180, loss = 0.21017570\n",
      "Iteration 181, loss = 0.21139904\n",
      "Iteration 182, loss = 0.20627391\n",
      "Iteration 183, loss = 0.20537782\n",
      "Iteration 184, loss = 0.20610676\n",
      "Iteration 185, loss = 0.20591421\n",
      "Iteration 186, loss = 0.20466707\n",
      "Iteration 187, loss = 0.20236081\n",
      "Iteration 188, loss = 0.20680598\n",
      "Iteration 189, loss = 0.20342767\n",
      "Iteration 190, loss = 0.20057767\n",
      "Iteration 191, loss = 0.20604367\n",
      "Iteration 192, loss = 0.20244070\n",
      "Iteration 193, loss = 0.20009981\n",
      "Iteration 194, loss = 0.19934504\n",
      "Iteration 195, loss = 0.19807844\n",
      "Iteration 196, loss = 0.20217305\n",
      "Iteration 197, loss = 0.19853354\n",
      "Iteration 198, loss = 0.19498187\n",
      "Iteration 199, loss = 0.19485464\n",
      "Iteration 200, loss = 0.19258989\n",
      "Iteration 201, loss = 0.19211848\n",
      "Iteration 202, loss = 0.19191986\n",
      "Iteration 203, loss = 0.18961858\n",
      "Iteration 204, loss = 0.18879221\n",
      "Iteration 205, loss = 0.19008050\n",
      "Iteration 206, loss = 0.19045162\n",
      "Iteration 207, loss = 0.18848505\n",
      "Iteration 208, loss = 0.18723074\n",
      "Iteration 209, loss = 0.18756243\n",
      "Iteration 210, loss = 0.18580086\n",
      "Iteration 211, loss = 0.18382652\n",
      "Iteration 212, loss = 0.18435661\n",
      "Iteration 213, loss = 0.18323068\n",
      "Iteration 214, loss = 0.18523530\n",
      "Iteration 215, loss = 0.18439568\n",
      "Iteration 216, loss = 0.17964116\n",
      "Iteration 217, loss = 0.18038314\n",
      "Iteration 218, loss = 0.18005689\n",
      "Iteration 219, loss = 0.18099398\n",
      "Iteration 220, loss = 0.18146546\n",
      "Iteration 221, loss = 0.17500818\n",
      "Iteration 222, loss = 0.18571160\n",
      "Iteration 223, loss = 0.17440993\n",
      "Iteration 224, loss = 0.19064869\n",
      "Iteration 225, loss = 0.19045544\n",
      "Iteration 226, loss = 0.16864188\n",
      "Iteration 227, loss = 0.18123044\n",
      "Iteration 228, loss = 0.18366494\n",
      "Iteration 229, loss = 0.16640622\n",
      "Iteration 230, loss = 0.18348031\n",
      "Iteration 231, loss = 0.17864181\n",
      "Iteration 232, loss = 0.17021039\n",
      "Iteration 233, loss = 0.17562974\n",
      "Iteration 234, loss = 0.16718184\n",
      "Iteration 235, loss = 0.16353503\n",
      "Iteration 236, loss = 0.16694619\n",
      "Iteration 237, loss = 0.16425773\n",
      "Iteration 238, loss = 0.16319738\n",
      "Iteration 239, loss = 0.16794706\n",
      "Iteration 240, loss = 0.15798503\n",
      "Iteration 241, loss = 0.16973839\n",
      "Iteration 242, loss = 0.16959850\n",
      "Iteration 243, loss = 0.15752916\n",
      "Iteration 244, loss = 0.16086779\n",
      "Iteration 245, loss = 0.15711143\n",
      "Iteration 246, loss = 0.15604800\n",
      "Iteration 247, loss = 0.15551996\n",
      "Iteration 248, loss = 0.15580059\n",
      "Iteration 249, loss = 0.15832773\n",
      "Iteration 250, loss = 0.15190829\n",
      "Iteration 251, loss = 0.15660973\n",
      "Iteration 252, loss = 0.15003604\n",
      "Iteration 253, loss = 0.15627413\n",
      "Iteration 254, loss = 0.15652650\n",
      "Iteration 255, loss = 0.14804023\n",
      "Iteration 256, loss = 0.15841156\n",
      "Iteration 257, loss = 0.15297689\n",
      "Iteration 258, loss = 0.14867061\n",
      "Iteration 259, loss = 0.14629635\n",
      "Iteration 260, loss = 0.14386924\n",
      "Iteration 261, loss = 0.15117592\n",
      "Iteration 262, loss = 0.14457942\n",
      "Iteration 263, loss = 0.14683136\n",
      "Iteration 264, loss = 0.14257825\n",
      "Iteration 265, loss = 0.14387036\n",
      "Iteration 266, loss = 0.13974868\n",
      "Iteration 267, loss = 0.14103207\n",
      "Iteration 268, loss = 0.13941237\n",
      "Iteration 269, loss = 0.13590702\n",
      "Iteration 270, loss = 0.13695744\n",
      "Iteration 271, loss = 0.13565917\n",
      "Iteration 272, loss = 0.13861749\n",
      "Iteration 273, loss = 0.14504516\n",
      "Iteration 274, loss = 0.13154954\n",
      "Iteration 275, loss = 0.13733622\n",
      "Iteration 276, loss = 0.13394156\n",
      "Iteration 277, loss = 0.13053415\n",
      "Iteration 278, loss = 0.13510182\n",
      "Iteration 279, loss = 0.12887672\n",
      "Iteration 280, loss = 0.13407253\n",
      "Iteration 281, loss = 0.13502057\n",
      "Iteration 282, loss = 0.12815939\n",
      "Iteration 283, loss = 0.13638540\n",
      "Iteration 284, loss = 0.13289892\n",
      "Iteration 285, loss = 0.12531056\n",
      "Iteration 286, loss = 0.12272604\n",
      "Iteration 287, loss = 0.12484573\n",
      "Iteration 288, loss = 0.12394866\n",
      "Iteration 289, loss = 0.12438322\n",
      "Iteration 290, loss = 0.12387418\n",
      "Iteration 291, loss = 0.11813298\n",
      "Iteration 292, loss = 0.13341335\n",
      "Iteration 293, loss = 0.11914976\n",
      "Iteration 294, loss = 0.14425183\n",
      "Iteration 295, loss = 0.13319335\n",
      "Iteration 296, loss = 0.12247643\n",
      "Iteration 297, loss = 0.12636708\n",
      "Iteration 298, loss = 0.11768980\n",
      "Iteration 299, loss = 0.12312790\n",
      "Iteration 300, loss = 0.11991252\n",
      "Iteration 301, loss = 0.11457659\n",
      "Iteration 302, loss = 0.11350479\n",
      "Iteration 303, loss = 0.11144785\n",
      "Iteration 304, loss = 0.11355095\n",
      "Iteration 305, loss = 0.10910518\n",
      "Iteration 306, loss = 0.11382184\n",
      "Iteration 307, loss = 0.11247936\n",
      "Iteration 308, loss = 0.10945363\n",
      "Iteration 309, loss = 0.10745615\n",
      "Iteration 310, loss = 0.10795681\n",
      "Iteration 311, loss = 0.11244078\n",
      "Iteration 312, loss = 0.10720240\n",
      "Iteration 313, loss = 0.10911746\n",
      "Iteration 314, loss = 0.10454195\n",
      "Iteration 315, loss = 0.10926159\n",
      "Iteration 316, loss = 0.10573830\n",
      "Iteration 317, loss = 0.10086552\n",
      "Iteration 318, loss = 0.10924007\n",
      "Iteration 319, loss = 0.10921906\n",
      "Iteration 320, loss = 0.10268517\n",
      "Iteration 321, loss = 0.10489236\n",
      "Iteration 322, loss = 0.10112293\n",
      "Iteration 323, loss = 0.10745991\n",
      "Iteration 324, loss = 0.09944738\n",
      "Iteration 325, loss = 0.09800171\n",
      "Iteration 326, loss = 0.10255993\n",
      "Iteration 327, loss = 0.09782709\n",
      "Iteration 328, loss = 0.09575220\n",
      "Iteration 329, loss = 0.09378833\n",
      "Iteration 330, loss = 0.09272051\n",
      "Iteration 331, loss = 0.09650776\n",
      "Iteration 332, loss = 0.09284100\n",
      "Iteration 333, loss = 0.09399250\n",
      "Iteration 334, loss = 0.09429499\n",
      "Iteration 335, loss = 0.09240091\n",
      "Iteration 336, loss = 0.09668091\n",
      "Iteration 337, loss = 0.08674319\n",
      "Iteration 338, loss = 0.09945796\n",
      "Iteration 339, loss = 0.09324660\n",
      "Iteration 340, loss = 0.09594256\n",
      "Iteration 341, loss = 0.08956353\n",
      "Iteration 342, loss = 0.09154707\n",
      "Iteration 343, loss = 0.08629259\n",
      "Iteration 344, loss = 0.09435156\n",
      "Iteration 345, loss = 0.08863627\n",
      "Iteration 346, loss = 0.08794433\n",
      "Iteration 347, loss = 0.08730308\n",
      "Iteration 348, loss = 0.08544815\n",
      "Iteration 349, loss = 0.08965662\n",
      "Iteration 350, loss = 0.08314598\n",
      "Iteration 351, loss = 0.08147440\n",
      "Iteration 352, loss = 0.08135818\n",
      "Iteration 353, loss = 0.07885236\n",
      "Iteration 354, loss = 0.08199388\n",
      "Iteration 355, loss = 0.07786614\n",
      "Iteration 356, loss = 0.07886114\n",
      "Iteration 357, loss = 0.07927462\n",
      "Iteration 358, loss = 0.07548229\n",
      "Iteration 359, loss = 0.07753978\n",
      "Iteration 360, loss = 0.07911912\n",
      "Iteration 361, loss = 0.07623487\n",
      "Iteration 362, loss = 0.07496146\n",
      "Iteration 363, loss = 0.07635487\n",
      "Iteration 364, loss = 0.07231954\n",
      "Iteration 365, loss = 0.07457238\n",
      "Iteration 366, loss = 0.07150206\n",
      "Iteration 367, loss = 0.06906306\n",
      "Iteration 368, loss = 0.06886438\n",
      "Iteration 369, loss = 0.06599132\n",
      "Iteration 370, loss = 0.06507397\n",
      "Iteration 371, loss = 0.06359288\n",
      "Iteration 372, loss = 0.06216428\n",
      "Iteration 373, loss = 0.05925433\n",
      "Iteration 374, loss = 0.05993531\n",
      "Iteration 375, loss = 0.05674369\n",
      "Iteration 376, loss = 0.05470137\n",
      "Iteration 377, loss = 0.05656481\n",
      "Iteration 378, loss = 0.05466904\n",
      "Iteration 379, loss = 0.05312790\n",
      "Iteration 380, loss = 0.05004969\n",
      "Iteration 381, loss = 0.04894112\n",
      "Iteration 382, loss = 0.04744171\n",
      "Iteration 383, loss = 0.05024155\n",
      "Iteration 384, loss = 0.04918042\n",
      "Iteration 385, loss = 0.05453765\n",
      "Iteration 386, loss = 0.04775989\n",
      "Iteration 387, loss = 0.04574789\n",
      "Iteration 388, loss = 0.04430461\n",
      "Iteration 389, loss = 0.04461270\n",
      "Iteration 390, loss = 0.04553927\n",
      "Iteration 391, loss = 0.04518454\n",
      "Iteration 392, loss = 0.04283137\n",
      "Iteration 393, loss = 0.04123926\n",
      "Iteration 394, loss = 0.04386138\n",
      "Iteration 395, loss = 0.04337777\n",
      "Iteration 396, loss = 0.04184605\n",
      "Iteration 397, loss = 0.04448526\n",
      "Iteration 398, loss = 0.04127166\n",
      "Iteration 399, loss = 0.04253991\n",
      "Iteration 400, loss = 0.03863423\n",
      "Iteration 401, loss = 0.04955451\n",
      "Iteration 402, loss = 0.04025154\n",
      "Iteration 403, loss = 0.04333366\n",
      "Iteration 404, loss = 0.03981201\n",
      "Iteration 405, loss = 0.04188418\n",
      "Iteration 406, loss = 0.04149262\n",
      "Iteration 407, loss = 0.03871291\n",
      "Iteration 408, loss = 0.03945455\n",
      "Iteration 409, loss = 0.03809277\n",
      "Iteration 410, loss = 0.03436259\n",
      "Iteration 411, loss = 0.03970317\n",
      "Iteration 412, loss = 0.03430528\n",
      "Iteration 413, loss = 0.04122940\n",
      "Iteration 414, loss = 0.03534095\n",
      "Iteration 415, loss = 0.03502297\n",
      "Iteration 416, loss = 0.03794251\n",
      "Iteration 417, loss = 0.03255763\n",
      "Iteration 418, loss = 0.04155914\n",
      "Iteration 419, loss = 0.03296443\n",
      "Iteration 420, loss = 0.04151718\n",
      "Iteration 421, loss = 0.03229186\n",
      "Iteration 422, loss = 0.03987337\n",
      "Iteration 423, loss = 0.03759786\n",
      "Iteration 424, loss = 0.03082727\n",
      "Iteration 425, loss = 0.03254594\n",
      "Iteration 426, loss = 0.03091517\n",
      "Iteration 427, loss = 0.03210545\n",
      "Iteration 428, loss = 0.03034621\n",
      "Iteration 429, loss = 0.02808593\n",
      "Iteration 430, loss = 0.02820793\n",
      "Iteration 431, loss = 0.02898768\n",
      "Iteration 432, loss = 0.02930857\n",
      "Iteration 433, loss = 0.02757458\n",
      "Iteration 434, loss = 0.02809331\n",
      "Iteration 435, loss = 0.02748104\n",
      "Iteration 436, loss = 0.02733111\n",
      "Iteration 437, loss = 0.02647267\n",
      "Iteration 438, loss = 0.02551782\n",
      "Iteration 439, loss = 0.02504165\n",
      "Iteration 440, loss = 0.02485856\n",
      "Iteration 441, loss = 0.02439569\n",
      "Iteration 442, loss = 0.02423673\n",
      "Iteration 443, loss = 0.02427086\n",
      "Iteration 444, loss = 0.02408347\n",
      "Iteration 445, loss = 0.02392533\n",
      "Iteration 446, loss = 0.02372512\n",
      "Iteration 447, loss = 0.02419925\n",
      "Iteration 448, loss = 0.02320247\n",
      "Iteration 449, loss = 0.02453380\n",
      "Iteration 450, loss = 0.02262293\n",
      "Iteration 451, loss = 0.02409020\n",
      "Iteration 452, loss = 0.02221111\n",
      "Iteration 453, loss = 0.03129668\n",
      "Iteration 454, loss = 0.02221126\n",
      "Iteration 455, loss = 0.02824482\n",
      "Iteration 456, loss = 0.02335246\n",
      "Iteration 457, loss = 0.02529247\n",
      "Iteration 458, loss = 0.02284613\n",
      "Iteration 459, loss = 0.02398408\n",
      "Iteration 460, loss = 0.02447399\n",
      "Iteration 461, loss = 0.02082178\n",
      "Iteration 462, loss = 0.02433709\n",
      "Iteration 463, loss = 0.02202524\n",
      "Iteration 464, loss = 0.02220195\n",
      "Iteration 465, loss = 0.02003880\n",
      "Iteration 466, loss = 0.02139814\n",
      "Iteration 467, loss = 0.02084392\n",
      "Iteration 468, loss = 0.02045086\n",
      "Iteration 469, loss = 0.02031194\n",
      "Iteration 470, loss = 0.01932314\n",
      "Iteration 471, loss = 0.02110556\n",
      "Iteration 472, loss = 0.01929343\n",
      "Iteration 473, loss = 0.02180829\n",
      "Iteration 474, loss = 0.01894648\n",
      "Iteration 475, loss = 0.01980434\n",
      "Iteration 476, loss = 0.01921763\n",
      "Iteration 477, loss = 0.01804500\n",
      "Iteration 478, loss = 0.01801161\n",
      "Iteration 479, loss = 0.01744292\n",
      "Iteration 480, loss = 0.01668440\n",
      "Iteration 481, loss = 0.01731252\n",
      "Iteration 482, loss = 0.01773395\n",
      "Iteration 483, loss = 0.01752619\n",
      "Iteration 484, loss = 0.01661552\n",
      "Iteration 485, loss = 0.01637265\n",
      "Iteration 486, loss = 0.01771415\n",
      "Iteration 487, loss = 0.01709551\n",
      "Iteration 488, loss = 0.01641298\n",
      "Iteration 489, loss = 0.01655572\n",
      "Iteration 490, loss = 0.01580534\n",
      "Iteration 491, loss = 0.01604936\n",
      "Iteration 492, loss = 0.01567343\n",
      "Iteration 493, loss = 0.01553042\n",
      "Iteration 494, loss = 0.01577375\n",
      "Iteration 495, loss = 0.01527060\n",
      "Iteration 496, loss = 0.01617360\n",
      "Iteration 497, loss = 0.01508698\n",
      "Iteration 498, loss = 0.01526376\n",
      "Iteration 499, loss = 0.01620735\n",
      "Iteration 500, loss = 0.01480944\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 50, 25),\n",
       "              max_iter=500, random_state=42, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\r\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8225806451612904"
      ]
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\r\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode-se ver, chegou-se a uma acurácia de 82% utilizando essa configuração de MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n    Abnormal       0.88      0.86      0.87        44\n      Normal       0.68      0.72      0.70        18\n\n    accuracy                           0.82        62\n   macro avg       0.78      0.79      0.79        62\nweighted avg       0.83      0.82      0.82        62\n\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\r\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O relatório nos mostra que obteve-se uma *precision* de 88% nos casos anormais e 68% nos casos normais, e um *recall* de 86% nos casos anormais e 72% nos casos normais."
   ]
  },
  {
   "source": [
    "A sinalização de warning diz que o número limite de interações foi alcançado, porém a otmização ainda não convergiu. Iremos dobrar o número de interações e verificaremos como o MPL se comporta com os mesmos valores de hidden_layer_sizes e após com os valores dobrados hidden_layer_sizes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**EXPERIMENTO 2**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "O caso em que apenas duplicamos o número de interações"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "s = 0.52626177\n",
      "Iteration 30, loss = 0.51759395\n",
      "Iteration 31, loss = 0.50849511\n",
      "Iteration 32, loss = 0.49877042\n",
      "Iteration 33, loss = 0.49079236\n",
      "Iteration 34, loss = 0.48267256\n",
      "Iteration 35, loss = 0.47408682\n",
      "Iteration 36, loss = 0.46559063\n",
      "Iteration 37, loss = 0.45766057\n",
      "Iteration 38, loss = 0.44948514\n",
      "Iteration 39, loss = 0.44238572\n",
      "Iteration 40, loss = 0.43702578\n",
      "Iteration 41, loss = 0.43196796\n",
      "Iteration 42, loss = 0.42478124\n",
      "Iteration 43, loss = 0.41859180\n",
      "Iteration 44, loss = 0.41363341\n",
      "Iteration 45, loss = 0.40927624\n",
      "Iteration 46, loss = 0.40405530\n",
      "Iteration 47, loss = 0.40019234\n",
      "Iteration 48, loss = 0.39594084\n",
      "Iteration 49, loss = 0.39179582\n",
      "Iteration 50, loss = 0.38802776\n",
      "Iteration 51, loss = 0.38437351\n",
      "Iteration 52, loss = 0.38193176\n",
      "Iteration 53, loss = 0.37792769\n",
      "Iteration 54, loss = 0.37452595\n",
      "Iteration 55, loss = 0.37279817\n",
      "Iteration 56, loss = 0.37127401\n",
      "Iteration 57, loss = 0.36826991\n",
      "Iteration 58, loss = 0.36419292\n",
      "Iteration 59, loss = 0.36115357\n",
      "Iteration 60, loss = 0.35782583\n",
      "Iteration 61, loss = 0.35545849\n",
      "Iteration 62, loss = 0.35294844\n",
      "Iteration 63, loss = 0.35107136\n",
      "Iteration 64, loss = 0.34712682\n",
      "Iteration 65, loss = 0.34758777\n",
      "Iteration 66, loss = 0.34537064\n",
      "Iteration 67, loss = 0.33999710\n",
      "Iteration 68, loss = 0.33879099\n",
      "Iteration 69, loss = 0.33818018\n",
      "Iteration 70, loss = 0.33354463\n",
      "Iteration 71, loss = 0.33292385\n",
      "Iteration 72, loss = 0.33524424\n",
      "Iteration 73, loss = 0.32799442\n",
      "Iteration 74, loss = 0.32757823\n",
      "Iteration 75, loss = 0.32611956\n",
      "Iteration 76, loss = 0.32116031\n",
      "Iteration 77, loss = 0.32275155\n",
      "Iteration 78, loss = 0.32055675\n",
      "Iteration 79, loss = 0.31558219\n",
      "Iteration 80, loss = 0.31675372\n",
      "Iteration 81, loss = 0.31376971\n",
      "Iteration 82, loss = 0.30847143\n",
      "Iteration 83, loss = 0.31753825\n",
      "Iteration 84, loss = 0.31160867\n",
      "Iteration 85, loss = 0.30583011\n",
      "Iteration 86, loss = 0.30666358\n",
      "Iteration 87, loss = 0.30237626\n",
      "Iteration 88, loss = 0.29814873\n",
      "Iteration 89, loss = 0.30514419\n",
      "Iteration 90, loss = 0.30775100\n",
      "Iteration 91, loss = 0.29693683\n",
      "Iteration 92, loss = 0.29533361\n",
      "Iteration 93, loss = 0.29731344\n",
      "Iteration 94, loss = 0.29436712\n",
      "Iteration 95, loss = 0.28953060\n",
      "Iteration 96, loss = 0.28868308\n",
      "Iteration 97, loss = 0.29034118\n",
      "Iteration 98, loss = 0.28581745\n",
      "Iteration 99, loss = 0.28587602\n",
      "Iteration 100, loss = 0.28475216\n",
      "Iteration 101, loss = 0.28053796\n",
      "Iteration 102, loss = 0.28372154\n",
      "Iteration 103, loss = 0.27829373\n",
      "Iteration 104, loss = 0.27642433\n",
      "Iteration 105, loss = 0.29213007\n",
      "Iteration 106, loss = 0.30104348\n",
      "Iteration 107, loss = 0.28616454\n",
      "Iteration 108, loss = 0.27125432\n",
      "Iteration 109, loss = 0.28334280\n",
      "Iteration 110, loss = 0.28364845\n",
      "Iteration 111, loss = 0.27234517\n",
      "Iteration 112, loss = 0.26892194\n",
      "Iteration 113, loss = 0.26995750\n",
      "Iteration 114, loss = 0.26709117\n",
      "Iteration 115, loss = 0.26539239\n",
      "Iteration 116, loss = 0.26358374\n",
      "Iteration 117, loss = 0.26256107\n",
      "Iteration 118, loss = 0.26122995\n",
      "Iteration 119, loss = 0.26086074\n",
      "Iteration 120, loss = 0.25934023\n",
      "Iteration 121, loss = 0.25764393\n",
      "Iteration 122, loss = 0.25657370\n",
      "Iteration 123, loss = 0.25514046\n",
      "Iteration 124, loss = 0.25532976\n",
      "Iteration 125, loss = 0.25655989\n",
      "Iteration 126, loss = 0.25523253\n",
      "Iteration 127, loss = 0.25469523\n",
      "Iteration 128, loss = 0.25245739\n",
      "Iteration 129, loss = 0.25016205\n",
      "Iteration 130, loss = 0.24852917\n",
      "Iteration 131, loss = 0.24710990\n",
      "Iteration 132, loss = 0.24683131\n",
      "Iteration 133, loss = 0.24545871\n",
      "Iteration 134, loss = 0.24597220\n",
      "Iteration 135, loss = 0.24481001\n",
      "Iteration 136, loss = 0.24199649\n",
      "Iteration 137, loss = 0.24658371\n",
      "Iteration 138, loss = 0.24507141\n",
      "Iteration 139, loss = 0.23868282\n",
      "Iteration 140, loss = 0.24355670\n",
      "Iteration 141, loss = 0.23880273\n",
      "Iteration 142, loss = 0.23985506\n",
      "Iteration 143, loss = 0.24050698\n",
      "Iteration 144, loss = 0.23648598\n",
      "Iteration 145, loss = 0.23704775\n",
      "Iteration 146, loss = 0.23800253\n",
      "Iteration 147, loss = 0.23536641\n",
      "Iteration 148, loss = 0.23180790\n",
      "Iteration 149, loss = 0.23648561\n",
      "Iteration 150, loss = 0.24047690\n",
      "Iteration 151, loss = 0.22887455\n",
      "Iteration 152, loss = 0.24021424\n",
      "Iteration 153, loss = 0.23720978\n",
      "Iteration 154, loss = 0.22815818\n",
      "Iteration 155, loss = 0.23304232\n",
      "Iteration 156, loss = 0.22954892\n",
      "Iteration 157, loss = 0.22470505\n",
      "Iteration 158, loss = 0.22595796\n",
      "Iteration 159, loss = 0.22557058\n",
      "Iteration 160, loss = 0.22249979\n",
      "Iteration 161, loss = 0.22291141\n",
      "Iteration 162, loss = 0.22520359\n",
      "Iteration 163, loss = 0.22152053\n",
      "Iteration 164, loss = 0.22240555\n",
      "Iteration 165, loss = 0.21918356\n",
      "Iteration 166, loss = 0.22021325\n",
      "Iteration 167, loss = 0.21950229\n",
      "Iteration 168, loss = 0.21715877\n",
      "Iteration 169, loss = 0.21568332\n",
      "Iteration 170, loss = 0.21483070\n",
      "Iteration 171, loss = 0.21313074\n",
      "Iteration 172, loss = 0.21334271\n",
      "Iteration 173, loss = 0.21282682\n",
      "Iteration 174, loss = 0.21122780\n",
      "Iteration 175, loss = 0.21240677\n",
      "Iteration 176, loss = 0.20962332\n",
      "Iteration 177, loss = 0.21144870\n",
      "Iteration 178, loss = 0.21173123\n",
      "Iteration 179, loss = 0.20787954\n",
      "Iteration 180, loss = 0.21017570\n",
      "Iteration 181, loss = 0.21139904\n",
      "Iteration 182, loss = 0.20627391\n",
      "Iteration 183, loss = 0.20537782\n",
      "Iteration 184, loss = 0.20610676\n",
      "Iteration 185, loss = 0.20591421\n",
      "Iteration 186, loss = 0.20466707\n",
      "Iteration 187, loss = 0.20236081\n",
      "Iteration 188, loss = 0.20680598\n",
      "Iteration 189, loss = 0.20342767\n",
      "Iteration 190, loss = 0.20057767\n",
      "Iteration 191, loss = 0.20604367\n",
      "Iteration 192, loss = 0.20244070\n",
      "Iteration 193, loss = 0.20009981\n",
      "Iteration 194, loss = 0.19934504\n",
      "Iteration 195, loss = 0.19807844\n",
      "Iteration 196, loss = 0.20217305\n",
      "Iteration 197, loss = 0.19853354\n",
      "Iteration 198, loss = 0.19498187\n",
      "Iteration 199, loss = 0.19485464\n",
      "Iteration 200, loss = 0.19258989\n",
      "Iteration 201, loss = 0.19211848\n",
      "Iteration 202, loss = 0.19191986\n",
      "Iteration 203, loss = 0.18961858\n",
      "Iteration 204, loss = 0.18879221\n",
      "Iteration 205, loss = 0.19008050\n",
      "Iteration 206, loss = 0.19045162\n",
      "Iteration 207, loss = 0.18848505\n",
      "Iteration 208, loss = 0.18723074\n",
      "Iteration 209, loss = 0.18756243\n",
      "Iteration 210, loss = 0.18580086\n",
      "Iteration 211, loss = 0.18382652\n",
      "Iteration 212, loss = 0.18435661\n",
      "Iteration 213, loss = 0.18323068\n",
      "Iteration 214, loss = 0.18523530\n",
      "Iteration 215, loss = 0.18439568\n",
      "Iteration 216, loss = 0.17964116\n",
      "Iteration 217, loss = 0.18038314\n",
      "Iteration 218, loss = 0.18005689\n",
      "Iteration 219, loss = 0.18099398\n",
      "Iteration 220, loss = 0.18146546\n",
      "Iteration 221, loss = 0.17500818\n",
      "Iteration 222, loss = 0.18571160\n",
      "Iteration 223, loss = 0.17440993\n",
      "Iteration 224, loss = 0.19064869\n",
      "Iteration 225, loss = 0.19045544\n",
      "Iteration 226, loss = 0.16864188\n",
      "Iteration 227, loss = 0.18123044\n",
      "Iteration 228, loss = 0.18366494\n",
      "Iteration 229, loss = 0.16640622\n",
      "Iteration 230, loss = 0.18348031\n",
      "Iteration 231, loss = 0.17864181\n",
      "Iteration 232, loss = 0.17021039\n",
      "Iteration 233, loss = 0.17562974\n",
      "Iteration 234, loss = 0.16718184\n",
      "Iteration 235, loss = 0.16353503\n",
      "Iteration 236, loss = 0.16694619\n",
      "Iteration 237, loss = 0.16425773\n",
      "Iteration 238, loss = 0.16319738\n",
      "Iteration 239, loss = 0.16794706\n",
      "Iteration 240, loss = 0.15798503\n",
      "Iteration 241, loss = 0.16973839\n",
      "Iteration 242, loss = 0.16959850\n",
      "Iteration 243, loss = 0.15752916\n",
      "Iteration 244, loss = 0.16086779\n",
      "Iteration 245, loss = 0.15711143\n",
      "Iteration 246, loss = 0.15604800\n",
      "Iteration 247, loss = 0.15551996\n",
      "Iteration 248, loss = 0.15580059\n",
      "Iteration 249, loss = 0.15832773\n",
      "Iteration 250, loss = 0.15190829\n",
      "Iteration 251, loss = 0.15660973\n",
      "Iteration 252, loss = 0.15003604\n",
      "Iteration 253, loss = 0.15627413\n",
      "Iteration 254, loss = 0.15652650\n",
      "Iteration 255, loss = 0.14804023\n",
      "Iteration 256, loss = 0.15841156\n",
      "Iteration 257, loss = 0.15297689\n",
      "Iteration 258, loss = 0.14867061\n",
      "Iteration 259, loss = 0.14629635\n",
      "Iteration 260, loss = 0.14386924\n",
      "Iteration 261, loss = 0.15117592\n",
      "Iteration 262, loss = 0.14457942\n",
      "Iteration 263, loss = 0.14683136\n",
      "Iteration 264, loss = 0.14257825\n",
      "Iteration 265, loss = 0.14387036\n",
      "Iteration 266, loss = 0.13974868\n",
      "Iteration 267, loss = 0.14103207\n",
      "Iteration 268, loss = 0.13941237\n",
      "Iteration 269, loss = 0.13590702\n",
      "Iteration 270, loss = 0.13695744\n",
      "Iteration 271, loss = 0.13565917\n",
      "Iteration 272, loss = 0.13861749\n",
      "Iteration 273, loss = 0.14504516\n",
      "Iteration 274, loss = 0.13154954\n",
      "Iteration 275, loss = 0.13733622\n",
      "Iteration 276, loss = 0.13394156\n",
      "Iteration 277, loss = 0.13053415\n",
      "Iteration 278, loss = 0.13510182\n",
      "Iteration 279, loss = 0.12887672\n",
      "Iteration 280, loss = 0.13407253\n",
      "Iteration 281, loss = 0.13502057\n",
      "Iteration 282, loss = 0.12815939\n",
      "Iteration 283, loss = 0.13638540\n",
      "Iteration 284, loss = 0.13289892\n",
      "Iteration 285, loss = 0.12531056\n",
      "Iteration 286, loss = 0.12272604\n",
      "Iteration 287, loss = 0.12484573\n",
      "Iteration 288, loss = 0.12394866\n",
      "Iteration 289, loss = 0.12438322\n",
      "Iteration 290, loss = 0.12387418\n",
      "Iteration 291, loss = 0.11813298\n",
      "Iteration 292, loss = 0.13341335\n",
      "Iteration 293, loss = 0.11914976\n",
      "Iteration 294, loss = 0.14425183\n",
      "Iteration 295, loss = 0.13319335\n",
      "Iteration 296, loss = 0.12247643\n",
      "Iteration 297, loss = 0.12636708\n",
      "Iteration 298, loss = 0.11768980\n",
      "Iteration 299, loss = 0.12312790\n",
      "Iteration 300, loss = 0.11991252\n",
      "Iteration 301, loss = 0.11457659\n",
      "Iteration 302, loss = 0.11350479\n",
      "Iteration 303, loss = 0.11144785\n",
      "Iteration 304, loss = 0.11355095\n",
      "Iteration 305, loss = 0.10910518\n",
      "Iteration 306, loss = 0.11382184\n",
      "Iteration 307, loss = 0.11247936\n",
      "Iteration 308, loss = 0.10945363\n",
      "Iteration 309, loss = 0.10745615\n",
      "Iteration 310, loss = 0.10795681\n",
      "Iteration 311, loss = 0.11244078\n",
      "Iteration 312, loss = 0.10720240\n",
      "Iteration 313, loss = 0.10911746\n",
      "Iteration 314, loss = 0.10454195\n",
      "Iteration 315, loss = 0.10926159\n",
      "Iteration 316, loss = 0.10573830\n",
      "Iteration 317, loss = 0.10086552\n",
      "Iteration 318, loss = 0.10924007\n",
      "Iteration 319, loss = 0.10921906\n",
      "Iteration 320, loss = 0.10268517\n",
      "Iteration 321, loss = 0.10489236\n",
      "Iteration 322, loss = 0.10112293\n",
      "Iteration 323, loss = 0.10745991\n",
      "Iteration 324, loss = 0.09944738\n",
      "Iteration 325, loss = 0.09800171\n",
      "Iteration 326, loss = 0.10255993\n",
      "Iteration 327, loss = 0.09782709\n",
      "Iteration 328, loss = 0.09575220\n",
      "Iteration 329, loss = 0.09378833\n",
      "Iteration 330, loss = 0.09272051\n",
      "Iteration 331, loss = 0.09650776\n",
      "Iteration 332, loss = 0.09284100\n",
      "Iteration 333, loss = 0.09399250\n",
      "Iteration 334, loss = 0.09429499\n",
      "Iteration 335, loss = 0.09240091\n",
      "Iteration 336, loss = 0.09668091\n",
      "Iteration 337, loss = 0.08674319\n",
      "Iteration 338, loss = 0.09945796\n",
      "Iteration 339, loss = 0.09324660\n",
      "Iteration 340, loss = 0.09594256\n",
      "Iteration 341, loss = 0.08956353\n",
      "Iteration 342, loss = 0.09154707\n",
      "Iteration 343, loss = 0.08629259\n",
      "Iteration 344, loss = 0.09435156\n",
      "Iteration 345, loss = 0.08863627\n",
      "Iteration 346, loss = 0.08794433\n",
      "Iteration 347, loss = 0.08730308\n",
      "Iteration 348, loss = 0.08544815\n",
      "Iteration 349, loss = 0.08965662\n",
      "Iteration 350, loss = 0.08314598\n",
      "Iteration 351, loss = 0.08147440\n",
      "Iteration 352, loss = 0.08135818\n",
      "Iteration 353, loss = 0.07885236\n",
      "Iteration 354, loss = 0.08199388\n",
      "Iteration 355, loss = 0.07786614\n",
      "Iteration 356, loss = 0.07886114\n",
      "Iteration 357, loss = 0.07927462\n",
      "Iteration 358, loss = 0.07548229\n",
      "Iteration 359, loss = 0.07753978\n",
      "Iteration 360, loss = 0.07911912\n",
      "Iteration 361, loss = 0.07623487\n",
      "Iteration 362, loss = 0.07496146\n",
      "Iteration 363, loss = 0.07635487\n",
      "Iteration 364, loss = 0.07231954\n",
      "Iteration 365, loss = 0.07457238\n",
      "Iteration 366, loss = 0.07150206\n",
      "Iteration 367, loss = 0.06906306\n",
      "Iteration 368, loss = 0.06886438\n",
      "Iteration 369, loss = 0.06599132\n",
      "Iteration 370, loss = 0.06507397\n",
      "Iteration 371, loss = 0.06359288\n",
      "Iteration 372, loss = 0.06216428\n",
      "Iteration 373, loss = 0.05925433\n",
      "Iteration 374, loss = 0.05993531\n",
      "Iteration 375, loss = 0.05674369\n",
      "Iteration 376, loss = 0.05470137\n",
      "Iteration 377, loss = 0.05656481\n",
      "Iteration 378, loss = 0.05466904\n",
      "Iteration 379, loss = 0.05312790\n",
      "Iteration 380, loss = 0.05004969\n",
      "Iteration 381, loss = 0.04894112\n",
      "Iteration 382, loss = 0.04744171\n",
      "Iteration 383, loss = 0.05024155\n",
      "Iteration 384, loss = 0.04918042\n",
      "Iteration 385, loss = 0.05453765\n",
      "Iteration 386, loss = 0.04775989\n",
      "Iteration 387, loss = 0.04574789\n",
      "Iteration 388, loss = 0.04430461\n",
      "Iteration 389, loss = 0.04461270\n",
      "Iteration 390, loss = 0.04553927\n",
      "Iteration 391, loss = 0.04518454\n",
      "Iteration 392, loss = 0.04283137\n",
      "Iteration 393, loss = 0.04123926\n",
      "Iteration 394, loss = 0.04386138\n",
      "Iteration 395, loss = 0.04337777\n",
      "Iteration 396, loss = 0.04184605\n",
      "Iteration 397, loss = 0.04448526\n",
      "Iteration 398, loss = 0.04127166\n",
      "Iteration 399, loss = 0.04253991\n",
      "Iteration 400, loss = 0.03863423\n",
      "Iteration 401, loss = 0.04955451\n",
      "Iteration 402, loss = 0.04025154\n",
      "Iteration 403, loss = 0.04333366\n",
      "Iteration 404, loss = 0.03981201\n",
      "Iteration 405, loss = 0.04188418\n",
      "Iteration 406, loss = 0.04149262\n",
      "Iteration 407, loss = 0.03871291\n",
      "Iteration 408, loss = 0.03945455\n",
      "Iteration 409, loss = 0.03809277\n",
      "Iteration 410, loss = 0.03436259\n",
      "Iteration 411, loss = 0.03970317\n",
      "Iteration 412, loss = 0.03430528\n",
      "Iteration 413, loss = 0.04122940\n",
      "Iteration 414, loss = 0.03534095\n",
      "Iteration 415, loss = 0.03502297\n",
      "Iteration 416, loss = 0.03794251\n",
      "Iteration 417, loss = 0.03255763\n",
      "Iteration 418, loss = 0.04155914\n",
      "Iteration 419, loss = 0.03296443\n",
      "Iteration 420, loss = 0.04151718\n",
      "Iteration 421, loss = 0.03229186\n",
      "Iteration 422, loss = 0.03987337\n",
      "Iteration 423, loss = 0.03759786\n",
      "Iteration 424, loss = 0.03082727\n",
      "Iteration 425, loss = 0.03254594\n",
      "Iteration 426, loss = 0.03091517\n",
      "Iteration 427, loss = 0.03210545\n",
      "Iteration 428, loss = 0.03034621\n",
      "Iteration 429, loss = 0.02808593\n",
      "Iteration 430, loss = 0.02820793\n",
      "Iteration 431, loss = 0.02898768\n",
      "Iteration 432, loss = 0.02930857\n",
      "Iteration 433, loss = 0.02757458\n",
      "Iteration 434, loss = 0.02809331\n",
      "Iteration 435, loss = 0.02748104\n",
      "Iteration 436, loss = 0.02733111\n",
      "Iteration 437, loss = 0.02647267\n",
      "Iteration 438, loss = 0.02551782\n",
      "Iteration 439, loss = 0.02504165\n",
      "Iteration 440, loss = 0.02485856\n",
      "Iteration 441, loss = 0.02439569\n",
      "Iteration 442, loss = 0.02423673\n",
      "Iteration 443, loss = 0.02427086\n",
      "Iteration 444, loss = 0.02408347\n",
      "Iteration 445, loss = 0.02392533\n",
      "Iteration 446, loss = 0.02372512\n",
      "Iteration 447, loss = 0.02419925\n",
      "Iteration 448, loss = 0.02320247\n",
      "Iteration 449, loss = 0.02453380\n",
      "Iteration 450, loss = 0.02262293\n",
      "Iteration 451, loss = 0.02409020\n",
      "Iteration 452, loss = 0.02221111\n",
      "Iteration 453, loss = 0.03129668\n",
      "Iteration 454, loss = 0.02221126\n",
      "Iteration 455, loss = 0.02824482\n",
      "Iteration 456, loss = 0.02335246\n",
      "Iteration 457, loss = 0.02529247\n",
      "Iteration 458, loss = 0.02284613\n",
      "Iteration 459, loss = 0.02398408\n",
      "Iteration 460, loss = 0.02447399\n",
      "Iteration 461, loss = 0.02082178\n",
      "Iteration 462, loss = 0.02433709\n",
      "Iteration 463, loss = 0.02202524\n",
      "Iteration 464, loss = 0.02220195\n",
      "Iteration 465, loss = 0.02003880\n",
      "Iteration 466, loss = 0.02139814\n",
      "Iteration 467, loss = 0.02084392\n",
      "Iteration 468, loss = 0.02045086\n",
      "Iteration 469, loss = 0.02031194\n",
      "Iteration 470, loss = 0.01932314\n",
      "Iteration 471, loss = 0.02110556\n",
      "Iteration 472, loss = 0.01929343\n",
      "Iteration 473, loss = 0.02180829\n",
      "Iteration 474, loss = 0.01894648\n",
      "Iteration 475, loss = 0.01980434\n",
      "Iteration 476, loss = 0.01921763\n",
      "Iteration 477, loss = 0.01804500\n",
      "Iteration 478, loss = 0.01801161\n",
      "Iteration 479, loss = 0.01744292\n",
      "Iteration 480, loss = 0.01668440\n",
      "Iteration 481, loss = 0.01731252\n",
      "Iteration 482, loss = 0.01773395\n",
      "Iteration 483, loss = 0.01752619\n",
      "Iteration 484, loss = 0.01661552\n",
      "Iteration 485, loss = 0.01637265\n",
      "Iteration 486, loss = 0.01771415\n",
      "Iteration 487, loss = 0.01709551\n",
      "Iteration 488, loss = 0.01641298\n",
      "Iteration 489, loss = 0.01655572\n",
      "Iteration 490, loss = 0.01580534\n",
      "Iteration 491, loss = 0.01604936\n",
      "Iteration 492, loss = 0.01567343\n",
      "Iteration 493, loss = 0.01553042\n",
      "Iteration 494, loss = 0.01577375\n",
      "Iteration 495, loss = 0.01527060\n",
      "Iteration 496, loss = 0.01617360\n",
      "Iteration 497, loss = 0.01508698\n",
      "Iteration 498, loss = 0.01526376\n",
      "Iteration 499, loss = 0.01620735\n",
      "Iteration 500, loss = 0.01480944\n",
      "Iteration 501, loss = 0.01434613\n",
      "Iteration 502, loss = 0.01502526\n",
      "Iteration 503, loss = 0.01504638\n",
      "Iteration 504, loss = 0.01427286\n",
      "Iteration 505, loss = 0.01365332\n",
      "Iteration 506, loss = 0.01395716\n",
      "Iteration 507, loss = 0.01452077\n",
      "Iteration 508, loss = 0.01386258\n",
      "Iteration 509, loss = 0.01350361\n",
      "Iteration 510, loss = 0.01385295\n",
      "Iteration 511, loss = 0.01361773\n",
      "Iteration 512, loss = 0.01350933\n",
      "Iteration 513, loss = 0.01357446\n",
      "Iteration 514, loss = 0.01304862\n",
      "Iteration 515, loss = 0.01316604\n",
      "Iteration 516, loss = 0.01296510\n",
      "Iteration 517, loss = 0.01283202\n",
      "Iteration 518, loss = 0.01351654\n",
      "Iteration 519, loss = 0.01268857\n",
      "Iteration 520, loss = 0.01277707\n",
      "Iteration 521, loss = 0.01277499\n",
      "Iteration 522, loss = 0.01238046\n",
      "Iteration 523, loss = 0.01312755\n",
      "Iteration 524, loss = 0.01182304\n",
      "Iteration 525, loss = 0.01367198\n",
      "Iteration 526, loss = 0.01405552\n",
      "Iteration 527, loss = 0.01188437\n",
      "Iteration 528, loss = 0.01304997\n",
      "Iteration 529, loss = 0.01220591\n",
      "Iteration 530, loss = 0.01203294\n",
      "Iteration 531, loss = 0.01235236\n",
      "Iteration 532, loss = 0.01130445\n",
      "Iteration 533, loss = 0.01169703\n",
      "Iteration 534, loss = 0.01212396\n",
      "Iteration 535, loss = 0.01129311\n",
      "Iteration 536, loss = 0.01223677\n",
      "Iteration 537, loss = 0.01186252\n",
      "Iteration 538, loss = 0.01182704\n",
      "Iteration 539, loss = 0.01154930\n",
      "Iteration 540, loss = 0.01114710\n",
      "Iteration 541, loss = 0.01194795\n",
      "Iteration 542, loss = 0.01131952\n",
      "Iteration 543, loss = 0.01042558\n",
      "Iteration 544, loss = 0.01105694\n",
      "Iteration 545, loss = 0.01095799\n",
      "Iteration 546, loss = 0.01049691\n",
      "Iteration 547, loss = 0.01093052\n",
      "Iteration 548, loss = 0.00988633\n",
      "Iteration 549, loss = 0.01121366\n",
      "Iteration 550, loss = 0.01137982\n",
      "Iteration 551, loss = 0.01068167\n",
      "Iteration 552, loss = 0.01079222\n",
      "Iteration 553, loss = 0.00973416\n",
      "Iteration 554, loss = 0.00973205\n",
      "Iteration 555, loss = 0.01060984\n",
      "Iteration 556, loss = 0.01036935\n",
      "Iteration 557, loss = 0.01021898\n",
      "Iteration 558, loss = 0.01007449\n",
      "Iteration 559, loss = 0.00957953\n",
      "Iteration 560, loss = 0.00957426\n",
      "Iteration 561, loss = 0.00952698\n",
      "Iteration 562, loss = 0.00930481\n",
      "Iteration 563, loss = 0.00946157\n",
      "Iteration 564, loss = 0.00971077\n",
      "Iteration 565, loss = 0.00929163\n",
      "Iteration 566, loss = 0.00892762\n",
      "Iteration 567, loss = 0.00901365\n",
      "Iteration 568, loss = 0.00887318\n",
      "Iteration 569, loss = 0.00881513\n",
      "Iteration 570, loss = 0.00928869\n",
      "Iteration 571, loss = 0.00918503\n",
      "Iteration 572, loss = 0.00866175\n",
      "Iteration 573, loss = 0.00870435\n",
      "Iteration 574, loss = 0.00862351\n",
      "Iteration 575, loss = 0.00863551\n",
      "Iteration 576, loss = 0.00866221\n",
      "Iteration 577, loss = 0.00862224\n",
      "Iteration 578, loss = 0.00850783\n",
      "Iteration 579, loss = 0.00837706\n",
      "Iteration 580, loss = 0.00824037\n",
      "Iteration 581, loss = 0.00819853\n",
      "Iteration 582, loss = 0.00834949\n",
      "Iteration 583, loss = 0.00807153\n",
      "Iteration 584, loss = 0.00838441\n",
      "Iteration 585, loss = 0.00882826\n",
      "Iteration 586, loss = 0.00822678\n",
      "Iteration 587, loss = 0.00806190\n",
      "Iteration 588, loss = 0.00857047\n",
      "Iteration 589, loss = 0.00827647\n",
      "Iteration 590, loss = 0.00780875\n",
      "Iteration 591, loss = 0.00789467\n",
      "Iteration 592, loss = 0.00802524\n",
      "Iteration 593, loss = 0.00777484\n",
      "Iteration 594, loss = 0.00770340\n",
      "Iteration 595, loss = 0.00758786\n",
      "Iteration 596, loss = 0.00763716\n",
      "Iteration 597, loss = 0.00776099\n",
      "Iteration 598, loss = 0.00772614\n",
      "Iteration 599, loss = 0.00761460\n",
      "Iteration 600, loss = 0.00743653\n",
      "Iteration 601, loss = 0.00778604\n",
      "Iteration 602, loss = 0.00795020\n",
      "Iteration 603, loss = 0.00748313\n",
      "Iteration 604, loss = 0.00717785\n",
      "Iteration 605, loss = 0.00751804\n",
      "Iteration 606, loss = 0.00720428\n",
      "Iteration 607, loss = 0.00734431\n",
      "Iteration 608, loss = 0.00764052\n",
      "Iteration 609, loss = 0.00702330\n",
      "Iteration 610, loss = 0.00728261\n",
      "Iteration 611, loss = 0.00827493\n",
      "Iteration 612, loss = 0.00724802\n",
      "Iteration 613, loss = 0.00715392\n",
      "Iteration 614, loss = 0.00770984\n",
      "Iteration 615, loss = 0.00725966\n",
      "Iteration 616, loss = 0.00681834\n",
      "Iteration 617, loss = 0.00760815\n",
      "Iteration 618, loss = 0.00781443\n",
      "Iteration 619, loss = 0.00682269\n",
      "Iteration 620, loss = 0.00679759\n",
      "Iteration 621, loss = 0.00713095\n",
      "Iteration 622, loss = 0.00684099\n",
      "Iteration 623, loss = 0.00655276\n",
      "Iteration 624, loss = 0.00673673\n",
      "Iteration 625, loss = 0.00667442\n",
      "Iteration 626, loss = 0.00649704\n",
      "Iteration 627, loss = 0.00641223\n",
      "Iteration 628, loss = 0.00640480\n",
      "Iteration 629, loss = 0.00636604\n",
      "Iteration 630, loss = 0.00631580\n",
      "Iteration 631, loss = 0.00631018\n",
      "Iteration 632, loss = 0.00643560\n",
      "Iteration 633, loss = 0.00632808\n",
      "Iteration 634, loss = 0.00628150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(100, 50, 25),\n",
       "              max_iter=1000, random_state=42, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=1000, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8225806451612904"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "Tivemos um aumento da accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n    Abnormal       0.88      0.86      0.87        44\n      Normal       0.68      0.72      0.70        18\n\n    accuracy                           0.82        62\n   macro avg       0.78      0.79      0.79        62\nweighted avg       0.83      0.82      0.82        62\n\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "Tivemos uma melhora de precision e recall para ambaos os casos, se comporado com o experimento anterior."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**EXPERIMENTO3**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Agora iremos verificar o comportamento do MPL quando dobrandos os valores para o hidden_layer_sizes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.65445155\n",
      "Iteration 2, loss = 0.63917334\n",
      "Iteration 3, loss = 0.62999389\n",
      "Iteration 4, loss = 0.62672357\n",
      "Iteration 5, loss = 0.62598898\n",
      "Iteration 6, loss = 0.62473913\n",
      "Iteration 7, loss = 0.62163221\n",
      "Iteration 8, loss = 0.61556801\n",
      "Iteration 9, loss = 0.60834522\n",
      "Iteration 10, loss = 0.59979065\n",
      "Iteration 11, loss = 0.59252952\n",
      "Iteration 12, loss = 0.58449774\n",
      "Iteration 13, loss = 0.57515478\n",
      "Iteration 14, loss = 0.56469295\n",
      "Iteration 15, loss = 0.55212589\n",
      "Iteration 16, loss = 0.53878341\n",
      "Iteration 17, loss = 0.52473140\n",
      "Iteration 18, loss = 0.50958594\n",
      "Iteration 19, loss = 0.49649645\n",
      "Iteration 20, loss = 0.48430963\n",
      "Iteration 21, loss = 0.46855104\n",
      "Iteration 22, loss = 0.45427329\n",
      "Iteration 23, loss = 0.44510418\n",
      "Iteration 24, loss = 0.43572529\n",
      "Iteration 25, loss = 0.42434212\n",
      "Iteration 26, loss = 0.41854495\n",
      "Iteration 27, loss = 0.40999669\n",
      "Iteration 28, loss = 0.40199925\n",
      "Iteration 29, loss = 0.39639928\n",
      "Iteration 30, loss = 0.38992296\n",
      "Iteration 31, loss = 0.38501335\n",
      "Iteration 32, loss = 0.37883726\n",
      "Iteration 33, loss = 0.37327287\n",
      "Iteration 34, loss = 0.36825294\n",
      "Iteration 35, loss = 0.36229184\n",
      "Iteration 36, loss = 0.35975625\n",
      "Iteration 37, loss = 0.35658742\n",
      "Iteration 38, loss = 0.34868360\n",
      "Iteration 39, loss = 0.34254591\n",
      "Iteration 40, loss = 0.33857300\n",
      "Iteration 41, loss = 0.33521471\n",
      "Iteration 42, loss = 0.32996312\n",
      "Iteration 43, loss = 0.33199613\n",
      "Iteration 44, loss = 0.32167027\n",
      "Iteration 45, loss = 0.32514450\n",
      "Iteration 46, loss = 0.32282434\n",
      "Iteration 47, loss = 0.31195092\n",
      "Iteration 48, loss = 0.30892340\n",
      "Iteration 49, loss = 0.30613361\n",
      "Iteration 50, loss = 0.30532195\n",
      "Iteration 51, loss = 0.30170444\n",
      "Iteration 52, loss = 0.30090967\n",
      "Iteration 53, loss = 0.29803532\n",
      "Iteration 54, loss = 0.29451217\n",
      "Iteration 55, loss = 0.29389779\n",
      "Iteration 56, loss = 0.29239329\n",
      "Iteration 57, loss = 0.28983993\n",
      "Iteration 58, loss = 0.28646210\n",
      "Iteration 59, loss = 0.29041281\n",
      "Iteration 60, loss = 0.28166089\n",
      "Iteration 61, loss = 0.29216980\n",
      "Iteration 62, loss = 0.28232172\n",
      "Iteration 63, loss = 0.28715694\n",
      "Iteration 64, loss = 0.28823211\n",
      "Iteration 65, loss = 0.27705320\n",
      "Iteration 66, loss = 0.27378687\n",
      "Iteration 67, loss = 0.27510405\n",
      "Iteration 68, loss = 0.27314040\n",
      "Iteration 69, loss = 0.27229593\n",
      "Iteration 70, loss = 0.27086799\n",
      "Iteration 71, loss = 0.26897282\n",
      "Iteration 72, loss = 0.28166237\n",
      "Iteration 73, loss = 0.27386369\n",
      "Iteration 74, loss = 0.26418539\n",
      "Iteration 75, loss = 0.26181301\n",
      "Iteration 76, loss = 0.26108087\n",
      "Iteration 77, loss = 0.25804455\n",
      "Iteration 78, loss = 0.26115699\n",
      "Iteration 79, loss = 0.25729976\n",
      "Iteration 80, loss = 0.25843635\n",
      "Iteration 81, loss = 0.25539353\n",
      "Iteration 82, loss = 0.25716966\n",
      "Iteration 83, loss = 0.24978108\n",
      "Iteration 84, loss = 0.26661295\n",
      "Iteration 85, loss = 0.25959763\n",
      "Iteration 86, loss = 0.25466633\n",
      "Iteration 87, loss = 0.29135320\n",
      "Iteration 88, loss = 0.25485279\n",
      "Iteration 89, loss = 0.26733753\n",
      "Iteration 90, loss = 0.26547155\n",
      "Iteration 91, loss = 0.24155933\n",
      "Iteration 92, loss = 0.27702337\n",
      "Iteration 93, loss = 0.26642041\n",
      "Iteration 94, loss = 0.24718294\n",
      "Iteration 95, loss = 0.29006326\n",
      "Iteration 96, loss = 0.27216155\n",
      "Iteration 97, loss = 0.24239920\n",
      "Iteration 98, loss = 0.27297691\n",
      "Iteration 99, loss = 0.25515104\n",
      "Iteration 100, loss = 0.23930332\n",
      "Iteration 101, loss = 0.25193387\n",
      "Iteration 102, loss = 0.24232665\n",
      "Iteration 103, loss = 0.23843659\n",
      "Iteration 104, loss = 0.25100434\n",
      "Iteration 105, loss = 0.23179176\n",
      "Iteration 106, loss = 0.24627801\n",
      "Iteration 107, loss = 0.26740748\n",
      "Iteration 108, loss = 0.23428312\n",
      "Iteration 109, loss = 0.25742216\n",
      "Iteration 110, loss = 0.26622558\n",
      "Iteration 111, loss = 0.23500034\n",
      "Iteration 112, loss = 0.23593655\n",
      "Iteration 113, loss = 0.23104151\n",
      "Iteration 114, loss = 0.22786844\n",
      "Iteration 115, loss = 0.23020578\n",
      "Iteration 116, loss = 0.22703925\n",
      "Iteration 117, loss = 0.22585441\n",
      "Iteration 118, loss = 0.22530049\n",
      "Iteration 119, loss = 0.22336661\n",
      "Iteration 120, loss = 0.22194111\n",
      "Iteration 121, loss = 0.22626390\n",
      "Iteration 122, loss = 0.22612694\n",
      "Iteration 123, loss = 0.21930418\n",
      "Iteration 124, loss = 0.22400327\n",
      "Iteration 125, loss = 0.22361215\n",
      "Iteration 126, loss = 0.21730621\n",
      "Iteration 127, loss = 0.22815474\n",
      "Iteration 128, loss = 0.22132118\n",
      "Iteration 129, loss = 0.21976855\n",
      "Iteration 130, loss = 0.22048955\n",
      "Iteration 131, loss = 0.21331468\n",
      "Iteration 132, loss = 0.21880420\n",
      "Iteration 133, loss = 0.21427826\n",
      "Iteration 134, loss = 0.21670275\n",
      "Iteration 135, loss = 0.22131128\n",
      "Iteration 136, loss = 0.21057743\n",
      "Iteration 137, loss = 0.21983574\n",
      "Iteration 138, loss = 0.21086409\n",
      "Iteration 139, loss = 0.20920702\n",
      "Iteration 140, loss = 0.21474931\n",
      "Iteration 141, loss = 0.21371067\n",
      "Iteration 142, loss = 0.21268263\n",
      "Iteration 143, loss = 0.20765112\n",
      "Iteration 144, loss = 0.20648380\n",
      "Iteration 145, loss = 0.20461498\n",
      "Iteration 146, loss = 0.20448549\n",
      "Iteration 147, loss = 0.20253301\n",
      "Iteration 148, loss = 0.20709197\n",
      "Iteration 149, loss = 0.20302534\n",
      "Iteration 150, loss = 0.19880242\n",
      "Iteration 151, loss = 0.20665214\n",
      "Iteration 152, loss = 0.19429799\n",
      "Iteration 153, loss = 0.19996530\n",
      "Iteration 154, loss = 0.20618227\n",
      "Iteration 155, loss = 0.19287067\n",
      "Iteration 156, loss = 0.23199272\n",
      "Iteration 157, loss = 0.21122851\n",
      "Iteration 158, loss = 0.20300256\n",
      "Iteration 159, loss = 0.19080612\n",
      "Iteration 160, loss = 0.19798414\n",
      "Iteration 161, loss = 0.20797920\n",
      "Iteration 162, loss = 0.19351541\n",
      "Iteration 163, loss = 0.19124161\n",
      "Iteration 164, loss = 0.18642336\n",
      "Iteration 165, loss = 0.19256438\n",
      "Iteration 166, loss = 0.19176784\n",
      "Iteration 167, loss = 0.18476158\n",
      "Iteration 168, loss = 0.19097027\n",
      "Iteration 169, loss = 0.18548737\n",
      "Iteration 170, loss = 0.19005587\n",
      "Iteration 171, loss = 0.18175445\n",
      "Iteration 172, loss = 0.17982398\n",
      "Iteration 173, loss = 0.18789274\n",
      "Iteration 174, loss = 0.18349475\n",
      "Iteration 175, loss = 0.18280447\n",
      "Iteration 176, loss = 0.18232267\n",
      "Iteration 177, loss = 0.18091398\n",
      "Iteration 178, loss = 0.18360789\n",
      "Iteration 179, loss = 0.17505298\n",
      "Iteration 180, loss = 0.18758826\n",
      "Iteration 181, loss = 0.17151432\n",
      "Iteration 182, loss = 0.17691004\n",
      "Iteration 183, loss = 0.17182171\n",
      "Iteration 184, loss = 0.17840954\n",
      "Iteration 185, loss = 0.17322520\n",
      "Iteration 186, loss = 0.17357905\n",
      "Iteration 187, loss = 0.16718691\n",
      "Iteration 188, loss = 0.17031533\n",
      "Iteration 189, loss = 0.16785302\n",
      "Iteration 190, loss = 0.16542452\n",
      "Iteration 191, loss = 0.16163118\n",
      "Iteration 192, loss = 0.16638659\n",
      "Iteration 193, loss = 0.16399088\n",
      "Iteration 194, loss = 0.16470451\n",
      "Iteration 195, loss = 0.17688016\n",
      "Iteration 196, loss = 0.15533827\n",
      "Iteration 197, loss = 0.18447589\n",
      "Iteration 198, loss = 0.16168856\n",
      "Iteration 199, loss = 0.17145269\n",
      "Iteration 200, loss = 0.18637508\n",
      "Iteration 201, loss = 0.15491659\n",
      "Iteration 202, loss = 0.17594230\n",
      "Iteration 203, loss = 0.15827037\n",
      "Iteration 204, loss = 0.15751604\n",
      "Iteration 205, loss = 0.15723575\n",
      "Iteration 206, loss = 0.15339859\n",
      "Iteration 207, loss = 0.16617891\n",
      "Iteration 208, loss = 0.14787141\n",
      "Iteration 209, loss = 0.15405218\n",
      "Iteration 210, loss = 0.15249149\n",
      "Iteration 211, loss = 0.16086449\n",
      "Iteration 212, loss = 0.14922089\n",
      "Iteration 213, loss = 0.15818286\n",
      "Iteration 214, loss = 0.15782711\n",
      "Iteration 215, loss = 0.14493853\n",
      "Iteration 216, loss = 0.15065706\n",
      "Iteration 217, loss = 0.14269890\n",
      "Iteration 218, loss = 0.14694617\n",
      "Iteration 219, loss = 0.14320436\n",
      "Iteration 220, loss = 0.13916475\n",
      "Iteration 221, loss = 0.14032328\n",
      "Iteration 222, loss = 0.13909060\n",
      "Iteration 223, loss = 0.13783034\n",
      "Iteration 224, loss = 0.13630276\n",
      "Iteration 225, loss = 0.13712127\n",
      "Iteration 226, loss = 0.14384860\n",
      "Iteration 227, loss = 0.13257186\n",
      "Iteration 228, loss = 0.15485612\n",
      "Iteration 229, loss = 0.13149940\n",
      "Iteration 230, loss = 0.17362158\n",
      "Iteration 231, loss = 0.13151214\n",
      "Iteration 232, loss = 0.16656127\n",
      "Iteration 233, loss = 0.13911596\n",
      "Iteration 234, loss = 0.15016451\n",
      "Iteration 235, loss = 0.14464740\n",
      "Iteration 236, loss = 0.13592007\n",
      "Iteration 237, loss = 0.13480324\n",
      "Iteration 238, loss = 0.13219875\n",
      "Iteration 239, loss = 0.13884423\n",
      "Iteration 240, loss = 0.12925857\n",
      "Iteration 241, loss = 0.15053856\n",
      "Iteration 242, loss = 0.13241526\n",
      "Iteration 243, loss = 0.13495416\n",
      "Iteration 244, loss = 0.12255471\n",
      "Iteration 245, loss = 0.12440465\n",
      "Iteration 246, loss = 0.12107366\n",
      "Iteration 247, loss = 0.12674982\n",
      "Iteration 248, loss = 0.11917575\n",
      "Iteration 249, loss = 0.13064229\n",
      "Iteration 250, loss = 0.11921068\n",
      "Iteration 251, loss = 0.13849039\n",
      "Iteration 252, loss = 0.13052483\n",
      "Iteration 253, loss = 0.12438270\n",
      "Iteration 254, loss = 0.12175693\n",
      "Iteration 255, loss = 0.11721615\n",
      "Iteration 256, loss = 0.11406695\n",
      "Iteration 257, loss = 0.12210539\n",
      "Iteration 258, loss = 0.11702159\n",
      "Iteration 259, loss = 0.11263909\n",
      "Iteration 260, loss = 0.11568653\n",
      "Iteration 261, loss = 0.11408955\n",
      "Iteration 262, loss = 0.11024768\n",
      "Iteration 263, loss = 0.11139739\n",
      "Iteration 264, loss = 0.10763348\n",
      "Iteration 265, loss = 0.11647206\n",
      "Iteration 266, loss = 0.11310023\n",
      "Iteration 267, loss = 0.10863726\n",
      "Iteration 268, loss = 0.10841407\n",
      "Iteration 269, loss = 0.10501573\n",
      "Iteration 270, loss = 0.10231990\n",
      "Iteration 271, loss = 0.10361562\n",
      "Iteration 272, loss = 0.10999885\n",
      "Iteration 273, loss = 0.11605427\n",
      "Iteration 274, loss = 0.10925296\n",
      "Iteration 275, loss = 0.10690204\n",
      "Iteration 276, loss = 0.10906611\n",
      "Iteration 277, loss = 0.11006039\n",
      "Iteration 278, loss = 0.10183987\n",
      "Iteration 279, loss = 0.10918653\n",
      "Iteration 280, loss = 0.09753913\n",
      "Iteration 281, loss = 0.11828798\n",
      "Iteration 282, loss = 0.10260360\n",
      "Iteration 283, loss = 0.09925102\n",
      "Iteration 284, loss = 0.09826846\n",
      "Iteration 285, loss = 0.09655431\n",
      "Iteration 286, loss = 0.10276406\n",
      "Iteration 287, loss = 0.10397143\n",
      "Iteration 288, loss = 0.09549325\n",
      "Iteration 289, loss = 0.09214328\n",
      "Iteration 290, loss = 0.10318599\n",
      "Iteration 291, loss = 0.09187310\n",
      "Iteration 292, loss = 0.10473231\n",
      "Iteration 293, loss = 0.09450261\n",
      "Iteration 294, loss = 0.09919153\n",
      "Iteration 295, loss = 0.09120696\n",
      "Iteration 296, loss = 0.08835425\n",
      "Iteration 297, loss = 0.08794339\n",
      "Iteration 298, loss = 0.08577874\n",
      "Iteration 299, loss = 0.08670173\n",
      "Iteration 300, loss = 0.08603121\n",
      "Iteration 301, loss = 0.08555486\n",
      "Iteration 302, loss = 0.08309039\n",
      "Iteration 303, loss = 0.08306278\n",
      "Iteration 304, loss = 0.08451201\n",
      "Iteration 305, loss = 0.08571938\n",
      "Iteration 306, loss = 0.08488104\n",
      "Iteration 307, loss = 0.08537883\n",
      "Iteration 308, loss = 0.08433159\n",
      "Iteration 309, loss = 0.08005371\n",
      "Iteration 310, loss = 0.08146229\n",
      "Iteration 311, loss = 0.09509288\n",
      "Iteration 312, loss = 0.08111523\n",
      "Iteration 313, loss = 0.09354722\n",
      "Iteration 314, loss = 0.07580711\n",
      "Iteration 315, loss = 0.09866907\n",
      "Iteration 316, loss = 0.08103461\n",
      "Iteration 317, loss = 0.08774394\n",
      "Iteration 318, loss = 0.07886135\n",
      "Iteration 319, loss = 0.07934850\n",
      "Iteration 320, loss = 0.07607806\n",
      "Iteration 321, loss = 0.07576768\n",
      "Iteration 322, loss = 0.07522036\n",
      "Iteration 323, loss = 0.07633351\n",
      "Iteration 324, loss = 0.07492994\n",
      "Iteration 325, loss = 0.07575626\n",
      "Iteration 326, loss = 0.07232925\n",
      "Iteration 327, loss = 0.07115528\n",
      "Iteration 328, loss = 0.07187664\n",
      "Iteration 329, loss = 0.06954244\n",
      "Iteration 330, loss = 0.06963405\n",
      "Iteration 331, loss = 0.06847157\n",
      "Iteration 332, loss = 0.07337136\n",
      "Iteration 333, loss = 0.07137164\n",
      "Iteration 334, loss = 0.06908759\n",
      "Iteration 335, loss = 0.07037590\n",
      "Iteration 336, loss = 0.06980631\n",
      "Iteration 337, loss = 0.06633656\n",
      "Iteration 338, loss = 0.06500458\n",
      "Iteration 339, loss = 0.06501620\n",
      "Iteration 340, loss = 0.06524648\n",
      "Iteration 341, loss = 0.06606288\n",
      "Iteration 342, loss = 0.06460393\n",
      "Iteration 343, loss = 0.06502457\n",
      "Iteration 344, loss = 0.06321016\n",
      "Iteration 345, loss = 0.06353755\n",
      "Iteration 346, loss = 0.06399526\n",
      "Iteration 347, loss = 0.06286892\n",
      "Iteration 348, loss = 0.06279783\n",
      "Iteration 349, loss = 0.06117724\n",
      "Iteration 350, loss = 0.06019012\n",
      "Iteration 351, loss = 0.06170870\n",
      "Iteration 352, loss = 0.05920142\n",
      "Iteration 353, loss = 0.06317017\n",
      "Iteration 354, loss = 0.06015724\n",
      "Iteration 355, loss = 0.06095892\n",
      "Iteration 356, loss = 0.06850456\n",
      "Iteration 357, loss = 0.05883405\n",
      "Iteration 358, loss = 0.06774841\n",
      "Iteration 359, loss = 0.05994882\n",
      "Iteration 360, loss = 0.05865978\n",
      "Iteration 361, loss = 0.05640268\n",
      "Iteration 362, loss = 0.06174752\n",
      "Iteration 363, loss = 0.05780117\n",
      "Iteration 364, loss = 0.05706705\n",
      "Iteration 365, loss = 0.05612487\n",
      "Iteration 366, loss = 0.05886140\n",
      "Iteration 367, loss = 0.05837250\n",
      "Iteration 368, loss = 0.05831047\n",
      "Iteration 369, loss = 0.05675988\n",
      "Iteration 370, loss = 0.05826464\n",
      "Iteration 371, loss = 0.05638386\n",
      "Iteration 372, loss = 0.05608859\n",
      "Iteration 373, loss = 0.06096118\n",
      "Iteration 374, loss = 0.06171046\n",
      "Iteration 375, loss = 0.05484262\n",
      "Iteration 376, loss = 0.05798457\n",
      "Iteration 377, loss = 0.05336854\n",
      "Iteration 378, loss = 0.05228961\n",
      "Iteration 379, loss = 0.05546275\n",
      "Iteration 380, loss = 0.05525479\n",
      "Iteration 381, loss = 0.05174325\n",
      "Iteration 382, loss = 0.05867240\n",
      "Iteration 383, loss = 0.05134373\n",
      "Iteration 384, loss = 0.05235047\n",
      "Iteration 385, loss = 0.05026747\n",
      "Iteration 386, loss = 0.05018786\n",
      "Iteration 387, loss = 0.05255335\n",
      "Iteration 388, loss = 0.05603982\n",
      "Iteration 389, loss = 0.04789646\n",
      "Iteration 390, loss = 0.05492220\n",
      "Iteration 391, loss = 0.05005087\n",
      "Iteration 392, loss = 0.05601798\n",
      "Iteration 393, loss = 0.04883011\n",
      "Iteration 394, loss = 0.05288347\n",
      "Iteration 395, loss = 0.04788006\n",
      "Iteration 396, loss = 0.05266742\n",
      "Iteration 397, loss = 0.05208916\n",
      "Iteration 398, loss = 0.05008281\n",
      "Iteration 399, loss = 0.04974430\n",
      "Iteration 400, loss = 0.04868394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(200, 100, 50),\n",
       "              max_iter=1000, random_state=42, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(200,100,50,), activation='logistic', max_iter=1000, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8064516129032258"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "Verificamos uma melhora de 82 para 86% na accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n    Abnormal       0.86      0.86      0.86        44\n      Normal       0.67      0.67      0.67        18\n\n    accuracy                           0.81        62\n   macro avg       0.77      0.77      0.77        62\nweighted avg       0.81      0.81      0.81        62\n\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "Ao dobrar os valores da hidden_layer_sizes nós tivemos um aumento de pouco mais de 1% na acurácia, porém tivemos uma piora de precision tanto para os casos anormais quanto para os casos normais, enquanto o recall melhorou para os casos anormais e piorou para os casos normais."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Abnormal    210\n",
       "Normal      100\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o banco de dados é desbalanceado, visto o número de casos anormais e normais, o valor da acurácia não é um boa métrica para avaliar a MLP, portanto, deve-se atentar as métricas *precision* e *recall* do *report*, pois são elas que nos dirão se o MLP está classificando corretamente os casos. *Precision* se diz respeito a porcentagem dos casos selecionados daquela classe que foram classificados corretamente, já *recall* significa a porcentagem dos casos totais daquela classe que seriam classificados corretamente.\r\n",
    "\r\n",
    "Vamos agora alterar alguns dos parâmetros da MLP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.67024566\n",
      "Iteration 2, loss = 0.64236872\n",
      "Iteration 3, loss = 0.63358321\n",
      "Iteration 4, loss = 0.63752815\n",
      "Iteration 5, loss = 0.64198806\n",
      "Iteration 6, loss = 0.64131365\n",
      "Iteration 7, loss = 0.63844617\n",
      "Iteration 8, loss = 0.63671240\n",
      "Iteration 9, loss = 0.63381061\n",
      "Iteration 10, loss = 0.62967288\n",
      "Iteration 11, loss = 0.62449397\n",
      "Iteration 12, loss = 0.62125073\n",
      "Iteration 13, loss = 0.61740508\n",
      "Iteration 14, loss = 0.61322855\n",
      "Iteration 15, loss = 0.60866736\n",
      "Iteration 16, loss = 0.60480839\n",
      "Iteration 17, loss = 0.59977024\n",
      "Iteration 18, loss = 0.59384897\n",
      "Iteration 19, loss = 0.58738872\n",
      "Iteration 20, loss = 0.58148577\n",
      "Iteration 21, loss = 0.57477812\n",
      "Iteration 22, loss = 0.56644647\n",
      "Iteration 23, loss = 0.55708925\n",
      "Iteration 24, loss = 0.54686711\n",
      "Iteration 25, loss = 0.53596548\n",
      "Iteration 26, loss = 0.52380761\n",
      "Iteration 27, loss = 0.51139480\n",
      "Iteration 28, loss = 0.49765665\n",
      "Iteration 29, loss = 0.48454518\n",
      "Iteration 30, loss = 0.47046950\n",
      "Iteration 31, loss = 0.45711939\n",
      "Iteration 32, loss = 0.44414121\n",
      "Iteration 33, loss = 0.43211391\n",
      "Iteration 34, loss = 0.42241119\n",
      "Iteration 35, loss = 0.41369596\n",
      "Iteration 36, loss = 0.40481297\n",
      "Iteration 37, loss = 0.39485241\n",
      "Iteration 38, loss = 0.38720654\n",
      "Iteration 39, loss = 0.38215598\n",
      "Iteration 40, loss = 0.37458809\n",
      "Iteration 41, loss = 0.37377357\n",
      "Iteration 42, loss = 0.37447289\n",
      "Iteration 43, loss = 0.36849841\n",
      "Iteration 44, loss = 0.36075904\n",
      "Iteration 45, loss = 0.35493997\n",
      "Iteration 46, loss = 0.35190238\n",
      "Iteration 47, loss = 0.35158336\n",
      "Iteration 48, loss = 0.35354267\n",
      "Iteration 49, loss = 0.35949555\n",
      "Iteration 50, loss = 0.35255346\n",
      "Iteration 51, loss = 0.34055287\n",
      "Iteration 52, loss = 0.33660056\n",
      "Iteration 53, loss = 0.33427608\n",
      "Iteration 54, loss = 0.33197081\n",
      "Iteration 55, loss = 0.33171360\n",
      "Iteration 56, loss = 0.32906745\n",
      "Iteration 57, loss = 0.32639709\n",
      "Iteration 58, loss = 0.32833264\n",
      "Iteration 59, loss = 0.32914013\n",
      "Iteration 60, loss = 0.32446919\n",
      "Iteration 61, loss = 0.31874489\n",
      "Iteration 62, loss = 0.32129641\n",
      "Iteration 63, loss = 0.32337271\n",
      "Iteration 64, loss = 0.31726224\n",
      "Iteration 65, loss = 0.31358005\n",
      "Iteration 66, loss = 0.31365186\n",
      "Iteration 67, loss = 0.31270927\n",
      "Iteration 68, loss = 0.31008529\n",
      "Iteration 69, loss = 0.31268301\n",
      "Iteration 70, loss = 0.31829610\n",
      "Iteration 71, loss = 0.31088978\n",
      "Iteration 72, loss = 0.30500684\n",
      "Iteration 73, loss = 0.30475108\n",
      "Iteration 74, loss = 0.30552584\n",
      "Iteration 75, loss = 0.30390858\n",
      "Iteration 76, loss = 0.29983461\n",
      "Iteration 77, loss = 0.30854380\n",
      "Iteration 78, loss = 0.30857598\n",
      "Iteration 79, loss = 0.29786029\n",
      "Iteration 80, loss = 0.30846892\n",
      "Iteration 81, loss = 0.31191556\n",
      "Iteration 82, loss = 0.30402747\n",
      "Iteration 83, loss = 0.29810228\n",
      "Iteration 84, loss = 0.29572199\n",
      "Iteration 85, loss = 0.29644856\n",
      "Iteration 86, loss = 0.29577515\n",
      "Iteration 87, loss = 0.29361203\n",
      "Iteration 88, loss = 0.29469129\n",
      "Iteration 89, loss = 0.28734426\n",
      "Iteration 90, loss = 0.28586577\n",
      "Iteration 91, loss = 0.28543620\n",
      "Iteration 92, loss = 0.28252988\n",
      "Iteration 93, loss = 0.28063635\n",
      "Iteration 94, loss = 0.28284463\n",
      "Iteration 95, loss = 0.28218077\n",
      "Iteration 96, loss = 0.27661870\n",
      "Iteration 97, loss = 0.27816674\n",
      "Iteration 98, loss = 0.27664981\n",
      "Iteration 99, loss = 0.27187109\n",
      "Iteration 100, loss = 0.27186173\n",
      "Iteration 101, loss = 0.27457031\n",
      "Iteration 102, loss = 0.28117100\n",
      "Iteration 103, loss = 0.27541688\n",
      "Iteration 104, loss = 0.26583020\n",
      "Iteration 105, loss = 0.26493631\n",
      "Iteration 106, loss = 0.26483307\n",
      "Iteration 107, loss = 0.26317281\n",
      "Iteration 108, loss = 0.26183482\n",
      "Iteration 109, loss = 0.26363229\n",
      "Iteration 110, loss = 0.26116329\n",
      "Iteration 111, loss = 0.25741226\n",
      "Iteration 112, loss = 0.25577393\n",
      "Iteration 113, loss = 0.25489619\n",
      "Iteration 114, loss = 0.25446979\n",
      "Iteration 115, loss = 0.25677384\n",
      "Iteration 116, loss = 0.26043782\n",
      "Iteration 117, loss = 0.25689815\n",
      "Iteration 118, loss = 0.25061573\n",
      "Iteration 119, loss = 0.24934559\n",
      "Iteration 120, loss = 0.24854298\n",
      "Iteration 121, loss = 0.26715791\n",
      "Iteration 122, loss = 0.26362106\n",
      "Iteration 123, loss = 0.24981820\n",
      "Iteration 124, loss = 0.24719449\n",
      "Iteration 125, loss = 0.24472929\n",
      "Iteration 126, loss = 0.24054983\n",
      "Iteration 127, loss = 0.23696444\n",
      "Iteration 128, loss = 0.23705244\n",
      "Iteration 129, loss = 0.23895408\n",
      "Iteration 130, loss = 0.23586838\n",
      "Iteration 131, loss = 0.23339892\n",
      "Iteration 132, loss = 0.23305961\n",
      "Iteration 133, loss = 0.23494463\n",
      "Iteration 134, loss = 0.22907229\n",
      "Iteration 135, loss = 0.23074045\n",
      "Iteration 136, loss = 0.23317131\n",
      "Iteration 137, loss = 0.22529112\n",
      "Iteration 138, loss = 0.22711798\n",
      "Iteration 139, loss = 0.24327139\n",
      "Iteration 140, loss = 0.26008651\n",
      "Iteration 141, loss = 0.24406218\n",
      "Iteration 142, loss = 0.22111835\n",
      "Iteration 143, loss = 0.24884821\n",
      "Iteration 144, loss = 0.23472778\n",
      "Iteration 145, loss = 0.21916253\n",
      "Iteration 146, loss = 0.23636469\n",
      "Iteration 147, loss = 0.26649090\n",
      "Iteration 148, loss = 0.26007082\n",
      "Iteration 149, loss = 0.22300002\n",
      "Iteration 150, loss = 0.22476134\n",
      "Iteration 151, loss = 0.24393553\n",
      "Iteration 152, loss = 0.23384532\n",
      "Iteration 153, loss = 0.21819655\n",
      "Iteration 154, loss = 0.21147202\n",
      "Iteration 155, loss = 0.21198585\n",
      "Iteration 156, loss = 0.22543029\n",
      "Iteration 157, loss = 0.22170698\n",
      "Iteration 158, loss = 0.20660880\n",
      "Iteration 159, loss = 0.22895873\n",
      "Iteration 160, loss = 0.23542523\n",
      "Iteration 161, loss = 0.20659679\n",
      "Iteration 162, loss = 0.22600608\n",
      "Iteration 163, loss = 0.24723961\n",
      "Iteration 164, loss = 0.23217960\n",
      "Iteration 165, loss = 0.22932027\n",
      "Iteration 166, loss = 0.23047723\n",
      "Iteration 167, loss = 0.21566429\n",
      "Iteration 168, loss = 0.20811212\n",
      "Iteration 169, loss = 0.20450713\n",
      "Iteration 170, loss = 0.20301730\n",
      "Iteration 171, loss = 0.21241240\n",
      "Iteration 172, loss = 0.23158744\n",
      "Iteration 173, loss = 0.21554936\n",
      "Iteration 174, loss = 0.23075900\n",
      "Iteration 175, loss = 0.25098343\n",
      "Iteration 176, loss = 0.21128468\n",
      "Iteration 177, loss = 0.20715243\n",
      "Iteration 178, loss = 0.22171646\n",
      "Iteration 179, loss = 0.21043135\n",
      "Iteration 180, loss = 0.19725456\n",
      "Iteration 181, loss = 0.20109086\n",
      "Iteration 182, loss = 0.20483339\n",
      "Iteration 183, loss = 0.20381576\n",
      "Iteration 184, loss = 0.20018604\n",
      "Iteration 185, loss = 0.19611873\n",
      "Iteration 186, loss = 0.20969820\n",
      "Iteration 187, loss = 0.21841460\n",
      "Iteration 188, loss = 0.21121357\n",
      "Iteration 189, loss = 0.19613876\n",
      "Iteration 190, loss = 0.19105901\n",
      "Iteration 191, loss = 0.19200615\n",
      "Iteration 192, loss = 0.19045694\n",
      "Iteration 193, loss = 0.19116287\n",
      "Iteration 194, loss = 0.18788793\n",
      "Iteration 195, loss = 0.19096162\n",
      "Iteration 196, loss = 0.24153851\n",
      "Iteration 197, loss = 0.26291606\n",
      "Iteration 198, loss = 0.22608775\n",
      "Iteration 199, loss = 0.19139051\n",
      "Iteration 200, loss = 0.19117783\n",
      "Iteration 201, loss = 0.20092839\n",
      "Iteration 202, loss = 0.18662927\n",
      "Iteration 203, loss = 0.18193755\n",
      "Iteration 204, loss = 0.19064188\n",
      "Iteration 205, loss = 0.18408937\n",
      "Iteration 206, loss = 0.18660740\n",
      "Iteration 207, loss = 0.20537486\n",
      "Iteration 208, loss = 0.20866704\n",
      "Iteration 209, loss = 0.18307107\n",
      "Iteration 210, loss = 0.18291656\n",
      "Iteration 211, loss = 0.20567167\n",
      "Iteration 212, loss = 0.20265396\n",
      "Iteration 213, loss = 0.18026646\n",
      "Iteration 214, loss = 0.17578971\n",
      "Iteration 215, loss = 0.17251627\n",
      "Iteration 216, loss = 0.17510259\n",
      "Iteration 217, loss = 0.17842165\n",
      "Iteration 218, loss = 0.17182225\n",
      "Iteration 219, loss = 0.17481571\n",
      "Iteration 220, loss = 0.18136746\n",
      "Iteration 221, loss = 0.17217197\n",
      "Iteration 222, loss = 0.17893204\n",
      "Iteration 223, loss = 0.19350497\n",
      "Iteration 224, loss = 0.17307521\n",
      "Iteration 225, loss = 0.17633748\n",
      "Iteration 226, loss = 0.18700343\n",
      "Iteration 227, loss = 0.16402798\n",
      "Iteration 228, loss = 0.18464504\n",
      "Iteration 229, loss = 0.17965450\n",
      "Iteration 230, loss = 0.16831208\n",
      "Iteration 231, loss = 0.19973840\n",
      "Iteration 232, loss = 0.17830448\n",
      "Iteration 233, loss = 0.16163847\n",
      "Iteration 234, loss = 0.17616916\n",
      "Iteration 235, loss = 0.17961617\n",
      "Iteration 236, loss = 0.16540951\n",
      "Iteration 237, loss = 0.16696613\n",
      "Iteration 238, loss = 0.18303738\n",
      "Iteration 239, loss = 0.15983442\n",
      "Iteration 240, loss = 0.17497342\n",
      "Iteration 241, loss = 0.18738032\n",
      "Iteration 242, loss = 0.15644612\n",
      "Iteration 243, loss = 0.19480859\n",
      "Iteration 244, loss = 0.20607216\n",
      "Iteration 245, loss = 0.15217180\n",
      "Iteration 246, loss = 0.19281846\n",
      "Iteration 247, loss = 0.20705564\n",
      "Iteration 248, loss = 0.16277827\n",
      "Iteration 249, loss = 0.17369294\n",
      "Iteration 250, loss = 0.19860721\n",
      "Iteration 251, loss = 0.16888102\n",
      "Iteration 252, loss = 0.15171193\n",
      "Iteration 253, loss = 0.16780495\n",
      "Iteration 254, loss = 0.16919537\n",
      "Iteration 255, loss = 0.15923163\n",
      "Iteration 256, loss = 0.20006713\n",
      "Iteration 257, loss = 0.21332145\n",
      "Iteration 258, loss = 0.17223000\n",
      "Iteration 259, loss = 0.15599643\n",
      "Iteration 260, loss = 0.15065969\n",
      "Iteration 261, loss = 0.15093591\n",
      "Iteration 262, loss = 0.15826805\n",
      "Iteration 263, loss = 0.16155292\n",
      "Iteration 264, loss = 0.15523650\n",
      "Iteration 265, loss = 0.14954454\n",
      "Iteration 266, loss = 0.14759390\n",
      "Iteration 267, loss = 0.14680058\n",
      "Iteration 268, loss = 0.15893078\n",
      "Iteration 269, loss = 0.19670839\n",
      "Iteration 270, loss = 0.19243587\n",
      "Iteration 271, loss = 0.15436657\n",
      "Iteration 272, loss = 0.15461674\n",
      "Iteration 273, loss = 0.16707044\n",
      "Iteration 274, loss = 0.14877671\n",
      "Iteration 275, loss = 0.14257209\n",
      "Iteration 276, loss = 0.18102384\n",
      "Iteration 277, loss = 0.17339780\n",
      "Iteration 278, loss = 0.14315032\n",
      "Iteration 279, loss = 0.15899480\n",
      "Iteration 280, loss = 0.15986633\n",
      "Iteration 281, loss = 0.14182902\n",
      "Iteration 282, loss = 0.17174288\n",
      "Iteration 283, loss = 0.18372044\n",
      "Iteration 284, loss = 0.16291409\n",
      "Iteration 285, loss = 0.16134849\n",
      "Iteration 286, loss = 0.16612368\n",
      "Iteration 287, loss = 0.15816459\n",
      "Iteration 288, loss = 0.14709318\n",
      "Iteration 289, loss = 0.14628587\n",
      "Iteration 290, loss = 0.14265556\n",
      "Iteration 291, loss = 0.13969749\n",
      "Iteration 292, loss = 0.14202432\n",
      "Iteration 293, loss = 0.14797278\n",
      "Iteration 294, loss = 0.15841847\n",
      "Iteration 295, loss = 0.14820719\n",
      "Iteration 296, loss = 0.13551519\n",
      "Iteration 297, loss = 0.13570970\n",
      "Iteration 298, loss = 0.13095813\n",
      "Iteration 299, loss = 0.12857607\n",
      "Iteration 300, loss = 0.14207464\n",
      "Iteration 301, loss = 0.13753373\n",
      "Iteration 302, loss = 0.12593093\n",
      "Iteration 303, loss = 0.19005112\n",
      "Iteration 304, loss = 0.16874898\n",
      "Iteration 305, loss = 0.12371783\n",
      "Iteration 306, loss = 0.14892931\n",
      "Iteration 307, loss = 0.14400256\n",
      "Iteration 308, loss = 0.12947243\n",
      "Iteration 309, loss = 0.15104412\n",
      "Iteration 310, loss = 0.15077675\n",
      "Iteration 311, loss = 0.13132314\n",
      "Iteration 312, loss = 0.13357965\n",
      "Iteration 313, loss = 0.13567604\n",
      "Iteration 314, loss = 0.12921566\n",
      "Iteration 315, loss = 0.12562478\n",
      "Iteration 316, loss = 0.12180074\n",
      "Iteration 317, loss = 0.12221881\n",
      "Iteration 318, loss = 0.14613983\n",
      "Iteration 319, loss = 0.17780155\n",
      "Iteration 320, loss = 0.15008002\n",
      "Iteration 321, loss = 0.12015837\n",
      "Iteration 322, loss = 0.13000215\n",
      "Iteration 323, loss = 0.14738857\n",
      "Iteration 324, loss = 0.14887605\n",
      "Iteration 325, loss = 0.12687161\n",
      "Iteration 326, loss = 0.11592854\n",
      "Iteration 327, loss = 0.12025244\n",
      "Iteration 328, loss = 0.12749226\n",
      "Iteration 329, loss = 0.11931852\n",
      "Iteration 330, loss = 0.11231684\n",
      "Iteration 331, loss = 0.12667387\n",
      "Iteration 332, loss = 0.13868484\n",
      "Iteration 333, loss = 0.12253011\n",
      "Iteration 334, loss = 0.11829913\n",
      "Iteration 335, loss = 0.14360470\n",
      "Iteration 336, loss = 0.13706122\n",
      "Iteration 337, loss = 0.11075607\n",
      "Iteration 338, loss = 0.11391060\n",
      "Iteration 339, loss = 0.13777999\n",
      "Iteration 340, loss = 0.13771850\n",
      "Iteration 341, loss = 0.11148762\n",
      "Iteration 342, loss = 0.11892553\n",
      "Iteration 343, loss = 0.11797576\n",
      "Iteration 344, loss = 0.10765173\n",
      "Iteration 345, loss = 0.12747967\n",
      "Iteration 346, loss = 0.13851190\n",
      "Iteration 347, loss = 0.11841665\n",
      "Iteration 348, loss = 0.10258455\n",
      "Iteration 349, loss = 0.11164886\n",
      "Iteration 350, loss = 0.10394725\n",
      "Iteration 351, loss = 0.10897640\n",
      "Iteration 352, loss = 0.11704498\n",
      "Iteration 353, loss = 0.10211626\n",
      "Iteration 354, loss = 0.12201422\n",
      "Iteration 355, loss = 0.12218535\n",
      "Iteration 356, loss = 0.10158347\n",
      "Iteration 357, loss = 0.11778781\n",
      "Iteration 358, loss = 0.13764462\n",
      "Iteration 359, loss = 0.10829652\n",
      "Iteration 360, loss = 0.10170979\n",
      "Iteration 361, loss = 0.12308337\n",
      "Iteration 362, loss = 0.10547240\n",
      "Iteration 363, loss = 0.09643481\n",
      "Iteration 364, loss = 0.10049333\n",
      "Iteration 365, loss = 0.09891753\n",
      "Iteration 366, loss = 0.09609193\n",
      "Iteration 367, loss = 0.09601778\n",
      "Iteration 368, loss = 0.09729446\n",
      "Iteration 369, loss = 0.09274330\n",
      "Iteration 370, loss = 0.13027165\n",
      "Iteration 371, loss = 0.13168841\n",
      "Iteration 372, loss = 0.09094637\n",
      "Iteration 373, loss = 0.12695647\n",
      "Iteration 374, loss = 0.13847785\n",
      "Iteration 375, loss = 0.11433216\n",
      "Iteration 376, loss = 0.09831711\n",
      "Iteration 377, loss = 0.09567814\n",
      "Iteration 378, loss = 0.09105540\n",
      "Iteration 379, loss = 0.09297632\n",
      "Iteration 380, loss = 0.09961925\n",
      "Iteration 381, loss = 0.09962248\n",
      "Iteration 382, loss = 0.09574332\n",
      "Iteration 383, loss = 0.09231624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8172043010752689\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.86      0.88      0.87        65\n",
      "      Normal       0.70      0.68      0.69        28\n",
      "\n",
      "    accuracy                           0.82        93\n",
      "   macro avg       0.78      0.78      0.78        93\n",
      "weighted avg       0.82      0.82      0.82        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=15)\r\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='logistic', max_iter=500, random_state=42, verbose=True)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "y_pred = clf.predict(x_test)\r\n",
    "print(accuracy_score(y_test, y_pred))\r\n",
    "report = classification_report(y_test, y_pred)\r\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode-se ver a acurácia, *precison* e o *recall* não mudaram muito.\r\n",
    "\r\n",
    "Vamos agora tentar descobrir quais são as melhores variáveis para utilizar no nosso banco de dados da MLP para tentar maximizar a precisão dos casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 481.79375 248.518125\" width=\"481.79375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-06T19:42:41.587378</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 481.79375 248.518125 \nL 481.79375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 139.79375 224.64 \nL 474.59375 224.64 \nL 474.59375 7.2 \nL 139.79375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 220.11 \nL 458.650893 220.11 \nL 458.650893 211.05 \nL 139.79375 211.05 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 201.99 \nL 309.887668 201.99 \nL 309.887668 192.93 \nL 139.79375 192.93 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 183.87 \nL 274.534931 183.87 \nL 274.534931 174.81 \nL 139.79375 174.81 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 165.75 \nL 266.389104 165.75 \nL 266.389104 156.69 \nL 139.79375 156.69 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 147.63 \nL 256.287928 147.63 \nL 256.287928 138.57 \nL 139.79375 138.57 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 129.51 \nL 250.946136 129.51 \nL 250.946136 120.45 \nL 139.79375 120.45 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 111.39 \nL 222.7283 111.39 \nL 222.7283 102.33 \nL 139.79375 102.33 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 93.27 \nL 220.442573 93.27 \nL 220.442573 84.21 \nL 139.79375 84.21 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 75.15 \nL 218.099273 75.15 \nL 218.099273 66.09 \nL 139.79375 66.09 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 57.03 \nL 215.941389 57.03 \nL 215.941389 47.97 \nL 139.79375 47.97 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 38.91 \nL 215.687837 38.91 \nL 215.687837 29.85 \nL 139.79375 29.85 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p27e29fee60)\" d=\"M 139.79375 20.79 \nL 212.30219 20.79 \nL 212.30219 11.73 \nL 139.79375 11.73 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m68ea9c8b78\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#m68ea9c8b78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.00 -->\n      <g transform=\"translate(128.660938 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"212.012411\" xlink:href=\"#m68ea9c8b78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.05 -->\n      <g transform=\"translate(200.879599 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.231072\" xlink:href=\"#m68ea9c8b78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.10 -->\n      <g transform=\"translate(273.09826 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.449733\" xlink:href=\"#m68ea9c8b78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.15 -->\n      <g transform=\"translate(345.316921 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"428.668394\" xlink:href=\"#m68ea9c8b78\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.20 -->\n      <g transform=\"translate(417.535582 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me81092e231\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"215.58\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- degree_spondylolisthesis -->\n      <g transform=\"translate(7.2 219.379219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 3263 -1063 \nL 3263 -1509 \nL -63 -1509 \nL -63 -1063 \nL 3263 -1063 \nz\n\" id=\"DejaVuSans-5f\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"125\" xlink:href=\"#DejaVuSans-67\"/>\n       <use x=\"188.476562\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"227.339844\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"288.863281\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"350.386719\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"400.386719\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"452.486328\" xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"515.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"577.144531\" xlink:href=\"#DejaVuSans-6e\"/>\n       <use x=\"640.523438\" xlink:href=\"#DejaVuSans-64\"/>\n       <use x=\"704\" xlink:href=\"#DejaVuSans-79\"/>\n       <use x=\"763.179688\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"790.962891\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"852.144531\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"879.927734\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"907.710938\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"959.810547\" xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"999.019531\" xlink:href=\"#DejaVuSans-68\"/>\n       <use x=\"1062.398438\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"1123.921875\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"1176.021484\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1203.804688\" xlink:href=\"#DejaVuSans-73\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"197.46\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- pelvic_radius -->\n      <g transform=\"translate(67.40625 201.259219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"125\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"152.783203\" xlink:href=\"#DejaVuSans-76\"/>\n       <use x=\"211.962891\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"239.746094\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"294.726562\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"344.726562\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"385.839844\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"447.119141\" xlink:href=\"#DejaVuSans-64\"/>\n       <use x=\"510.595703\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"538.378906\" xlink:href=\"#DejaVuSans-75\"/>\n       <use x=\"601.757812\" xlink:href=\"#DejaVuSans-73\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"179.34\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- pelvic_tilt -->\n      <g transform=\"translate(84.921875 183.139219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"125\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"152.783203\" xlink:href=\"#DejaVuSans-76\"/>\n       <use x=\"211.962891\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"239.746094\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"294.726562\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"344.726562\" xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"383.935547\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"411.71875\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"439.501953\" xlink:href=\"#DejaVuSans-74\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"161.22\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- pelvic_incidence -->\n      <g transform=\"translate(50.435938 165.019219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"125\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"152.783203\" xlink:href=\"#DejaVuSans-76\"/>\n       <use x=\"211.962891\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"239.746094\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"294.726562\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"344.726562\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"372.509766\" xlink:href=\"#DejaVuSans-6e\"/>\n       <use x=\"435.888672\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"490.869141\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"518.652344\" xlink:href=\"#DejaVuSans-64\"/>\n       <use x=\"582.128906\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"643.652344\" xlink:href=\"#DejaVuSans-6e\"/>\n       <use x=\"707.03125\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"762.011719\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- lumbar_lordosis_angle -->\n      <g transform=\"translate(21.107813 146.899219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" id=\"DejaVuSans-62\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-75\"/>\n       <use x=\"91.162109\" xlink:href=\"#DejaVuSans-6d\"/>\n       <use x=\"188.574219\" xlink:href=\"#DejaVuSans-62\"/>\n       <use x=\"252.050781\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"313.330078\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"354.443359\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"404.443359\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"432.226562\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"493.408203\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"532.771484\" xlink:href=\"#DejaVuSans-64\"/>\n       <use x=\"596.248047\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"657.429688\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"709.529297\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"737.3125\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"789.412109\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"839.412109\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"900.691406\" xlink:href=\"#DejaVuSans-6e\"/>\n       <use x=\"964.070312\" xlink:href=\"#DejaVuSans-67\"/>\n       <use x=\"1027.546875\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"1055.330078\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"124.98\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- sacral_slope -->\n      <g transform=\"translate(71.332813 128.779219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"113.378906\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"168.359375\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"209.472656\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"270.751953\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"298.535156\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"348.535156\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"400.634766\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"428.417969\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"489.599609\" xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"553.076172\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"106.86\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- cervical_tilt -->\n      <g transform=\"translate(75.532813 110.659219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"116.503906\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"157.617188\" xlink:href=\"#DejaVuSans-76\"/>\n       <use x=\"216.796875\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"244.580078\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"299.560547\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"360.839844\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"388.623047\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"438.623047\" xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"477.832031\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"505.615234\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"533.398438\" xlink:href=\"#DejaVuSans-74\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- sacrum_angle -->\n      <g transform=\"translate(63.023438 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"113.378906\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"168.359375\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"209.472656\" xlink:href=\"#DejaVuSans-75\"/>\n       <use x=\"272.851562\" xlink:href=\"#DejaVuSans-6d\"/>\n       <use x=\"370.263672\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"420.263672\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"481.542969\" xlink:href=\"#DejaVuSans-6e\"/>\n       <use x=\"544.921875\" xlink:href=\"#DejaVuSans-67\"/>\n       <use x=\"608.398438\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"636.181641\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"70.62\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- direct_tilt -->\n      <g transform=\"translate(85.8125 74.419219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-64\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"91.259766\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"130.123047\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"191.646484\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"246.626953\" xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"285.835938\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"335.835938\" xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"375.044922\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"402.828125\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"430.611328\" xlink:href=\"#DejaVuSans-74\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"52.5\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- scoliosis_slope -->\n      <g transform=\"translate(59.4875 56.299219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"107.080078\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"168.261719\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"196.044922\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"223.828125\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"285.009766\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"337.109375\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"364.892578\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"416.992188\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"466.992188\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"519.091797\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"546.875\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"608.056641\" xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"671.533203\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"34.38\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- thoracic_slope -->\n      <g transform=\"translate(60.795313 38.179219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-68\"/>\n       <use x=\"102.587891\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"163.769531\" xlink:href=\"#DejaVuSans-72\"/>\n       <use x=\"204.882812\" xlink:href=\"#DejaVuSans-61\"/>\n       <use x=\"266.162109\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"321.142578\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"348.925781\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"403.90625\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"453.90625\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"506.005859\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"533.789062\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"594.970703\" xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"658.447266\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"139.79375\" xlink:href=\"#me81092e231\" y=\"16.26\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- pelvic_slope -->\n      <g transform=\"translate(71.710938 20.059219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"125\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"152.783203\" xlink:href=\"#DejaVuSans-76\"/>\n       <use x=\"211.962891\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"239.746094\" xlink:href=\"#DejaVuSans-63\"/>\n       <use x=\"294.726562\" xlink:href=\"#DejaVuSans-5f\"/>\n       <use x=\"344.726562\" xlink:href=\"#DejaVuSans-73\"/>\n       <use x=\"396.826172\" xlink:href=\"#DejaVuSans-6c\"/>\n       <use x=\"424.609375\" xlink:href=\"#DejaVuSans-6f\"/>\n       <use x=\"485.791016\" xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"549.267578\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 139.79375 224.64 \nL 139.79375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 474.59375 224.64 \nL 474.59375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path d=\"M 139.79375 224.64 \nL 474.59375 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 139.79375 7.2 \nL 474.59375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p27e29fee60\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"139.79375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "model = ExtraTreesClassifier()\r\n",
    "model.fit(x,y)\r\n",
    "feat_importances = pd.Series(model.feature_importances_, index=x.columns)\r\n",
    "feat_importances.nlargest(12).plot(kind='barh')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O gráfico nos mostra em ordem cresente as variáveis que mais impactam na classificação dos casos, logo, vamos descartar as primeiras 6 e utilizar o resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data[['degree_spondylolisthesis', 'pelvic_radius', 'pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle', 'sacral_slope', 'class']]\r\n",
    "y = new_data['class']\r\n",
    "x = new_data.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar várias configurações de MLP para tentar descobrir uma possível boa implementação de MLP para o problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.70112488\n",
      "Iteration 2, loss = 0.68654367\n",
      "Iteration 3, loss = 0.67462953\n",
      "Iteration 4, loss = 0.66398732\n",
      "Iteration 5, loss = 0.65432369\n",
      "Iteration 6, loss = 0.64666128\n",
      "Iteration 7, loss = 0.64006787\n",
      "Iteration 8, loss = 0.63399689\n",
      "Iteration 9, loss = 0.63055873\n",
      "Iteration 10, loss = 0.62797376\n",
      "Iteration 11, loss = 0.62460442\n",
      "Iteration 12, loss = 0.62275161\n",
      "Iteration 13, loss = 0.62038554\n",
      "Iteration 14, loss = 0.61821419\n",
      "Iteration 15, loss = 0.61588486\n",
      "Iteration 16, loss = 0.61339693\n",
      "Iteration 17, loss = 0.61064566\n",
      "Iteration 18, loss = 0.60747443\n",
      "Iteration 19, loss = 0.60388243\n",
      "Iteration 20, loss = 0.60041992\n",
      "Iteration 21, loss = 0.59622385\n",
      "Iteration 22, loss = 0.59223184\n",
      "Iteration 23, loss = 0.58780858\n",
      "Iteration 24, loss = 0.58354370\n",
      "Iteration 25, loss = 0.57911310\n",
      "Iteration 26, loss = 0.57405196\n",
      "Iteration 27, loss = 0.56849718\n",
      "Iteration 28, loss = 0.56288560\n",
      "Iteration 29, loss = 0.55697413\n",
      "Iteration 30, loss = 0.55057633\n",
      "Iteration 31, loss = 0.54448569\n",
      "Iteration 32, loss = 0.53811159\n",
      "Iteration 33, loss = 0.53106572\n",
      "Iteration 34, loss = 0.52411610\n",
      "Iteration 35, loss = 0.51778440\n",
      "Iteration 36, loss = 0.51130501\n",
      "Iteration 37, loss = 0.50515926\n",
      "Iteration 38, loss = 0.49816449\n",
      "Iteration 39, loss = 0.49183443\n",
      "Iteration 40, loss = 0.48524561\n",
      "Iteration 41, loss = 0.47968679\n",
      "Iteration 42, loss = 0.47347388\n",
      "Iteration 43, loss = 0.46805733\n",
      "Iteration 44, loss = 0.46260430\n",
      "Iteration 45, loss = 0.45714279\n",
      "Iteration 46, loss = 0.45193546\n",
      "Iteration 47, loss = 0.44766095\n",
      "Iteration 48, loss = 0.44363047\n",
      "Iteration 49, loss = 0.43898995\n",
      "Iteration 50, loss = 0.43405920\n",
      "Iteration 51, loss = 0.42981216\n",
      "Iteration 52, loss = 0.42742184\n",
      "Iteration 53, loss = 0.42496039\n",
      "Iteration 54, loss = 0.42100689\n",
      "Iteration 55, loss = 0.41630392\n",
      "Iteration 56, loss = 0.41187032\n",
      "Iteration 57, loss = 0.40780880\n",
      "Iteration 58, loss = 0.40440459\n",
      "Iteration 59, loss = 0.40154529\n",
      "Iteration 60, loss = 0.39808506\n",
      "Iteration 61, loss = 0.39380677\n",
      "Iteration 62, loss = 0.38988509\n",
      "Iteration 63, loss = 0.38642564\n",
      "Iteration 64, loss = 0.38460458\n",
      "Iteration 65, loss = 0.38110084\n",
      "Iteration 66, loss = 0.37738284\n",
      "Iteration 67, loss = 0.37410958\n",
      "Iteration 68, loss = 0.37230612\n",
      "Iteration 69, loss = 0.36950297\n",
      "Iteration 70, loss = 0.36610974\n",
      "Iteration 71, loss = 0.36389075\n",
      "Iteration 72, loss = 0.36143406\n",
      "Iteration 73, loss = 0.35871754\n",
      "Iteration 74, loss = 0.35670458\n",
      "Iteration 75, loss = 0.35415202\n",
      "Iteration 76, loss = 0.35201505\n",
      "Iteration 77, loss = 0.35305481\n",
      "Iteration 78, loss = 0.35161599\n",
      "Iteration 79, loss = 0.34742726\n",
      "Iteration 80, loss = 0.34533980\n",
      "Iteration 81, loss = 0.34306861\n",
      "Iteration 82, loss = 0.34078462\n",
      "Iteration 83, loss = 0.33906057\n",
      "Iteration 84, loss = 0.33776752\n",
      "Iteration 85, loss = 0.33649036\n",
      "Iteration 86, loss = 0.33417684\n",
      "Iteration 87, loss = 0.33360679\n",
      "Iteration 88, loss = 0.33167432\n",
      "Iteration 89, loss = 0.33294221\n",
      "Iteration 90, loss = 0.33000509\n",
      "Iteration 91, loss = 0.32857104\n",
      "Iteration 92, loss = 0.33187048\n",
      "Iteration 93, loss = 0.33051837\n",
      "Iteration 94, loss = 0.32503512\n",
      "Iteration 95, loss = 0.32689083\n",
      "Iteration 96, loss = 0.32576996\n",
      "Iteration 97, loss = 0.32364078\n",
      "Iteration 98, loss = 0.32152637\n",
      "Iteration 99, loss = 0.32075225\n",
      "Iteration 100, loss = 0.31908345\n",
      "Iteration 101, loss = 0.31816447\n",
      "Iteration 102, loss = 0.31811329\n",
      "Iteration 103, loss = 0.31737727\n",
      "Iteration 104, loss = 0.31651803\n",
      "Iteration 105, loss = 0.31579968\n",
      "Iteration 106, loss = 0.31405146\n",
      "Iteration 107, loss = 0.31219800\n",
      "Iteration 108, loss = 0.31239064\n",
      "Iteration 109, loss = 0.31498540\n",
      "Iteration 110, loss = 0.31134574\n",
      "Iteration 111, loss = 0.31017341\n",
      "Iteration 112, loss = 0.31175318\n",
      "Iteration 113, loss = 0.31105353\n",
      "Iteration 114, loss = 0.30946540\n",
      "Iteration 115, loss = 0.30646436\n",
      "Iteration 116, loss = 0.30592355\n",
      "Iteration 117, loss = 0.30732394\n",
      "Iteration 118, loss = 0.30445815\n",
      "Iteration 119, loss = 0.30718655\n",
      "Iteration 120, loss = 0.31277634\n",
      "Iteration 121, loss = 0.30741135\n",
      "Iteration 122, loss = 0.30334864\n",
      "Iteration 123, loss = 0.31262080\n",
      "Iteration 124, loss = 0.30842764\n",
      "Iteration 125, loss = 0.30199377\n",
      "Iteration 126, loss = 0.30290003\n",
      "Iteration 127, loss = 0.30232287\n",
      "Iteration 128, loss = 0.30126379\n",
      "Iteration 129, loss = 0.29924708\n",
      "Iteration 130, loss = 0.29877108\n",
      "Iteration 131, loss = 0.29847307\n",
      "Iteration 132, loss = 0.29723635\n",
      "Iteration 133, loss = 0.29740598\n",
      "Iteration 134, loss = 0.30220243\n",
      "Iteration 135, loss = 0.30678844\n",
      "Iteration 136, loss = 0.30323434\n",
      "Iteration 137, loss = 0.29581629\n",
      "Iteration 138, loss = 0.29629711\n",
      "Iteration 139, loss = 0.30031973\n",
      "Iteration 140, loss = 0.29792082\n",
      "Iteration 141, loss = 0.29514623\n",
      "Iteration 142, loss = 0.29509560\n",
      "Iteration 143, loss = 0.29527023\n",
      "Iteration 144, loss = 0.29481485\n",
      "Iteration 145, loss = 0.29296593\n",
      "Iteration 146, loss = 0.29206664\n",
      "Iteration 147, loss = 0.29463578\n",
      "Iteration 148, loss = 0.29662734\n",
      "Iteration 149, loss = 0.29276591\n",
      "Iteration 150, loss = 0.28978085\n",
      "Iteration 151, loss = 0.29582532\n",
      "Iteration 152, loss = 0.29635370\n",
      "Iteration 153, loss = 0.29045943\n",
      "Iteration 154, loss = 0.28929681\n",
      "Iteration 155, loss = 0.28928769\n",
      "Iteration 156, loss = 0.28903461\n",
      "Iteration 157, loss = 0.28931271\n",
      "Iteration 158, loss = 0.28734193\n",
      "Iteration 159, loss = 0.28741942\n",
      "Iteration 160, loss = 0.28859170\n",
      "Iteration 161, loss = 0.28859938\n",
      "Iteration 162, loss = 0.28681390\n",
      "Iteration 163, loss = 0.28918379\n",
      "Iteration 164, loss = 0.29264862\n",
      "Iteration 165, loss = 0.28811253\n",
      "Iteration 166, loss = 0.28480047\n",
      "Iteration 167, loss = 0.28869833\n",
      "Iteration 168, loss = 0.28753252\n",
      "Iteration 169, loss = 0.28709871\n",
      "Iteration 170, loss = 0.29348830\n",
      "Iteration 171, loss = 0.29194352\n",
      "Iteration 172, loss = 0.28565775\n",
      "Iteration 173, loss = 0.28366250\n",
      "Iteration 174, loss = 0.28355281\n",
      "Iteration 175, loss = 0.28299608\n",
      "Iteration 176, loss = 0.28233627\n",
      "Iteration 177, loss = 0.28251805\n",
      "Iteration 178, loss = 0.28266048\n",
      "Iteration 179, loss = 0.28171563\n",
      "Iteration 180, loss = 0.28106880\n",
      "Iteration 181, loss = 0.28117819\n",
      "Iteration 182, loss = 0.28128479\n",
      "Iteration 183, loss = 0.28155307\n",
      "Iteration 184, loss = 0.28126959\n",
      "Iteration 185, loss = 0.28125090\n",
      "Iteration 186, loss = 0.28259111\n",
      "Iteration 187, loss = 0.28154599\n",
      "Iteration 188, loss = 0.27844685\n",
      "Iteration 189, loss = 0.28633036\n",
      "Iteration 190, loss = 0.28752058\n",
      "Iteration 191, loss = 0.28138361\n",
      "Iteration 192, loss = 0.27930210\n",
      "Iteration 193, loss = 0.27924162\n",
      "Iteration 194, loss = 0.27879221\n",
      "Iteration 195, loss = 0.27883743\n",
      "Iteration 196, loss = 0.27841118\n",
      "Iteration 197, loss = 0.27747031\n",
      "Iteration 198, loss = 0.27867328\n",
      "Iteration 199, loss = 0.28525136\n",
      "Iteration 200, loss = 0.28182662\n",
      "Iteration 201, loss = 0.27649134\n",
      "Iteration 202, loss = 0.27765980\n",
      "Iteration 203, loss = 0.28048443\n",
      "Iteration 204, loss = 0.27926567\n",
      "Iteration 205, loss = 0.27612817\n",
      "Iteration 206, loss = 0.27587365\n",
      "Iteration 207, loss = 0.27519576\n",
      "Iteration 208, loss = 0.27519808\n",
      "Iteration 209, loss = 0.27518244\n",
      "Iteration 210, loss = 0.27476691\n",
      "Iteration 211, loss = 0.27465349\n",
      "Iteration 212, loss = 0.27440317\n",
      "Iteration 213, loss = 0.27470404\n",
      "Iteration 214, loss = 0.27831494\n",
      "Iteration 215, loss = 0.27712761\n",
      "Iteration 216, loss = 0.27429838\n",
      "Iteration 217, loss = 0.27435637\n",
      "Iteration 218, loss = 0.27785733\n",
      "Iteration 219, loss = 0.27593161\n",
      "Iteration 220, loss = 0.27349791\n",
      "Iteration 221, loss = 0.27668000\n",
      "Iteration 222, loss = 0.27568157\n",
      "Iteration 223, loss = 0.27325671\n",
      "Iteration 224, loss = 0.27353539\n",
      "Iteration 225, loss = 0.27518689\n",
      "Iteration 226, loss = 0.27273518\n",
      "Iteration 227, loss = 0.27332726\n",
      "Iteration 228, loss = 0.27158721\n",
      "Iteration 229, loss = 0.27275026\n",
      "Iteration 230, loss = 0.28075934\n",
      "Iteration 231, loss = 0.28154636\n",
      "Iteration 232, loss = 0.27527946\n",
      "Iteration 233, loss = 0.27229015\n",
      "Iteration 234, loss = 0.27266224\n",
      "Iteration 235, loss = 0.27402005\n",
      "Iteration 236, loss = 0.27506797\n",
      "Iteration 237, loss = 0.27324194\n",
      "Iteration 238, loss = 0.27060753\n",
      "Iteration 239, loss = 0.27419563\n",
      "Iteration 240, loss = 0.27598958\n",
      "Iteration 241, loss = 0.27209619\n",
      "Iteration 242, loss = 0.26944400\n",
      "Iteration 243, loss = 0.26867497\n",
      "Iteration 244, loss = 0.27081427\n",
      "Iteration 245, loss = 0.26772460\n",
      "Iteration 246, loss = 0.27147257\n",
      "Iteration 247, loss = 0.27515305\n",
      "Iteration 248, loss = 0.27013896\n",
      "Iteration 249, loss = 0.26983637\n",
      "Iteration 250, loss = 0.27177308\n",
      "Iteration 251, loss = 0.26775432\n",
      "Iteration 252, loss = 0.26955826\n",
      "Iteration 253, loss = 0.27379331\n",
      "Iteration 254, loss = 0.27148566\n",
      "Iteration 255, loss = 0.26780749\n",
      "Iteration 256, loss = 0.26740047\n",
      "Iteration 257, loss = 0.26803994\n",
      "Iteration 258, loss = 0.26793394\n",
      "Iteration 259, loss = 0.26807124\n",
      "Iteration 260, loss = 0.27022110\n",
      "Iteration 261, loss = 0.26706623\n",
      "Iteration 262, loss = 0.27046087\n",
      "Iteration 263, loss = 0.27192547\n",
      "Iteration 264, loss = 0.26671889\n",
      "Iteration 265, loss = 0.26596023\n",
      "Iteration 266, loss = 0.27011327\n",
      "Iteration 267, loss = 0.26599455\n",
      "Iteration 268, loss = 0.26957231\n",
      "Iteration 269, loss = 0.26987775\n",
      "Iteration 270, loss = 0.26703637\n",
      "Iteration 271, loss = 0.26507182\n",
      "Iteration 272, loss = 0.26372354\n",
      "Iteration 273, loss = 0.26723658\n",
      "Iteration 274, loss = 0.27048470\n",
      "Iteration 275, loss = 0.26673819\n",
      "Iteration 276, loss = 0.26359953\n",
      "Iteration 277, loss = 0.26666838\n",
      "Iteration 278, loss = 0.26967196\n",
      "Iteration 279, loss = 0.26700197\n",
      "Iteration 280, loss = 0.26420862\n",
      "Iteration 281, loss = 0.26265212\n",
      "Iteration 282, loss = 0.26503822\n",
      "Iteration 283, loss = 0.26476081\n",
      "Iteration 284, loss = 0.26255324\n",
      "Iteration 285, loss = 0.26308273\n",
      "Iteration 286, loss = 0.26603424\n",
      "Iteration 287, loss = 0.26411249\n",
      "Iteration 288, loss = 0.26151340\n",
      "Iteration 289, loss = 0.26412177\n",
      "Iteration 290, loss = 0.26362015\n",
      "Iteration 291, loss = 0.26192119\n",
      "Iteration 292, loss = 0.26211480\n",
      "Iteration 293, loss = 0.26601043\n",
      "Iteration 294, loss = 0.26314922\n",
      "Iteration 295, loss = 0.26071564\n",
      "Iteration 296, loss = 0.26445229\n",
      "Iteration 297, loss = 0.26265468\n",
      "Iteration 298, loss = 0.26036049\n",
      "Iteration 299, loss = 0.26126537\n",
      "Iteration 300, loss = 0.26135871\n",
      "Iteration 301, loss = 0.26159428\n",
      "Iteration 302, loss = 0.26065595\n",
      "Iteration 303, loss = 0.25945798\n",
      "Iteration 304, loss = 0.25979690\n",
      "Iteration 305, loss = 0.26041419\n",
      "Iteration 306, loss = 0.26269590\n",
      "Iteration 307, loss = 0.25795479\n",
      "Iteration 308, loss = 0.25885999\n",
      "Iteration 309, loss = 0.26529041\n",
      "Iteration 310, loss = 0.26345216\n",
      "Iteration 311, loss = 0.25705472\n",
      "Iteration 312, loss = 0.25945400\n",
      "Iteration 313, loss = 0.25890223\n",
      "Iteration 314, loss = 0.25745878\n",
      "Iteration 315, loss = 0.25763304\n",
      "Iteration 316, loss = 0.25929692\n",
      "Iteration 317, loss = 0.25766944\n",
      "Iteration 318, loss = 0.25711115\n",
      "Iteration 319, loss = 0.25610405\n",
      "Iteration 320, loss = 0.25708769\n",
      "Iteration 321, loss = 0.25800149\n",
      "Iteration 322, loss = 0.25693499\n",
      "Iteration 323, loss = 0.25460291\n",
      "Iteration 324, loss = 0.25572647\n",
      "Iteration 325, loss = 0.26310190\n",
      "Iteration 326, loss = 0.26423531\n",
      "Iteration 327, loss = 0.25597603\n",
      "Iteration 328, loss = 0.25606440\n",
      "Iteration 329, loss = 0.25699007\n",
      "Iteration 330, loss = 0.25469811\n",
      "Iteration 331, loss = 0.25642029\n",
      "Iteration 332, loss = 0.25508548\n",
      "Iteration 333, loss = 0.25251271\n",
      "Iteration 334, loss = 0.25854683\n",
      "Iteration 335, loss = 0.25735594\n",
      "Iteration 336, loss = 0.25161260\n",
      "Iteration 337, loss = 0.25664730\n",
      "Iteration 338, loss = 0.25944921\n",
      "Iteration 339, loss = 0.25325683\n",
      "Iteration 340, loss = 0.25412111\n",
      "Iteration 341, loss = 0.25867440\n",
      "Iteration 342, loss = 0.25411869\n",
      "Iteration 343, loss = 0.25292526\n",
      "Iteration 344, loss = 0.25768860\n",
      "Iteration 345, loss = 0.25192071\n",
      "Iteration 346, loss = 0.25595048\n",
      "Iteration 347, loss = 0.25958863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8064516129032258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.88      0.84      0.86        44\n",
      "      Normal       0.65      0.72      0.68        18\n",
      "\n",
      "    accuracy                           0.81        62\n",
      "   macro avg       0.77      0.78      0.77        62\n",
      "weighted avg       0.81      0.81      0.81        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=15)\r\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "y_pred = clf.predict(x_test)\r\n",
    "print(accuracy_score(y_test, y_pred))\r\n",
    "report = classification_report(y_test, y_pred)\r\n",
    "print(report)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.92920079\n",
      "Iteration 2, loss = 0.82382960\n",
      "Iteration 3, loss = 0.74420212\n",
      "Iteration 4, loss = 0.68556225\n",
      "Iteration 5, loss = 0.65105628\n",
      "Iteration 6, loss = 0.63663051\n",
      "Iteration 7, loss = 0.63306369\n",
      "Iteration 8, loss = 0.63498458\n",
      "Iteration 9, loss = 0.63990479\n",
      "Iteration 10, loss = 0.64611492\n",
      "Iteration 11, loss = 0.64727877\n",
      "Iteration 12, loss = 0.64367007\n",
      "Iteration 13, loss = 0.63714182\n",
      "Iteration 14, loss = 0.63000314\n",
      "Iteration 15, loss = 0.62114728\n",
      "Iteration 16, loss = 0.61419522\n",
      "Iteration 17, loss = 0.60774688\n",
      "Iteration 18, loss = 0.60491405\n",
      "Iteration 19, loss = 0.60188012\n",
      "Iteration 20, loss = 0.60046049\n",
      "Iteration 21, loss = 0.59861906\n",
      "Iteration 22, loss = 0.59550898\n",
      "Iteration 23, loss = 0.59079922\n",
      "Iteration 24, loss = 0.58449005\n",
      "Iteration 25, loss = 0.57702586\n",
      "Iteration 26, loss = 0.56996210\n",
      "Iteration 27, loss = 0.56371703\n",
      "Iteration 28, loss = 0.55889126\n",
      "Iteration 29, loss = 0.55414330\n",
      "Iteration 30, loss = 0.54803574\n",
      "Iteration 31, loss = 0.53917196\n",
      "Iteration 32, loss = 0.52876016\n",
      "Iteration 33, loss = 0.51955730\n",
      "Iteration 34, loss = 0.51157071\n",
      "Iteration 35, loss = 0.50480007\n",
      "Iteration 36, loss = 0.49767886\n",
      "Iteration 37, loss = 0.48978283\n",
      "Iteration 38, loss = 0.48103213\n",
      "Iteration 39, loss = 0.47101092\n",
      "Iteration 40, loss = 0.46161522\n",
      "Iteration 41, loss = 0.45355035\n",
      "Iteration 42, loss = 0.44638144\n",
      "Iteration 43, loss = 0.44017519\n",
      "Iteration 44, loss = 0.43241705\n",
      "Iteration 45, loss = 0.42407984\n",
      "Iteration 46, loss = 0.41848260\n",
      "Iteration 47, loss = 0.41306833\n",
      "Iteration 48, loss = 0.40820387\n",
      "Iteration 49, loss = 0.40367486\n",
      "Iteration 50, loss = 0.39896992\n",
      "Iteration 51, loss = 0.39446333\n",
      "Iteration 52, loss = 0.39047885\n",
      "Iteration 53, loss = 0.38734902\n",
      "Iteration 54, loss = 0.38415004\n",
      "Iteration 55, loss = 0.38197484\n",
      "Iteration 56, loss = 0.38075657\n",
      "Iteration 57, loss = 0.37822312\n",
      "Iteration 58, loss = 0.37443264\n",
      "Iteration 59, loss = 0.36960434\n",
      "Iteration 60, loss = 0.36736161\n",
      "Iteration 61, loss = 0.36511257\n",
      "Iteration 62, loss = 0.36228801\n",
      "Iteration 63, loss = 0.36000323\n",
      "Iteration 64, loss = 0.35825793\n",
      "Iteration 65, loss = 0.35617110\n",
      "Iteration 66, loss = 0.35420872\n",
      "Iteration 67, loss = 0.35223441\n",
      "Iteration 68, loss = 0.35008608\n",
      "Iteration 69, loss = 0.34805451\n",
      "Iteration 70, loss = 0.34721737\n",
      "Iteration 71, loss = 0.34630779\n",
      "Iteration 72, loss = 0.34290255\n",
      "Iteration 73, loss = 0.34345831\n",
      "Iteration 74, loss = 0.34899123\n",
      "Iteration 75, loss = 0.35010353\n",
      "Iteration 76, loss = 0.34129501\n",
      "Iteration 77, loss = 0.33691976\n",
      "Iteration 78, loss = 0.33685192\n",
      "Iteration 79, loss = 0.33612451\n",
      "Iteration 80, loss = 0.33310609\n",
      "Iteration 81, loss = 0.33050927\n",
      "Iteration 82, loss = 0.33185372\n",
      "Iteration 83, loss = 0.33477125\n",
      "Iteration 84, loss = 0.33655857\n",
      "Iteration 85, loss = 0.33328567\n",
      "Iteration 86, loss = 0.32716886\n",
      "Iteration 87, loss = 0.32423747\n",
      "Iteration 88, loss = 0.32360325\n",
      "Iteration 89, loss = 0.32256236\n",
      "Iteration 90, loss = 0.32227260\n",
      "Iteration 91, loss = 0.32234401\n",
      "Iteration 92, loss = 0.32184113\n",
      "Iteration 93, loss = 0.32142030\n",
      "Iteration 94, loss = 0.32052240\n",
      "Iteration 95, loss = 0.31883054\n",
      "Iteration 96, loss = 0.31739685\n",
      "Iteration 97, loss = 0.31645661\n",
      "Iteration 98, loss = 0.31591909\n",
      "Iteration 99, loss = 0.31526111\n",
      "Iteration 100, loss = 0.31621233\n",
      "Iteration 101, loss = 0.31740308\n",
      "Iteration 102, loss = 0.31234707\n",
      "Iteration 103, loss = 0.31187623\n",
      "Iteration 104, loss = 0.31585512\n",
      "Iteration 105, loss = 0.31045005\n",
      "Iteration 106, loss = 0.30753946\n",
      "Iteration 107, loss = 0.30923694\n",
      "Iteration 108, loss = 0.31012769\n",
      "Iteration 109, loss = 0.31011703\n",
      "Iteration 110, loss = 0.31117566\n",
      "Iteration 111, loss = 0.31658545\n",
      "Iteration 112, loss = 0.31039649\n",
      "Iteration 113, loss = 0.30167449\n",
      "Iteration 114, loss = 0.30307673\n",
      "Iteration 115, loss = 0.30750228\n",
      "Iteration 116, loss = 0.30789180\n",
      "Iteration 117, loss = 0.30149051\n",
      "Iteration 118, loss = 0.29926832\n",
      "Iteration 119, loss = 0.30554387\n",
      "Iteration 120, loss = 0.31636955\n",
      "Iteration 121, loss = 0.31558769\n",
      "Iteration 122, loss = 0.30271116\n",
      "Iteration 123, loss = 0.29604962\n",
      "Iteration 124, loss = 0.30484336\n",
      "Iteration 125, loss = 0.31939981\n",
      "Iteration 126, loss = 0.32179339\n",
      "Iteration 127, loss = 0.30479139\n",
      "Iteration 128, loss = 0.29322712\n",
      "Iteration 129, loss = 0.29921019\n",
      "Iteration 130, loss = 0.30570107\n",
      "Iteration 131, loss = 0.30416941\n",
      "Iteration 132, loss = 0.29694028\n",
      "Iteration 133, loss = 0.29233387\n",
      "Iteration 134, loss = 0.29811624\n",
      "Iteration 135, loss = 0.30258776\n",
      "Iteration 136, loss = 0.29546464\n",
      "Iteration 137, loss = 0.28952478\n",
      "Iteration 138, loss = 0.29936880\n",
      "Iteration 139, loss = 0.30990372\n",
      "Iteration 140, loss = 0.30632329\n",
      "Iteration 141, loss = 0.29654194\n",
      "Iteration 142, loss = 0.29187903\n",
      "Iteration 143, loss = 0.29583126\n",
      "Iteration 144, loss = 0.29779854\n",
      "Iteration 145, loss = 0.29302074\n",
      "Iteration 146, loss = 0.28757349\n",
      "Iteration 147, loss = 0.28904457\n",
      "Iteration 148, loss = 0.29502672\n",
      "Iteration 149, loss = 0.29768794\n",
      "Iteration 150, loss = 0.29409990\n",
      "Iteration 151, loss = 0.28897473\n",
      "Iteration 152, loss = 0.28590615\n",
      "Iteration 153, loss = 0.29037575\n",
      "Iteration 154, loss = 0.29683237\n",
      "Iteration 155, loss = 0.29385142\n",
      "Iteration 156, loss = 0.28916762\n",
      "Iteration 157, loss = 0.28436823\n",
      "Iteration 158, loss = 0.28660882\n",
      "Iteration 159, loss = 0.28415819\n",
      "Iteration 160, loss = 0.28140161\n",
      "Iteration 161, loss = 0.28299168\n",
      "Iteration 162, loss = 0.29017401\n",
      "Iteration 163, loss = 0.29512823\n",
      "Iteration 164, loss = 0.28734714\n",
      "Iteration 165, loss = 0.27875539\n",
      "Iteration 166, loss = 0.30016313\n",
      "Iteration 167, loss = 0.30927621\n",
      "Iteration 168, loss = 0.28783985\n",
      "Iteration 169, loss = 0.28069245\n",
      "Iteration 170, loss = 0.29870302\n",
      "Iteration 171, loss = 0.31039200\n",
      "Iteration 172, loss = 0.30316591\n",
      "Iteration 173, loss = 0.28741969\n",
      "Iteration 174, loss = 0.27805585\n",
      "Iteration 175, loss = 0.28587320\n",
      "Iteration 176, loss = 0.30232934\n",
      "Iteration 177, loss = 0.29495563\n",
      "Iteration 178, loss = 0.27656947\n",
      "Iteration 179, loss = 0.28549573\n",
      "Iteration 180, loss = 0.30185491\n",
      "Iteration 181, loss = 0.30641612\n",
      "Iteration 182, loss = 0.30302526\n",
      "Iteration 183, loss = 0.29565438\n",
      "Iteration 184, loss = 0.28576583\n",
      "Iteration 185, loss = 0.27985665\n",
      "Iteration 186, loss = 0.27809194\n",
      "Iteration 187, loss = 0.27673009\n",
      "Iteration 188, loss = 0.27646481\n",
      "Iteration 189, loss = 0.27681997\n",
      "Iteration 190, loss = 0.27543910\n",
      "Iteration 191, loss = 0.27323426\n",
      "Iteration 192, loss = 0.27277516\n",
      "Iteration 193, loss = 0.27304016\n",
      "Iteration 194, loss = 0.27233610\n",
      "Iteration 195, loss = 0.27064125\n",
      "Iteration 196, loss = 0.28101734\n",
      "Iteration 197, loss = 0.29380560\n",
      "Iteration 198, loss = 0.28124142\n",
      "Iteration 199, loss = 0.26963635\n",
      "Iteration 200, loss = 0.27593781\n",
      "Iteration 201, loss = 0.28026766\n",
      "Iteration 202, loss = 0.27799320\n",
      "Iteration 203, loss = 0.27371327\n",
      "Iteration 204, loss = 0.27458456\n",
      "Iteration 205, loss = 0.27653244\n",
      "Iteration 206, loss = 0.27380072\n",
      "Iteration 207, loss = 0.26989437\n",
      "Iteration 208, loss = 0.26922921\n",
      "Iteration 209, loss = 0.26844679\n",
      "Iteration 210, loss = 0.26895718\n",
      "Iteration 211, loss = 0.27023596\n",
      "Iteration 212, loss = 0.27127230\n",
      "Iteration 213, loss = 0.27006259\n",
      "Iteration 214, loss = 0.26720070\n",
      "Iteration 215, loss = 0.26613928\n",
      "Iteration 216, loss = 0.27573860\n",
      "Iteration 217, loss = 0.27871689\n",
      "Iteration 218, loss = 0.27113698\n",
      "Iteration 219, loss = 0.27015806\n",
      "Iteration 220, loss = 0.27009825\n",
      "Iteration 221, loss = 0.26907787\n",
      "Iteration 222, loss = 0.26695136\n",
      "Iteration 223, loss = 0.26747973\n",
      "Iteration 224, loss = 0.26709065\n",
      "Iteration 225, loss = 0.26583724\n",
      "Iteration 226, loss = 0.26689630\n",
      "Iteration 227, loss = 0.26760580\n",
      "Iteration 228, loss = 0.26794667\n",
      "Iteration 229, loss = 0.26660790\n",
      "Iteration 230, loss = 0.26462647\n",
      "Iteration 231, loss = 0.26429407\n",
      "Iteration 232, loss = 0.26557063\n",
      "Iteration 233, loss = 0.27563731\n",
      "Iteration 234, loss = 0.27195169\n",
      "Iteration 235, loss = 0.26266638\n",
      "Iteration 236, loss = 0.26162669\n",
      "Iteration 237, loss = 0.26418242\n",
      "Iteration 238, loss = 0.26822843\n",
      "Iteration 239, loss = 0.27103585\n",
      "Iteration 240, loss = 0.27043650\n",
      "Iteration 241, loss = 0.26907237\n",
      "Iteration 242, loss = 0.26366311\n",
      "Iteration 243, loss = 0.26018159\n",
      "Iteration 244, loss = 0.25953872\n",
      "Iteration 245, loss = 0.26341587\n",
      "Iteration 246, loss = 0.26665698\n",
      "Iteration 247, loss = 0.26501285\n",
      "Iteration 248, loss = 0.26238771\n",
      "Iteration 249, loss = 0.26057727\n",
      "Iteration 250, loss = 0.26058053\n",
      "Iteration 251, loss = 0.25970235\n",
      "Iteration 252, loss = 0.26508485\n",
      "Iteration 253, loss = 0.26577417\n",
      "Iteration 254, loss = 0.25958400\n",
      "Iteration 255, loss = 0.25997577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8064516129032258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.93      0.78      0.85        65\n",
      "      Normal       0.63      0.86      0.73        28\n",
      "\n",
      "    accuracy                           0.81        93\n",
      "   macro avg       0.78      0.82      0.79        93\n",
      "weighted avg       0.84      0.81      0.81        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=15)\r\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='logistic', max_iter=500, random_state=42, verbose=True)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "y_pred = clf.predict(x_test)\r\n",
    "print(accuracy_score(y_test, y_pred))\r\n",
    "report = classification_report(y_test, y_pred)\r\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 1.01597977\n",
      "Iteration 2, loss = 0.45706383\n",
      "Iteration 3, loss = 0.65186667\n",
      "Iteration 4, loss = 0.55867127\n",
      "Iteration 5, loss = 0.41518548\n",
      "Iteration 6, loss = 0.43206902\n",
      "Iteration 7, loss = 0.47590245\n",
      "Iteration 8, loss = 0.43806173\n",
      "Iteration 9, loss = 0.39114292\n",
      "Iteration 10, loss = 0.40799969\n",
      "Iteration 11, loss = 0.42346696\n",
      "Iteration 12, loss = 0.40383657\n",
      "Iteration 13, loss = 0.38132833\n",
      "Iteration 14, loss = 0.39794158\n",
      "Iteration 15, loss = 0.40669933\n",
      "Iteration 16, loss = 0.39048103\n",
      "Iteration 17, loss = 0.38168007\n",
      "Iteration 18, loss = 0.38326778\n",
      "Iteration 19, loss = 0.38375921\n",
      "Iteration 20, loss = 0.37428225\n",
      "Iteration 21, loss = 0.37173868\n",
      "Iteration 22, loss = 0.37579583\n",
      "Iteration 23, loss = 0.37145857\n",
      "Iteration 24, loss = 0.36410900\n",
      "Iteration 25, loss = 0.36495978\n",
      "Iteration 26, loss = 0.36968524\n",
      "Iteration 27, loss = 0.36241335\n",
      "Iteration 28, loss = 0.35841003\n",
      "Iteration 29, loss = 0.36592160\n",
      "Iteration 30, loss = 0.36458271\n",
      "Iteration 31, loss = 0.35424744\n",
      "Iteration 32, loss = 0.35388913\n",
      "Iteration 33, loss = 0.35212875\n",
      "Iteration 34, loss = 0.35137433\n",
      "Iteration 35, loss = 0.34860218\n",
      "Iteration 36, loss = 0.34925613\n",
      "Iteration 37, loss = 0.35166679\n",
      "Iteration 38, loss = 0.34700121\n",
      "Iteration 39, loss = 0.34468766\n",
      "Iteration 40, loss = 0.34414293\n",
      "Iteration 41, loss = 0.34612951\n",
      "Iteration 42, loss = 0.34397670\n",
      "Iteration 43, loss = 0.34174909\n",
      "Iteration 44, loss = 0.34215630\n",
      "Iteration 45, loss = 0.33992006\n",
      "Iteration 46, loss = 0.34068299\n",
      "Iteration 47, loss = 0.34689076\n",
      "Iteration 48, loss = 0.34104570\n",
      "Iteration 49, loss = 0.33753567\n",
      "Iteration 50, loss = 0.33888326\n",
      "Iteration 51, loss = 0.33256170\n",
      "Iteration 52, loss = 0.34169463\n",
      "Iteration 53, loss = 0.34215962\n",
      "Iteration 54, loss = 0.33733456\n",
      "Iteration 55, loss = 0.34244334\n",
      "Iteration 56, loss = 0.32992533\n",
      "Iteration 57, loss = 0.34769956\n",
      "Iteration 58, loss = 0.34068915\n",
      "Iteration 59, loss = 0.33373608\n",
      "Iteration 60, loss = 0.32924772\n",
      "Iteration 61, loss = 0.32738172\n",
      "Iteration 62, loss = 0.33151055\n",
      "Iteration 63, loss = 0.32774911\n",
      "Iteration 64, loss = 0.33007542\n",
      "Iteration 65, loss = 0.32584293\n",
      "Iteration 66, loss = 0.32553813\n",
      "Iteration 67, loss = 0.32653171\n",
      "Iteration 68, loss = 0.32283482\n",
      "Iteration 69, loss = 0.32585853\n",
      "Iteration 70, loss = 0.32539820\n",
      "Iteration 71, loss = 0.32155095\n",
      "Iteration 72, loss = 0.32185596\n",
      "Iteration 73, loss = 0.32307139\n",
      "Iteration 74, loss = 0.32279146\n",
      "Iteration 75, loss = 0.32165449\n",
      "Iteration 76, loss = 0.32313990\n",
      "Iteration 77, loss = 0.31875201\n",
      "Iteration 78, loss = 0.32311620\n",
      "Iteration 79, loss = 0.32109647\n",
      "Iteration 80, loss = 0.32023278\n",
      "Iteration 81, loss = 0.31716506\n",
      "Iteration 82, loss = 0.31881439\n",
      "Iteration 83, loss = 0.31892117\n",
      "Iteration 84, loss = 0.31664026\n",
      "Iteration 85, loss = 0.32931034\n",
      "Iteration 86, loss = 0.32429888\n",
      "Iteration 87, loss = 0.31855752\n",
      "Iteration 88, loss = 0.31396108\n",
      "Iteration 89, loss = 0.31917339\n",
      "Iteration 90, loss = 0.32213023\n",
      "Iteration 91, loss = 0.31427022\n",
      "Iteration 92, loss = 0.31501198\n",
      "Iteration 93, loss = 0.31536309\n",
      "Iteration 94, loss = 0.31322099\n",
      "Iteration 95, loss = 0.31167596\n",
      "Iteration 96, loss = 0.31193214\n",
      "Iteration 97, loss = 0.31296883\n",
      "Iteration 98, loss = 0.31171734\n",
      "Iteration 99, loss = 0.31201618\n",
      "Iteration 100, loss = 0.31638288\n",
      "Iteration 101, loss = 0.30998235\n",
      "Iteration 102, loss = 0.31437151\n",
      "Iteration 103, loss = 0.31756298\n",
      "Iteration 104, loss = 0.30787049\n",
      "Iteration 105, loss = 0.30981475\n",
      "Iteration 106, loss = 0.31067001\n",
      "Iteration 107, loss = 0.30644990\n",
      "Iteration 108, loss = 0.31624851\n",
      "Iteration 109, loss = 0.30709486\n",
      "Iteration 110, loss = 0.31169293\n",
      "Iteration 111, loss = 0.30752099\n",
      "Iteration 112, loss = 0.31086876\n",
      "Iteration 113, loss = 0.30882016\n",
      "Iteration 114, loss = 0.31081882\n",
      "Iteration 115, loss = 0.30903321\n",
      "Iteration 116, loss = 0.30545920\n",
      "Iteration 117, loss = 0.30462847\n",
      "Iteration 118, loss = 0.30401615\n",
      "Iteration 119, loss = 0.30304361\n",
      "Iteration 120, loss = 0.30423674\n",
      "Iteration 121, loss = 0.30597892\n",
      "Iteration 122, loss = 0.30140506\n",
      "Iteration 123, loss = 0.30724551\n",
      "Iteration 124, loss = 0.30566443\n",
      "Iteration 125, loss = 0.30962989\n",
      "Iteration 126, loss = 0.30341930\n",
      "Iteration 127, loss = 0.30403675\n",
      "Iteration 128, loss = 0.32112815\n",
      "Iteration 129, loss = 0.29961392\n",
      "Iteration 130, loss = 0.31658792\n",
      "Iteration 131, loss = 0.30624816\n",
      "Iteration 132, loss = 0.30668079\n",
      "Iteration 133, loss = 0.30373090\n",
      "Iteration 134, loss = 0.30215859\n",
      "Iteration 135, loss = 0.30785562\n",
      "Iteration 136, loss = 0.30235602\n",
      "Iteration 137, loss = 0.30157662\n",
      "Iteration 138, loss = 0.30105492\n",
      "Iteration 139, loss = 0.30872780\n",
      "Iteration 140, loss = 0.29657107\n",
      "Iteration 141, loss = 0.29894401\n",
      "Iteration 142, loss = 0.29767251\n",
      "Iteration 143, loss = 0.29529119\n",
      "Iteration 144, loss = 0.29633291\n",
      "Iteration 145, loss = 0.29924543\n",
      "Iteration 146, loss = 0.29550851\n",
      "Iteration 147, loss = 0.29513705\n",
      "Iteration 148, loss = 0.29445267\n",
      "Iteration 149, loss = 0.29437083\n",
      "Iteration 150, loss = 0.29454857\n",
      "Iteration 151, loss = 0.29430894\n",
      "Iteration 152, loss = 0.29381927\n",
      "Iteration 153, loss = 0.29336122\n",
      "Iteration 154, loss = 0.29875257\n",
      "Iteration 155, loss = 0.29640980\n",
      "Iteration 156, loss = 0.29601651\n",
      "Iteration 157, loss = 0.29547700\n",
      "Iteration 158, loss = 0.29339145\n",
      "Iteration 159, loss = 0.29849297\n",
      "Iteration 160, loss = 0.29353767\n",
      "Iteration 161, loss = 0.29733499\n",
      "Iteration 162, loss = 0.29261095\n",
      "Iteration 163, loss = 0.29999376\n",
      "Iteration 164, loss = 0.28910637\n",
      "Iteration 165, loss = 0.30427127\n",
      "Iteration 166, loss = 0.30041955\n",
      "Iteration 167, loss = 0.29833844\n",
      "Iteration 168, loss = 0.30805882\n",
      "Iteration 169, loss = 0.29344061\n",
      "Iteration 170, loss = 0.29240240\n",
      "Iteration 171, loss = 0.29124702\n",
      "Iteration 172, loss = 0.29729494\n",
      "Iteration 173, loss = 0.29149503\n",
      "Iteration 174, loss = 0.30146287\n",
      "Iteration 175, loss = 0.29215229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9354838709677419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.91      1.00      0.95        20\n",
      "      Normal       1.00      0.82      0.90        11\n",
      "\n",
      "    accuracy                           0.94        31\n",
      "   macro avg       0.95      0.91      0.93        31\n",
      "weighted avg       0.94      0.94      0.93        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.1, random_state=13)\r\n",
    "clf = MLPClassifier(max_iter=500, alpha=0.0001, verbose=True)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "y_pred = clf.predict(x_test)\r\n",
    "print(accuracy_score(y_test, y_pred))\r\n",
    "report = classification_report(y_test, y_pred)\r\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 5.71908057\n",
      "Iteration 2, loss = 5.07543211\n",
      "Iteration 3, loss = 3.98989017\n",
      "Iteration 4, loss = 3.44086363\n",
      "Iteration 5, loss = 0.85223806\n",
      "Iteration 6, loss = 2.07914529\n",
      "Iteration 7, loss = 1.49235852\n",
      "Iteration 8, loss = 0.59352445\n",
      "Iteration 9, loss = 1.34596246\n",
      "Iteration 10, loss = 0.63664359\n",
      "Iteration 11, loss = 0.75180113\n",
      "Iteration 12, loss = 0.80186329\n",
      "Iteration 13, loss = 0.42716413\n",
      "Iteration 14, loss = 0.66248641\n",
      "Iteration 15, loss = 0.53018090\n",
      "Iteration 16, loss = 0.44042675\n",
      "Iteration 17, loss = 0.54292012\n",
      "Iteration 18, loss = 0.38493070\n",
      "Iteration 19, loss = 0.42534572\n",
      "Iteration 20, loss = 0.40565483\n",
      "Iteration 21, loss = 0.34480990\n",
      "Iteration 22, loss = 0.43545857\n",
      "Iteration 23, loss = 0.35988394\n",
      "Iteration 24, loss = 0.37670510\n",
      "Iteration 25, loss = 0.33370036\n",
      "Iteration 26, loss = 0.36433885\n",
      "Iteration 27, loss = 0.33575231\n",
      "Iteration 28, loss = 0.34941110\n",
      "Iteration 29, loss = 0.32692301\n",
      "Iteration 30, loss = 0.34334746\n",
      "Iteration 31, loss = 0.33627455\n",
      "Iteration 32, loss = 0.32717788\n",
      "Iteration 33, loss = 0.33185382\n",
      "Iteration 34, loss = 0.31827032\n",
      "Iteration 35, loss = 0.31481301\n",
      "Iteration 36, loss = 0.31439122\n",
      "Iteration 37, loss = 0.31411170\n",
      "Iteration 38, loss = 0.31337511\n",
      "Iteration 39, loss = 0.31231073\n",
      "Iteration 40, loss = 0.31306485\n",
      "Iteration 41, loss = 0.30983681\n",
      "Iteration 42, loss = 0.30812174\n",
      "Iteration 43, loss = 0.30707907\n",
      "Iteration 44, loss = 0.30564975\n",
      "Iteration 45, loss = 0.30890892\n",
      "Iteration 46, loss = 0.30436657\n",
      "Iteration 47, loss = 0.31008864\n",
      "Iteration 48, loss = 0.30279010\n",
      "Iteration 49, loss = 0.30755685\n",
      "Iteration 50, loss = 0.30589337\n",
      "Iteration 51, loss = 0.30532243\n",
      "Iteration 52, loss = 0.30466235\n",
      "Iteration 53, loss = 0.30548477\n",
      "Iteration 54, loss = 0.30121013\n",
      "Iteration 55, loss = 0.30277892\n",
      "Iteration 56, loss = 0.30002049\n",
      "Iteration 57, loss = 0.30081172\n",
      "Iteration 58, loss = 0.30055536\n",
      "Iteration 59, loss = 0.30123786\n",
      "Iteration 60, loss = 0.29658702\n",
      "Iteration 61, loss = 0.30772293\n",
      "Iteration 62, loss = 0.29746993\n",
      "Iteration 63, loss = 0.30237079\n",
      "Iteration 64, loss = 0.30148428\n",
      "Iteration 65, loss = 0.29724622\n",
      "Iteration 66, loss = 0.30822552\n",
      "Iteration 67, loss = 0.29929705\n",
      "Iteration 68, loss = 0.30429786\n",
      "Iteration 69, loss = 0.29193309\n",
      "Iteration 70, loss = 0.29803961\n",
      "Iteration 71, loss = 0.28962570\n",
      "Iteration 72, loss = 0.30232818\n",
      "Iteration 73, loss = 0.29775899\n",
      "Iteration 74, loss = 0.30413970\n",
      "Iteration 75, loss = 0.29593781\n",
      "Iteration 76, loss = 0.28964472\n",
      "Iteration 77, loss = 0.28873673\n",
      "Iteration 78, loss = 0.28954551\n",
      "Iteration 79, loss = 0.28942736\n",
      "Iteration 80, loss = 0.28701909\n",
      "Iteration 81, loss = 0.29131319\n",
      "Iteration 82, loss = 0.28424549\n",
      "Iteration 83, loss = 0.30623427\n",
      "Iteration 84, loss = 0.28496043\n",
      "Iteration 85, loss = 0.30022352\n",
      "Iteration 86, loss = 0.29699089\n",
      "Iteration 87, loss = 0.29063109\n",
      "Iteration 88, loss = 0.29352934\n",
      "Iteration 89, loss = 0.30665444\n",
      "Iteration 90, loss = 0.28701396\n",
      "Iteration 91, loss = 0.30952523\n",
      "Iteration 92, loss = 0.28255556\n",
      "Iteration 93, loss = 0.30094828\n",
      "Iteration 94, loss = 0.28075027\n",
      "Iteration 95, loss = 0.28034060\n",
      "Iteration 96, loss = 0.28125227\n",
      "Iteration 97, loss = 0.27741509\n",
      "Iteration 98, loss = 0.28189310\n",
      "Iteration 99, loss = 0.27816084\n",
      "Iteration 100, loss = 0.28246307\n",
      "Iteration 101, loss = 0.28057768\n",
      "Iteration 102, loss = 0.27671418\n",
      "Iteration 103, loss = 0.27708024\n",
      "Iteration 104, loss = 0.27674391\n",
      "Iteration 105, loss = 0.27460992\n",
      "Iteration 106, loss = 0.27528364\n",
      "Iteration 107, loss = 0.27779306\n",
      "Iteration 108, loss = 0.27609865\n",
      "Iteration 109, loss = 0.26783166\n",
      "Iteration 110, loss = 0.29587684\n",
      "Iteration 111, loss = 0.28958488\n",
      "Iteration 112, loss = 0.28194715\n",
      "Iteration 113, loss = 0.28331071\n",
      "Iteration 114, loss = 0.29332308\n",
      "Iteration 115, loss = 0.26588134\n",
      "Iteration 116, loss = 0.28252456\n",
      "Iteration 117, loss = 0.29491891\n",
      "Iteration 118, loss = 0.28261204\n",
      "Iteration 119, loss = 0.27732343\n",
      "Iteration 120, loss = 0.28651505\n",
      "Iteration 121, loss = 0.27501990\n",
      "Iteration 122, loss = 0.26885253\n",
      "Iteration 123, loss = 0.26599197\n",
      "Iteration 124, loss = 0.26578316\n",
      "Iteration 125, loss = 0.26383700\n",
      "Iteration 126, loss = 0.26212352\n",
      "Iteration 127, loss = 0.25989185\n",
      "Iteration 128, loss = 0.26570657\n",
      "Iteration 129, loss = 0.26014054\n",
      "Iteration 130, loss = 0.25922420\n",
      "Iteration 131, loss = 0.26901881\n",
      "Iteration 132, loss = 0.26267821\n",
      "Iteration 133, loss = 0.27166533\n",
      "Iteration 134, loss = 0.25495097\n",
      "Iteration 135, loss = 0.27655801\n",
      "Iteration 136, loss = 0.25722985\n",
      "Iteration 137, loss = 0.25572609\n",
      "Iteration 138, loss = 0.25639130\n",
      "Iteration 139, loss = 0.25372676\n",
      "Iteration 140, loss = 0.25828522\n",
      "Iteration 141, loss = 0.25354647\n",
      "Iteration 142, loss = 0.25518733\n",
      "Iteration 143, loss = 0.26196720\n",
      "Iteration 144, loss = 0.25916168\n",
      "Iteration 145, loss = 0.24763200\n",
      "Iteration 146, loss = 0.25765080\n",
      "Iteration 147, loss = 0.26865410\n",
      "Iteration 148, loss = 0.27892531\n",
      "Iteration 149, loss = 0.26783017\n",
      "Iteration 150, loss = 0.25682692\n",
      "Iteration 151, loss = 0.28389931\n",
      "Iteration 152, loss = 0.28957327\n",
      "Iteration 153, loss = 0.27363745\n",
      "Iteration 154, loss = 0.25117592\n",
      "Iteration 155, loss = 0.28677883\n",
      "Iteration 156, loss = 0.28239878\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9354838709677419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.91      1.00      0.95        20\n",
      "      Normal       1.00      0.82      0.90        11\n",
      "\n",
      "    accuracy                           0.94        31\n",
      "   macro avg       0.95      0.91      0.93        31\n",
      "weighted avg       0.94      0.94      0.93        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.1, random_state=13)\r\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,300,500), max_iter=500, alpha=0.0001, verbose=True)\r\n",
    "clf.fit(x_train, y_train)\r\n",
    "y_pred = clf.predict(x_test)\r\n",
    "print(accuracy_score(y_test, y_pred))\r\n",
    "report = classification_report(y_test, y_pred)\r\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se ver que as últimas duas configurações de MLP apresentam excelentes resultados de acurácia, *precision* e *recall*, portanto, são possíveis boas candidatas a serem implementadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A equipe levantou a hipótese de que o fato de a quantidade de dados \"Abnormal\" estar acima da quantidade de dados \"Normal\" está baixando a precisão para classificação de dados \"Normal\" nos primeiros exemplos deste ralatório e baixando a acurácia geral nos dois exemplos acima (melhores resultados até então). Para testar isso, vamos igualar a quantidade de dados \"Abnormal\" e \"Normal\" com duas abordagens. Primeiro, expluindo os 110 primeiros dados \"Abnormal\" e depois, excluindo 110 dados \"Abnormal\" de modo aleatório.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 9**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, segue o exemplo excluindo os 110 primeiros dados \"Abnormal\", utilizando o dataset do primeiro experimento e o MLP classifier do primeiro experimento, ou seja, antes da aplicação de todas as melhorias propostas pelos experimentos acima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal      100\n",
      "Abnormal    100\n",
      "Name: class, dtype: int64\n",
      "Iteration 1, loss = 0.70557907\n",
      "Iteration 2, loss = 0.70216837\n",
      "Iteration 3, loss = 0.69914504\n",
      "Iteration 4, loss = 0.69651227\n",
      "Iteration 5, loss = 0.69426537\n",
      "Iteration 6, loss = 0.69238677\n",
      "Iteration 7, loss = 0.69084330\n",
      "Iteration 8, loss = 0.68958895\n",
      "Iteration 9, loss = 0.68856457\n",
      "Iteration 10, loss = 0.68769780\n",
      "Iteration 11, loss = 0.68690556\n",
      "Iteration 12, loss = 0.68610178\n",
      "Iteration 13, loss = 0.68520861\n",
      "Iteration 14, loss = 0.68416510\n",
      "Iteration 15, loss = 0.68293005\n",
      "Iteration 16, loss = 0.68147986\n",
      "Iteration 17, loss = 0.67980555\n",
      "Iteration 18, loss = 0.67790815\n",
      "Iteration 19, loss = 0.67579307\n",
      "Iteration 20, loss = 0.67346696\n",
      "Iteration 21, loss = 0.67093678\n",
      "Iteration 22, loss = 0.66821015\n",
      "Iteration 23, loss = 0.66529510\n",
      "Iteration 24, loss = 0.66219665\n",
      "Iteration 25, loss = 0.65891446\n",
      "Iteration 26, loss = 0.65543999\n",
      "Iteration 27, loss = 0.65175675\n",
      "Iteration 28, loss = 0.64784587\n",
      "Iteration 29, loss = 0.64369002\n",
      "Iteration 30, loss = 0.63927199\n",
      "Iteration 31, loss = 0.63457056\n",
      "Iteration 32, loss = 0.62956018\n",
      "Iteration 33, loss = 0.62421549\n",
      "Iteration 34, loss = 0.61851539\n",
      "Iteration 35, loss = 0.61243910\n",
      "Iteration 36, loss = 0.60595722\n",
      "Iteration 37, loss = 0.59903522\n",
      "Iteration 38, loss = 0.59165262\n",
      "Iteration 39, loss = 0.58381328\n",
      "Iteration 40, loss = 0.57553951\n",
      "Iteration 41, loss = 0.56685942\n",
      "Iteration 42, loss = 0.55779505\n",
      "Iteration 43, loss = 0.54836186\n",
      "Iteration 44, loss = 0.53857431\n",
      "Iteration 45, loss = 0.52845573\n",
      "Iteration 46, loss = 0.51804373\n",
      "Iteration 47, loss = 0.50738506\n",
      "Iteration 48, loss = 0.49652711\n",
      "Iteration 49, loss = 0.48551269\n",
      "Iteration 50, loss = 0.47438349\n",
      "Iteration 51, loss = 0.46318520\n",
      "Iteration 52, loss = 0.45195611\n",
      "Iteration 53, loss = 0.44071369\n",
      "Iteration 54, loss = 0.42946654\n",
      "Iteration 55, loss = 0.41823982\n",
      "Iteration 56, loss = 0.40709091\n",
      "Iteration 57, loss = 0.39609955\n",
      "Iteration 58, loss = 0.38533085\n",
      "Iteration 59, loss = 0.37480648\n",
      "Iteration 60, loss = 0.36453044\n",
      "Iteration 61, loss = 0.35453755\n",
      "Iteration 62, loss = 0.34488165\n",
      "Iteration 63, loss = 0.33558897\n",
      "Iteration 64, loss = 0.32665618\n",
      "Iteration 65, loss = 0.31807454\n",
      "Iteration 66, loss = 0.30981654\n",
      "Iteration 67, loss = 0.30182995\n",
      "Iteration 68, loss = 0.29406436\n",
      "Iteration 69, loss = 0.28649783\n",
      "Iteration 70, loss = 0.27914990\n",
      "Iteration 71, loss = 0.27206996\n",
      "Iteration 72, loss = 0.26530651\n",
      "Iteration 73, loss = 0.25888308\n",
      "Iteration 74, loss = 0.25280636\n",
      "Iteration 75, loss = 0.24706343\n",
      "Iteration 76, loss = 0.24161106\n",
      "Iteration 77, loss = 0.23640090\n",
      "Iteration 78, loss = 0.23140189\n",
      "Iteration 79, loss = 0.22658841\n",
      "Iteration 80, loss = 0.22193597\n",
      "Iteration 81, loss = 0.21743922\n",
      "Iteration 82, loss = 0.21310421\n",
      "Iteration 83, loss = 0.20893838\n",
      "Iteration 84, loss = 0.20494319\n",
      "Iteration 85, loss = 0.20111345\n",
      "Iteration 86, loss = 0.19744410\n",
      "Iteration 87, loss = 0.19393118\n",
      "Iteration 88, loss = 0.19057147\n",
      "Iteration 89, loss = 0.18736135\n",
      "Iteration 90, loss = 0.18429243\n",
      "Iteration 91, loss = 0.18135043\n",
      "Iteration 92, loss = 0.17851884\n",
      "Iteration 93, loss = 0.17578392\n",
      "Iteration 94, loss = 0.17313693\n",
      "Iteration 95, loss = 0.17057223\n",
      "Iteration 96, loss = 0.16808587\n",
      "Iteration 97, loss = 0.16567304\n",
      "Iteration 98, loss = 0.16332623\n",
      "Iteration 99, loss = 0.16103629\n",
      "Iteration 100, loss = 0.15879468\n",
      "Iteration 101, loss = 0.15659475\n",
      "Iteration 102, loss = 0.15443183\n",
      "Iteration 103, loss = 0.15230305\n",
      "Iteration 104, loss = 0.15020734\n",
      "Iteration 105, loss = 0.14814482\n",
      "Iteration 106, loss = 0.14611551\n",
      "Iteration 107, loss = 0.14411759\n",
      "Iteration 108, loss = 0.14214599\n",
      "Iteration 109, loss = 0.14019220\n",
      "Iteration 110, loss = 0.13824685\n",
      "Iteration 111, loss = 0.13630663\n",
      "Iteration 112, loss = 0.13438202\n",
      "Iteration 113, loss = 0.13249027\n",
      "Iteration 114, loss = 0.13061210\n",
      "Iteration 115, loss = 0.12873940\n",
      "Iteration 116, loss = 0.12687878\n",
      "Iteration 117, loss = 0.12503741\n",
      "Iteration 118, loss = 0.12321647\n",
      "Iteration 119, loss = 0.12140722\n",
      "Iteration 120, loss = 0.11959880\n",
      "Iteration 121, loss = 0.11778395\n",
      "Iteration 122, loss = 0.11595905\n",
      "Iteration 123, loss = 0.11412563\n",
      "Iteration 124, loss = 0.11228856\n",
      "Iteration 125, loss = 0.11045441\n",
      "Iteration 126, loss = 0.10862996\n",
      "Iteration 127, loss = 0.10682075\n",
      "Iteration 128, loss = 0.10503524\n",
      "Iteration 129, loss = 0.10328272\n",
      "Iteration 130, loss = 0.10156669\n",
      "Iteration 131, loss = 0.09988755\n",
      "Iteration 132, loss = 0.09824899\n",
      "Iteration 133, loss = 0.09664189\n",
      "Iteration 134, loss = 0.09506581\n",
      "Iteration 135, loss = 0.09352933\n",
      "Iteration 136, loss = 0.09203620\n",
      "Iteration 137, loss = 0.09058539\n",
      "Iteration 138, loss = 0.08917571\n",
      "Iteration 139, loss = 0.08780621\n",
      "Iteration 140, loss = 0.08647194\n",
      "Iteration 141, loss = 0.08516401\n",
      "Iteration 142, loss = 0.08387637\n",
      "Iteration 143, loss = 0.08261034\n",
      "Iteration 144, loss = 0.08137136\n",
      "Iteration 145, loss = 0.08015635\n",
      "Iteration 146, loss = 0.07896159\n",
      "Iteration 147, loss = 0.07779119\n",
      "Iteration 148, loss = 0.07664763\n",
      "Iteration 149, loss = 0.07552860\n",
      "Iteration 150, loss = 0.07443100\n",
      "Iteration 151, loss = 0.07335540\n",
      "Iteration 152, loss = 0.07230329\n",
      "Iteration 153, loss = 0.07127106\n",
      "Iteration 154, loss = 0.07026041\n",
      "Iteration 155, loss = 0.06927362\n",
      "Iteration 156, loss = 0.06830957\n",
      "Iteration 157, loss = 0.06736534\n",
      "Iteration 158, loss = 0.06643941\n",
      "Iteration 159, loss = 0.06553312\n",
      "Iteration 160, loss = 0.06464818\n",
      "Iteration 161, loss = 0.06378440\n",
      "Iteration 162, loss = 0.06293798\n",
      "Iteration 163, loss = 0.06210715\n",
      "Iteration 164, loss = 0.06129367\n",
      "Iteration 165, loss = 0.06049854\n",
      "Iteration 166, loss = 0.05971997\n",
      "Iteration 167, loss = 0.05895641\n",
      "Iteration 168, loss = 0.05820852\n",
      "Iteration 169, loss = 0.05747798\n",
      "Iteration 170, loss = 0.05676349\n",
      "Iteration 171, loss = 0.05606288\n",
      "Iteration 172, loss = 0.05537669\n",
      "Iteration 173, loss = 0.05470572\n",
      "Iteration 174, loss = 0.05404905\n",
      "Iteration 175, loss = 0.05340568\n",
      "Iteration 176, loss = 0.05277551\n",
      "Iteration 177, loss = 0.05215856\n",
      "Iteration 178, loss = 0.05155461\n",
      "Iteration 179, loss = 0.05096274\n",
      "Iteration 180, loss = 0.05038254\n",
      "Iteration 181, loss = 0.04981427\n",
      "Iteration 182, loss = 0.04925779\n",
      "Iteration 183, loss = 0.04871242\n",
      "Iteration 184, loss = 0.04817751\n",
      "Iteration 185, loss = 0.04765260\n",
      "Iteration 186, loss = 0.04713772\n",
      "Iteration 187, loss = 0.04663426\n",
      "Iteration 188, loss = 0.04614223\n",
      "Iteration 189, loss = 0.04565978\n",
      "Iteration 190, loss = 0.04518588\n",
      "Iteration 191, loss = 0.04471986\n",
      "Iteration 192, loss = 0.04426191\n",
      "Iteration 193, loss = 0.04381339\n",
      "Iteration 194, loss = 0.04337435\n",
      "Iteration 195, loss = 0.04294284\n",
      "Iteration 196, loss = 0.04251808\n",
      "Iteration 197, loss = 0.04210069\n",
      "Iteration 198, loss = 0.04169172\n",
      "Iteration 199, loss = 0.04129063\n",
      "Iteration 200, loss = 0.04089613\n",
      "Iteration 201, loss = 0.04050822\n",
      "Iteration 202, loss = 0.04012779\n",
      "Iteration 203, loss = 0.03975438\n",
      "Iteration 204, loss = 0.03938692\n",
      "Iteration 205, loss = 0.03902560\n",
      "Iteration 206, loss = 0.03867114\n",
      "Iteration 207, loss = 0.03832295\n",
      "Iteration 208, loss = 0.03798010\n",
      "Iteration 209, loss = 0.03764304\n",
      "Iteration 210, loss = 0.03731210\n",
      "Iteration 211, loss = 0.03698664\n",
      "Iteration 212, loss = 0.03666623\n",
      "Iteration 213, loss = 0.03635129\n",
      "Iteration 214, loss = 0.03604165\n",
      "Iteration 215, loss = 0.03573684\n",
      "Iteration 216, loss = 0.03543688\n",
      "Iteration 217, loss = 0.03514195\n",
      "Iteration 218, loss = 0.03485168\n",
      "Iteration 219, loss = 0.03456584\n",
      "Iteration 220, loss = 0.03428456\n",
      "Iteration 221, loss = 0.03400778\n",
      "Iteration 222, loss = 0.03373522\n",
      "Iteration 223, loss = 0.03346683\n",
      "Iteration 224, loss = 0.03320266\n",
      "Iteration 225, loss = 0.03294253\n",
      "Iteration 226, loss = 0.03268627\n",
      "Iteration 227, loss = 0.03243390\n",
      "Iteration 228, loss = 0.03218538\n",
      "Iteration 229, loss = 0.03194053\n",
      "Iteration 230, loss = 0.03169927\n",
      "Iteration 231, loss = 0.03146160\n",
      "Iteration 232, loss = 0.03122741\n",
      "Iteration 233, loss = 0.03099659\n",
      "Iteration 234, loss = 0.03076908\n",
      "Iteration 235, loss = 0.03054484\n",
      "Iteration 236, loss = 0.03032378\n",
      "Iteration 237, loss = 0.03010580\n",
      "Iteration 238, loss = 0.02989087\n",
      "Iteration 239, loss = 0.02967892\n",
      "Iteration 240, loss = 0.02946988\n",
      "Iteration 241, loss = 0.02926368\n",
      "Iteration 242, loss = 0.02906027\n",
      "Iteration 243, loss = 0.02885960\n",
      "Iteration 244, loss = 0.02866160\n",
      "Iteration 245, loss = 0.02846621\n",
      "Iteration 246, loss = 0.02827339\n",
      "Iteration 247, loss = 0.02808308\n",
      "Iteration 248, loss = 0.02789524\n",
      "Iteration 249, loss = 0.02770980\n",
      "Iteration 250, loss = 0.02752672\n",
      "Iteration 251, loss = 0.02734597\n",
      "Iteration 252, loss = 0.02716748\n",
      "Iteration 253, loss = 0.02699122\n",
      "Iteration 254, loss = 0.02681714\n",
      "Iteration 255, loss = 0.02664520\n",
      "Iteration 256, loss = 0.02647535\n",
      "Iteration 257, loss = 0.02630755\n",
      "Iteration 258, loss = 0.02614178\n",
      "Iteration 259, loss = 0.02597798\n",
      "Iteration 260, loss = 0.02581611\n",
      "Iteration 261, loss = 0.02565615\n",
      "Iteration 262, loss = 0.02549805\n",
      "Iteration 263, loss = 0.02534178\n",
      "Iteration 264, loss = 0.02518731\n",
      "Iteration 265, loss = 0.02503460\n",
      "Iteration 266, loss = 0.02488363\n",
      "Iteration 267, loss = 0.02473435\n",
      "Iteration 268, loss = 0.02458674\n",
      "Iteration 269, loss = 0.02444077\n",
      "Iteration 270, loss = 0.02429641\n",
      "Iteration 271, loss = 0.02415363\n",
      "Iteration 272, loss = 0.02401240\n",
      "Iteration 273, loss = 0.02387270\n",
      "Iteration 274, loss = 0.02373451\n",
      "Iteration 275, loss = 0.02359778\n",
      "Iteration 276, loss = 0.02346251\n",
      "Iteration 277, loss = 0.02332866\n",
      "Iteration 278, loss = 0.02319622\n",
      "Iteration 279, loss = 0.02306515\n",
      "Iteration 280, loss = 0.02293543\n",
      "Iteration 281, loss = 0.02280703\n",
      "Iteration 282, loss = 0.02267993\n",
      "Iteration 283, loss = 0.02255409\n",
      "Iteration 284, loss = 0.02242947\n",
      "Iteration 285, loss = 0.02230601\n",
      "Iteration 286, loss = 0.02218375\n",
      "Iteration 287, loss = 0.02206274\n",
      "Iteration 288, loss = 0.02194275\n",
      "Iteration 289, loss = 0.02182359\n",
      "Iteration 290, loss = 0.02170579\n",
      "Iteration 291, loss = 0.02158973\n",
      "Iteration 292, loss = 0.02147449\n",
      "Iteration 293, loss = 0.02136015\n",
      "Iteration 294, loss = 0.02124694\n",
      "Iteration 295, loss = 0.02113491\n",
      "Iteration 296, loss = 0.02102395\n",
      "Iteration 297, loss = 0.02091403\n",
      "Iteration 298, loss = 0.02080513\n",
      "Iteration 299, loss = 0.02069723\n",
      "Iteration 300, loss = 0.02059035\n",
      "Iteration 301, loss = 0.02048445\n",
      "Iteration 302, loss = 0.02037955\n",
      "Iteration 303, loss = 0.02027565\n",
      "Iteration 304, loss = 0.02017265\n",
      "Iteration 305, loss = 0.02007050\n",
      "Iteration 306, loss = 0.01996930\n",
      "Iteration 307, loss = 0.01986907\n",
      "Iteration 308, loss = 0.01976972\n",
      "Iteration 309, loss = 0.01967115\n",
      "Iteration 310, loss = 0.01957347\n",
      "Iteration 311, loss = 0.01947674\n",
      "Iteration 312, loss = 0.01938093\n",
      "Iteration 313, loss = 0.01928595\n",
      "Iteration 314, loss = 0.01919180\n",
      "Iteration 315, loss = 0.01909851\n",
      "Iteration 316, loss = 0.01900605\n",
      "Iteration 317, loss = 0.01891438\n",
      "Iteration 318, loss = 0.01882347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       1.00      0.94      0.97        17\n",
      "      Normal       0.96      1.00      0.98        23\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv(\"Dataset_spine.csv\")\n",
    "data1 = data1.drop(['Unnamed: 13'], axis=1)\n",
    "data1.rename(columns = {\n",
    "    \"Col1\" : \"pelvic_incidence\", \n",
    "    \"Col2\" : \"pelvic_tilt\",\n",
    "    \"Col3\" : \"lumbar_lordosis_angle\",\n",
    "    \"Col4\" : \"sacral_slope\", \n",
    "    \"Col5\" : \"pelvic_radius\",\n",
    "    \"Col6\" : \"degree_spondylolisthesis\", \n",
    "    \"Col7\" : \"pelvic_slope\",\n",
    "    \"Col8\" : \"direct_tilt\",\n",
    "    \"Col9\" : \"thoracic_slope\", \n",
    "    \"Col10\" :\"cervical_tilt\", \n",
    "    \"Col11\" : \"sacrum_angle\",\n",
    "    \"Col12\" : \"scoliosis_slope\", \n",
    "    \"Class_att\" : \"class\"}, inplace=True)\n",
    "\n",
    "new_data_copy = data1\n",
    "\n",
    "new_data3 = new_data_copy.copy()\n",
    "\n",
    "for i in range(110):\n",
    "    new_data3.drop((i+1),axis=0,inplace=True)\n",
    "\n",
    "y = new_data3['class']\n",
    "x = new_data3.drop(['class'], axis = 1)\n",
    "print(y.value_counts())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que podemos alcançar resultados similares aos dois melhores resultados apresentados até então apenas com esta técnica.\n",
    "\n",
    "Agora verificamos o que acontece se ao invés de removermos 110 casos 'Abnormal' nos adicionarmos 110 casos 'Normal' através da replicação de outros casos, para que o dataset fique igual"
   ]
  },
  {
   "source": [
    "**EXPERIMENTO10**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal      210\n",
      "Abnormal    210\n",
      "Name: class, dtype: int64\n",
      "Iteration 1, loss = 0.70985117\n",
      "Iteration 2, loss = 0.70219480\n",
      "Iteration 3, loss = 0.69710815\n",
      "Iteration 4, loss = 0.69423380\n",
      "Iteration 5, loss = 0.69203732\n",
      "Iteration 6, loss = 0.68938282\n",
      "Iteration 7, loss = 0.68862488\n",
      "Iteration 8, loss = 0.68725632\n",
      "Iteration 9, loss = 0.68583953\n",
      "Iteration 10, loss = 0.68377183\n",
      "Iteration 11, loss = 0.68134456\n",
      "Iteration 12, loss = 0.67853915\n",
      "Iteration 13, loss = 0.67527539\n",
      "Iteration 14, loss = 0.67183270\n",
      "Iteration 15, loss = 0.66808823\n",
      "Iteration 16, loss = 0.66409610\n",
      "Iteration 17, loss = 0.65957494\n",
      "Iteration 18, loss = 0.65438576\n",
      "Iteration 19, loss = 0.64822100\n",
      "Iteration 20, loss = 0.64164767\n",
      "Iteration 21, loss = 0.63368043\n",
      "Iteration 22, loss = 0.62505848\n",
      "Iteration 23, loss = 0.61567113\n",
      "Iteration 24, loss = 0.60555772\n",
      "Iteration 25, loss = 0.59466328\n",
      "Iteration 26, loss = 0.58319217\n",
      "Iteration 27, loss = 0.57119916\n",
      "Iteration 28, loss = 0.55852184\n",
      "Iteration 29, loss = 0.54588097\n",
      "Iteration 30, loss = 0.53270138\n",
      "Iteration 31, loss = 0.51966895\n",
      "Iteration 32, loss = 0.50674034\n",
      "Iteration 33, loss = 0.49467295\n",
      "Iteration 34, loss = 0.48319813\n",
      "Iteration 35, loss = 0.47288055\n",
      "Iteration 36, loss = 0.46170561\n",
      "Iteration 37, loss = 0.45254519\n",
      "Iteration 38, loss = 0.44290404\n",
      "Iteration 39, loss = 0.43388915\n",
      "Iteration 40, loss = 0.42567608\n",
      "Iteration 41, loss = 0.41785614\n",
      "Iteration 42, loss = 0.41077779\n",
      "Iteration 43, loss = 0.40424059\n",
      "Iteration 44, loss = 0.39717989\n",
      "Iteration 45, loss = 0.39139770\n",
      "Iteration 46, loss = 0.38593834\n",
      "Iteration 47, loss = 0.38095575\n",
      "Iteration 48, loss = 0.37476792\n",
      "Iteration 49, loss = 0.36956662\n",
      "Iteration 50, loss = 0.36493230\n",
      "Iteration 51, loss = 0.36002793\n",
      "Iteration 52, loss = 0.35656412\n",
      "Iteration 53, loss = 0.35255141\n",
      "Iteration 54, loss = 0.34756929\n",
      "Iteration 55, loss = 0.34382115\n",
      "Iteration 56, loss = 0.34045803\n",
      "Iteration 57, loss = 0.33667483\n",
      "Iteration 58, loss = 0.33327076\n",
      "Iteration 59, loss = 0.32906477\n",
      "Iteration 60, loss = 0.32696752\n",
      "Iteration 61, loss = 0.32288527\n",
      "Iteration 62, loss = 0.32046347\n",
      "Iteration 63, loss = 0.31946651\n",
      "Iteration 64, loss = 0.31425411\n",
      "Iteration 65, loss = 0.31153086\n",
      "Iteration 66, loss = 0.30955557\n",
      "Iteration 67, loss = 0.30787790\n",
      "Iteration 68, loss = 0.30323861\n",
      "Iteration 69, loss = 0.30333447\n",
      "Iteration 70, loss = 0.29946617\n",
      "Iteration 71, loss = 0.29637958\n",
      "Iteration 72, loss = 0.29530044\n",
      "Iteration 73, loss = 0.29313078\n",
      "Iteration 74, loss = 0.29029322\n",
      "Iteration 75, loss = 0.28841298\n",
      "Iteration 76, loss = 0.28658130\n",
      "Iteration 77, loss = 0.28458696\n",
      "Iteration 78, loss = 0.28355634\n",
      "Iteration 79, loss = 0.28091626\n",
      "Iteration 80, loss = 0.27846385\n",
      "Iteration 81, loss = 0.27814849\n",
      "Iteration 82, loss = 0.27536094\n",
      "Iteration 83, loss = 0.27431920\n",
      "Iteration 84, loss = 0.27106151\n",
      "Iteration 85, loss = 0.26991519\n",
      "Iteration 86, loss = 0.26782498\n",
      "Iteration 87, loss = 0.26680360\n",
      "Iteration 88, loss = 0.26496931\n",
      "Iteration 89, loss = 0.26408558\n",
      "Iteration 90, loss = 0.26163125\n",
      "Iteration 91, loss = 0.26185890\n",
      "Iteration 92, loss = 0.25772556\n",
      "Iteration 93, loss = 0.25644056\n",
      "Iteration 94, loss = 0.25407794\n",
      "Iteration 95, loss = 0.25408856\n",
      "Iteration 96, loss = 0.25166068\n",
      "Iteration 97, loss = 0.25201563\n",
      "Iteration 98, loss = 0.24783836\n",
      "Iteration 99, loss = 0.24812499\n",
      "Iteration 100, loss = 0.24430453\n",
      "Iteration 101, loss = 0.24375734\n",
      "Iteration 102, loss = 0.24257382\n",
      "Iteration 103, loss = 0.23949540\n",
      "Iteration 104, loss = 0.23726307\n",
      "Iteration 105, loss = 0.23591939\n",
      "Iteration 106, loss = 0.23610978\n",
      "Iteration 107, loss = 0.23183749\n",
      "Iteration 108, loss = 0.23070776\n",
      "Iteration 109, loss = 0.22946527\n",
      "Iteration 110, loss = 0.22757844\n",
      "Iteration 111, loss = 0.22613871\n",
      "Iteration 112, loss = 0.22470929\n",
      "Iteration 113, loss = 0.22502853\n",
      "Iteration 114, loss = 0.22047146\n",
      "Iteration 115, loss = 0.22104961\n",
      "Iteration 116, loss = 0.21658058\n",
      "Iteration 117, loss = 0.21619166\n",
      "Iteration 118, loss = 0.21292978\n",
      "Iteration 119, loss = 0.21276555\n",
      "Iteration 120, loss = 0.21306565\n",
      "Iteration 121, loss = 0.20914587\n",
      "Iteration 122, loss = 0.20632569\n",
      "Iteration 123, loss = 0.20538124\n",
      "Iteration 124, loss = 0.20465189\n",
      "Iteration 125, loss = 0.20189276\n",
      "Iteration 126, loss = 0.20088575\n",
      "Iteration 127, loss = 0.20012024\n",
      "Iteration 128, loss = 0.19747353\n",
      "Iteration 129, loss = 0.19495280\n",
      "Iteration 130, loss = 0.19420826\n",
      "Iteration 131, loss = 0.19225890\n",
      "Iteration 132, loss = 0.19047519\n",
      "Iteration 133, loss = 0.18912969\n",
      "Iteration 134, loss = 0.18758919\n",
      "Iteration 135, loss = 0.18771853\n",
      "Iteration 136, loss = 0.18562960\n",
      "Iteration 137, loss = 0.18362106\n",
      "Iteration 138, loss = 0.18211756\n",
      "Iteration 139, loss = 0.18042152\n",
      "Iteration 140, loss = 0.17915140\n",
      "Iteration 141, loss = 0.17728215\n",
      "Iteration 142, loss = 0.17625779\n",
      "Iteration 143, loss = 0.17429726\n",
      "Iteration 144, loss = 0.17266194\n",
      "Iteration 145, loss = 0.17206642\n",
      "Iteration 146, loss = 0.17305678\n",
      "Iteration 147, loss = 0.16868150\n",
      "Iteration 148, loss = 0.16863916\n",
      "Iteration 149, loss = 0.16559095\n",
      "Iteration 150, loss = 0.16647663\n",
      "Iteration 151, loss = 0.16462425\n",
      "Iteration 152, loss = 0.16297342\n",
      "Iteration 153, loss = 0.16205306\n",
      "Iteration 154, loss = 0.15953718\n",
      "Iteration 155, loss = 0.15801722\n",
      "Iteration 156, loss = 0.15641230\n",
      "Iteration 157, loss = 0.15560265\n",
      "Iteration 158, loss = 0.15627369\n",
      "Iteration 159, loss = 0.15225186\n",
      "Iteration 160, loss = 0.15402775\n",
      "Iteration 161, loss = 0.14981898\n",
      "Iteration 162, loss = 0.14908493\n",
      "Iteration 163, loss = 0.14761830\n",
      "Iteration 164, loss = 0.14673644\n",
      "Iteration 165, loss = 0.14652240\n",
      "Iteration 166, loss = 0.14488755\n",
      "Iteration 167, loss = 0.14267208\n",
      "Iteration 168, loss = 0.14162112\n",
      "Iteration 169, loss = 0.14115408\n",
      "Iteration 170, loss = 0.13865766\n",
      "Iteration 171, loss = 0.13817254\n",
      "Iteration 172, loss = 0.13790476\n",
      "Iteration 173, loss = 0.13672258\n",
      "Iteration 174, loss = 0.13319586\n",
      "Iteration 175, loss = 0.13486427\n",
      "Iteration 176, loss = 0.13122611\n",
      "Iteration 177, loss = 0.13193673\n",
      "Iteration 178, loss = 0.12834406\n",
      "Iteration 179, loss = 0.12942718\n",
      "Iteration 180, loss = 0.12613290\n",
      "Iteration 181, loss = 0.12646157\n",
      "Iteration 182, loss = 0.12508600\n",
      "Iteration 183, loss = 0.12615735\n",
      "Iteration 184, loss = 0.12132348\n",
      "Iteration 185, loss = 0.11906065\n",
      "Iteration 186, loss = 0.11791256\n",
      "Iteration 187, loss = 0.11628260\n",
      "Iteration 188, loss = 0.11510593\n",
      "Iteration 189, loss = 0.11304891\n",
      "Iteration 190, loss = 0.11201645\n",
      "Iteration 191, loss = 0.11065756\n",
      "Iteration 192, loss = 0.10961942\n",
      "Iteration 193, loss = 0.10817922\n",
      "Iteration 194, loss = 0.10730761\n",
      "Iteration 195, loss = 0.10645484\n",
      "Iteration 196, loss = 0.10380353\n",
      "Iteration 197, loss = 0.10407298\n",
      "Iteration 198, loss = 0.10129706\n",
      "Iteration 199, loss = 0.10042603\n",
      "Iteration 200, loss = 0.10124650\n",
      "Iteration 201, loss = 0.09870767\n",
      "Iteration 202, loss = 0.09801300\n",
      "Iteration 203, loss = 0.09975861\n",
      "Iteration 204, loss = 0.09764738\n",
      "Iteration 205, loss = 0.09724921\n",
      "Iteration 206, loss = 0.09526656\n",
      "Iteration 207, loss = 0.09270871\n",
      "Iteration 208, loss = 0.09986215\n",
      "Iteration 209, loss = 0.09427499\n",
      "Iteration 210, loss = 0.09371891\n",
      "Iteration 211, loss = 0.08598933\n",
      "Iteration 212, loss = 0.08815807\n",
      "Iteration 213, loss = 0.08522978\n",
      "Iteration 214, loss = 0.08434939\n",
      "Iteration 215, loss = 0.08406962\n",
      "Iteration 216, loss = 0.08273979\n",
      "Iteration 217, loss = 0.08100370\n",
      "Iteration 218, loss = 0.08066365\n",
      "Iteration 219, loss = 0.08025512\n",
      "Iteration 220, loss = 0.07854147\n",
      "Iteration 221, loss = 0.07764857\n",
      "Iteration 222, loss = 0.07760087\n",
      "Iteration 223, loss = 0.07588717\n",
      "Iteration 224, loss = 0.07518059\n",
      "Iteration 225, loss = 0.07554535\n",
      "Iteration 226, loss = 0.07324619\n",
      "Iteration 227, loss = 0.07391677\n",
      "Iteration 228, loss = 0.07177891\n",
      "Iteration 229, loss = 0.07158697\n",
      "Iteration 230, loss = 0.07022880\n",
      "Iteration 231, loss = 0.07021405\n",
      "Iteration 232, loss = 0.06893103\n",
      "Iteration 233, loss = 0.06841949\n",
      "Iteration 234, loss = 0.06781837\n",
      "Iteration 235, loss = 0.06709291\n",
      "Iteration 236, loss = 0.06692394\n",
      "Iteration 237, loss = 0.06603773\n",
      "Iteration 238, loss = 0.06548148\n",
      "Iteration 239, loss = 0.06454343\n",
      "Iteration 240, loss = 0.06433383\n",
      "Iteration 241, loss = 0.06377637\n",
      "Iteration 242, loss = 0.06395607\n",
      "Iteration 243, loss = 0.06211522\n",
      "Iteration 244, loss = 0.06252622\n",
      "Iteration 245, loss = 0.06328742\n",
      "Iteration 246, loss = 0.06284084\n",
      "Iteration 247, loss = 0.06033120\n",
      "Iteration 248, loss = 0.06115113\n",
      "Iteration 249, loss = 0.06062641\n",
      "Iteration 250, loss = 0.06003228\n",
      "Iteration 251, loss = 0.06012482\n",
      "Iteration 252, loss = 0.05847841\n",
      "Iteration 253, loss = 0.05846304\n",
      "Iteration 254, loss = 0.05717003\n",
      "Iteration 255, loss = 0.05787803\n",
      "Iteration 256, loss = 0.05674536\n",
      "Iteration 257, loss = 0.05639614\n",
      "Iteration 258, loss = 0.05478188\n",
      "Iteration 259, loss = 0.05546288\n",
      "Iteration 260, loss = 0.05359394\n",
      "Iteration 261, loss = 0.05321398\n",
      "Iteration 262, loss = 0.05331471\n",
      "Iteration 263, loss = 0.05240509\n",
      "Iteration 264, loss = 0.05245306\n",
      "Iteration 265, loss = 0.05393876\n",
      "Iteration 266, loss = 0.05081060\n",
      "Iteration 267, loss = 0.05118501\n",
      "Iteration 268, loss = 0.04982640\n",
      "Iteration 269, loss = 0.04986411\n",
      "Iteration 270, loss = 0.04910872\n",
      "Iteration 271, loss = 0.04860165\n",
      "Iteration 272, loss = 0.04808598\n",
      "Iteration 273, loss = 0.04768226\n",
      "Iteration 274, loss = 0.04902365\n",
      "Iteration 275, loss = 0.04762360\n",
      "Iteration 276, loss = 0.04725449\n",
      "Iteration 277, loss = 0.04624986\n",
      "Iteration 278, loss = 0.04641575\n",
      "Iteration 279, loss = 0.04599085\n",
      "Iteration 280, loss = 0.04498239\n",
      "Iteration 281, loss = 0.04448480\n",
      "Iteration 282, loss = 0.04442622\n",
      "Iteration 283, loss = 0.04358603\n",
      "Iteration 284, loss = 0.04370596\n",
      "Iteration 285, loss = 0.04338786\n",
      "Iteration 286, loss = 0.04265318\n",
      "Iteration 287, loss = 0.04388674\n",
      "Iteration 288, loss = 0.04217409\n",
      "Iteration 289, loss = 0.04243592\n",
      "Iteration 290, loss = 0.04105234\n",
      "Iteration 291, loss = 0.04143256\n",
      "Iteration 292, loss = 0.04016342\n",
      "Iteration 293, loss = 0.04072153\n",
      "Iteration 294, loss = 0.03964566\n",
      "Iteration 295, loss = 0.04008262\n",
      "Iteration 296, loss = 0.03897507\n",
      "Iteration 297, loss = 0.03899140\n",
      "Iteration 298, loss = 0.03844842\n",
      "Iteration 299, loss = 0.03854842\n",
      "Iteration 300, loss = 0.03775122\n",
      "Iteration 301, loss = 0.03783956\n",
      "Iteration 302, loss = 0.03752849\n",
      "Iteration 303, loss = 0.03698754\n",
      "Iteration 304, loss = 0.03654895\n",
      "Iteration 305, loss = 0.03626585\n",
      "Iteration 306, loss = 0.03613095\n",
      "Iteration 307, loss = 0.03588573\n",
      "Iteration 308, loss = 0.03547059\n",
      "Iteration 309, loss = 0.03546386\n",
      "Iteration 310, loss = 0.03514290\n",
      "Iteration 311, loss = 0.03507101\n",
      "Iteration 312, loss = 0.03465308\n",
      "Iteration 313, loss = 0.03464042\n",
      "Iteration 314, loss = 0.03452071\n",
      "Iteration 315, loss = 0.03449306\n",
      "Iteration 316, loss = 0.03404082\n",
      "Iteration 317, loss = 0.03378402\n",
      "Iteration 318, loss = 0.03353471\n",
      "Iteration 319, loss = 0.03333391\n",
      "Iteration 320, loss = 0.03310940\n",
      "Iteration 321, loss = 0.03304493\n",
      "Iteration 322, loss = 0.03270866\n",
      "Iteration 323, loss = 0.03247969\n",
      "Iteration 324, loss = 0.03234223\n",
      "Iteration 325, loss = 0.03219017\n",
      "Iteration 326, loss = 0.03200526\n",
      "Iteration 327, loss = 0.03188151\n",
      "Iteration 328, loss = 0.03180430\n",
      "Iteration 329, loss = 0.03159606\n",
      "Iteration 330, loss = 0.03152376\n",
      "Iteration 331, loss = 0.03126171\n",
      "Iteration 332, loss = 0.03129388\n",
      "Iteration 333, loss = 0.03093553\n",
      "Iteration 334, loss = 0.03093508\n",
      "Iteration 335, loss = 0.03076512\n",
      "Iteration 336, loss = 0.03054827\n",
      "Iteration 337, loss = 0.03041543\n",
      "Iteration 338, loss = 0.03029351\n",
      "Iteration 339, loss = 0.03021097\n",
      "Iteration 340, loss = 0.03001867\n",
      "Iteration 341, loss = 0.02994875\n",
      "Iteration 342, loss = 0.02977363\n",
      "Iteration 343, loss = 0.02964451\n",
      "Iteration 344, loss = 0.02960125\n",
      "Iteration 345, loss = 0.02944380\n",
      "Iteration 346, loss = 0.02954150\n",
      "Iteration 347, loss = 0.02944234\n",
      "Iteration 348, loss = 0.02934517\n",
      "Iteration 349, loss = 0.02922445\n",
      "Iteration 350, loss = 0.02910339\n",
      "Iteration 351, loss = 0.02873556\n",
      "Iteration 352, loss = 0.02886579\n",
      "Iteration 353, loss = 0.02864925\n",
      "Iteration 354, loss = 0.02855140\n",
      "Iteration 355, loss = 0.02847252\n",
      "Iteration 356, loss = 0.02828472\n",
      "Iteration 357, loss = 0.02820707\n",
      "Iteration 358, loss = 0.02808560\n",
      "Iteration 359, loss = 0.02810245\n",
      "Iteration 360, loss = 0.02793094\n",
      "Iteration 361, loss = 0.02788836\n",
      "Iteration 362, loss = 0.02782572\n",
      "Iteration 363, loss = 0.02767914\n",
      "Iteration 364, loss = 0.02757202\n",
      "Iteration 365, loss = 0.02759727\n",
      "Iteration 366, loss = 0.02747261\n",
      "Iteration 367, loss = 0.02739543\n",
      "Iteration 368, loss = 0.02731032\n",
      "Iteration 369, loss = 0.02719680\n",
      "Iteration 370, loss = 0.02720731\n",
      "Iteration 371, loss = 0.02706650\n",
      "Iteration 372, loss = 0.02707017\n",
      "Iteration 373, loss = 0.02691113\n",
      "Iteration 374, loss = 0.02691563\n",
      "Iteration 375, loss = 0.02682191\n",
      "Iteration 376, loss = 0.02669433\n",
      "Iteration 377, loss = 0.02661955\n",
      "Iteration 378, loss = 0.02660064\n",
      "Iteration 379, loss = 0.02648060\n",
      "Iteration 380, loss = 0.02650723\n",
      "Iteration 381, loss = 0.02640198\n",
      "Iteration 382, loss = 0.02629619\n",
      "Iteration 383, loss = 0.02621763\n",
      "Iteration 384, loss = 0.02621537\n",
      "Iteration 385, loss = 0.02614238\n",
      "Iteration 386, loss = 0.02607530\n",
      "Iteration 387, loss = 0.02605487\n",
      "Iteration 388, loss = 0.02598683\n",
      "Iteration 389, loss = 0.02591177\n",
      "Iteration 390, loss = 0.02582333\n",
      "Iteration 391, loss = 0.02582764\n",
      "Iteration 392, loss = 0.02577272\n",
      "Iteration 393, loss = 0.02570643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.94      0.76      0.84        41\n",
      "      Normal       0.80      0.95      0.87        43\n",
      "\n",
      "    accuracy                           0.86        84\n",
      "   macro avg       0.87      0.85      0.86        84\n",
      "weighted avg       0.87      0.86      0.86        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv(\"Dataset_spine.csv\")\n",
    "data2 = data2.drop(['Unnamed: 13'], axis=1)\n",
    "data2.rename(columns = {\n",
    "    \"Col1\" : \"pelvic_incidence\", \n",
    "    \"Col2\" : \"pelvic_tilt\",\n",
    "    \"Col3\" : \"lumbar_lordosis_angle\",\n",
    "    \"Col4\" : \"sacral_slope\", \n",
    "    \"Col5\" : \"pelvic_radius\",\n",
    "    \"Col6\" : \"degree_spondylolisthesis\", \n",
    "    \"Col7\" : \"pelvic_slope\",\n",
    "    \"Col8\" : \"direct_tilt\",\n",
    "    \"Col9\" : \"thoracic_slope\", \n",
    "    \"Col10\" :\"cervical_tilt\", \n",
    "    \"Col11\" : \"sacrum_angle\",\n",
    "    \"Col12\" : \"scoliosis_slope\", \n",
    "    \"Class_att\" : \"class\"}, inplace=True)\n",
    "\n",
    "new_data_copy = data2\n",
    "\n",
    "new_data4 = new_data_copy.copy()\n",
    "new_data5 = new_data_copy.copy()\n",
    "new_data6 = new_data_copy.copy()\n",
    "\n",
    "for i in range(210):\n",
    "    new_data5.drop((i+1),axis=0,inplace=True)\n",
    "\n",
    "for i in range(298):\n",
    "    new_data6.drop((i+1),axis=0,inplace=True)\n",
    "    \n",
    "new_data4 = new_data4.append(new_data5, ignore_index=True)\n",
    "new_data4 = new_data4.append(new_data6, ignore_index=True)\n",
    "\n",
    "for i in range(2):\n",
    "    new_data4.drop((i+1),axis=0,inplace=True)\n",
    "\n",
    "y = new_data4['class']\n",
    "x = new_data4.drop(['class'], axis = 1)\n",
    "print(y.value_counts())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota-se que os resultados melhoram signficativamente em relação aos primeiros testes do relatório e aos primeiros testes com número igual de testes por remoção, porém ainda não atingem os melhores resultados, verificamos agora com alguns parâmetros modificados"
   ]
  },
  {
   "source": [
    "**EXPERIMENTO11**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal      210\n",
      "Abnormal    210\n",
      "Name: class, dtype: int64\n",
      "Iteration 1, loss = 5.42744866\n",
      "Iteration 2, loss = 2.53286440\n",
      "Iteration 3, loss = 2.48639308\n",
      "Iteration 4, loss = 0.90714554\n",
      "Iteration 5, loss = 1.17000653\n",
      "Iteration 6, loss = 1.40889338\n",
      "Iteration 7, loss = 0.70331221\n",
      "Iteration 8, loss = 0.60487647\n",
      "Iteration 9, loss = 0.73532418\n",
      "Iteration 10, loss = 0.43564777\n",
      "Iteration 11, loss = 0.56429343\n",
      "Iteration 12, loss = 0.48336039\n",
      "Iteration 13, loss = 0.32748067\n",
      "Iteration 14, loss = 0.43502068\n",
      "Iteration 15, loss = 0.33470777\n",
      "Iteration 16, loss = 0.33213864\n",
      "Iteration 17, loss = 0.33311993\n",
      "Iteration 18, loss = 0.29199479\n",
      "Iteration 19, loss = 0.31535870\n",
      "Iteration 20, loss = 0.28145470\n",
      "Iteration 21, loss = 0.28272648\n",
      "Iteration 22, loss = 0.27976201\n",
      "Iteration 23, loss = 0.26294978\n",
      "Iteration 24, loss = 0.26406093\n",
      "Iteration 25, loss = 0.25488540\n",
      "Iteration 26, loss = 0.25391821\n",
      "Iteration 27, loss = 0.24558351\n",
      "Iteration 28, loss = 0.24420699\n",
      "Iteration 29, loss = 0.23757313\n",
      "Iteration 30, loss = 0.24153024\n",
      "Iteration 31, loss = 0.23417255\n",
      "Iteration 32, loss = 0.23437582\n",
      "Iteration 33, loss = 0.22550244\n",
      "Iteration 34, loss = 0.22768793\n",
      "Iteration 35, loss = 0.22015884\n",
      "Iteration 36, loss = 0.22240447\n",
      "Iteration 37, loss = 0.21512335\n",
      "Iteration 38, loss = 0.21446645\n",
      "Iteration 39, loss = 0.21505831\n",
      "Iteration 40, loss = 0.21077086\n",
      "Iteration 41, loss = 0.20660345\n",
      "Iteration 42, loss = 0.20246623\n",
      "Iteration 43, loss = 0.20253981\n",
      "Iteration 44, loss = 0.20243605\n",
      "Iteration 45, loss = 0.19520036\n",
      "Iteration 46, loss = 0.19970299\n",
      "Iteration 47, loss = 0.19713794\n",
      "Iteration 48, loss = 0.19277328\n",
      "Iteration 49, loss = 0.18942955\n",
      "Iteration 50, loss = 0.18603639\n",
      "Iteration 51, loss = 0.18310628\n",
      "Iteration 52, loss = 0.18272344\n",
      "Iteration 53, loss = 0.17683689\n",
      "Iteration 54, loss = 0.17678161\n",
      "Iteration 55, loss = 0.17246308\n",
      "Iteration 56, loss = 0.17494405\n",
      "Iteration 57, loss = 0.17071592\n",
      "Iteration 58, loss = 0.16938294\n",
      "Iteration 59, loss = 0.17024361\n",
      "Iteration 60, loss = 0.16934507\n",
      "Iteration 61, loss = 0.16387149\n",
      "Iteration 62, loss = 0.16700969\n",
      "Iteration 63, loss = 0.15551984\n",
      "Iteration 64, loss = 0.16251264\n",
      "Iteration 65, loss = 0.15765885\n",
      "Iteration 66, loss = 0.15048954\n",
      "Iteration 67, loss = 0.15930538\n",
      "Iteration 68, loss = 0.14940070\n",
      "Iteration 69, loss = 0.14662809\n",
      "Iteration 70, loss = 0.14845462\n",
      "Iteration 71, loss = 0.13948554\n",
      "Iteration 72, loss = 0.14193980\n",
      "Iteration 73, loss = 0.13983532\n",
      "Iteration 74, loss = 0.13684522\n",
      "Iteration 75, loss = 0.13616955\n",
      "Iteration 76, loss = 0.12980562\n",
      "Iteration 77, loss = 0.13186805\n",
      "Iteration 78, loss = 0.12337938\n",
      "Iteration 79, loss = 0.12547437\n",
      "Iteration 80, loss = 0.12197425\n",
      "Iteration 81, loss = 0.11880862\n",
      "Iteration 82, loss = 0.11935524\n",
      "Iteration 83, loss = 0.11381210\n",
      "Iteration 84, loss = 0.11442685\n",
      "Iteration 85, loss = 0.11140722\n",
      "Iteration 86, loss = 0.10861163\n",
      "Iteration 87, loss = 0.11403086\n",
      "Iteration 88, loss = 0.10749160\n",
      "Iteration 89, loss = 0.10578941\n",
      "Iteration 90, loss = 0.10402464\n",
      "Iteration 91, loss = 0.10183725\n",
      "Iteration 92, loss = 0.09854388\n",
      "Iteration 93, loss = 0.09651095\n",
      "Iteration 94, loss = 0.09849744\n",
      "Iteration 95, loss = 0.09393909\n",
      "Iteration 96, loss = 0.09104214\n",
      "Iteration 97, loss = 0.09174973\n",
      "Iteration 98, loss = 0.08988498\n",
      "Iteration 99, loss = 0.09130193\n",
      "Iteration 100, loss = 0.08737973\n",
      "Iteration 101, loss = 0.08360108\n",
      "Iteration 102, loss = 0.08138559\n",
      "Iteration 103, loss = 0.08088703\n",
      "Iteration 104, loss = 0.08114330\n",
      "Iteration 105, loss = 0.07513161\n",
      "Iteration 106, loss = 0.07620227\n",
      "Iteration 107, loss = 0.07412546\n",
      "Iteration 108, loss = 0.07460749\n",
      "Iteration 109, loss = 0.07010438\n",
      "Iteration 110, loss = 0.06971685\n",
      "Iteration 111, loss = 0.06958411\n",
      "Iteration 112, loss = 0.06605940\n",
      "Iteration 113, loss = 0.06445788\n",
      "Iteration 114, loss = 0.06358697\n",
      "Iteration 115, loss = 0.06285129\n",
      "Iteration 116, loss = 0.06289379\n",
      "Iteration 117, loss = 0.06733966\n",
      "Iteration 118, loss = 0.05993275\n",
      "Iteration 119, loss = 0.06072967\n",
      "Iteration 120, loss = 0.05798854\n",
      "Iteration 121, loss = 0.05757351\n",
      "Iteration 122, loss = 0.05673049\n",
      "Iteration 123, loss = 0.05834013\n",
      "Iteration 124, loss = 0.05456005\n",
      "Iteration 125, loss = 0.05180618\n",
      "Iteration 126, loss = 0.05883314\n",
      "Iteration 127, loss = 0.05494778\n",
      "Iteration 128, loss = 0.04654222\n",
      "Iteration 129, loss = 0.05224609\n",
      "Iteration 130, loss = 0.05645985\n",
      "Iteration 131, loss = 0.05570074\n",
      "Iteration 132, loss = 0.04333325\n",
      "Iteration 133, loss = 0.05150532\n",
      "Iteration 134, loss = 0.05033086\n",
      "Iteration 135, loss = 0.04528057\n",
      "Iteration 136, loss = 0.04678249\n",
      "Iteration 137, loss = 0.04016439\n",
      "Iteration 138, loss = 0.04032514\n",
      "Iteration 139, loss = 0.03789825\n",
      "Iteration 140, loss = 0.03825756\n",
      "Iteration 141, loss = 0.03674920\n",
      "Iteration 142, loss = 0.03658572\n",
      "Iteration 143, loss = 0.04103280\n",
      "Iteration 144, loss = 0.03447491\n",
      "Iteration 145, loss = 0.03479399\n",
      "Iteration 146, loss = 0.03487522\n",
      "Iteration 147, loss = 0.03245729\n",
      "Iteration 148, loss = 0.03294535\n",
      "Iteration 149, loss = 0.03038648\n",
      "Iteration 150, loss = 0.03216829\n",
      "Iteration 151, loss = 0.03005686\n",
      "Iteration 152, loss = 0.02881377\n",
      "Iteration 153, loss = 0.02795813\n",
      "Iteration 154, loss = 0.02714253\n",
      "Iteration 155, loss = 0.02636024\n",
      "Iteration 156, loss = 0.02818894\n",
      "Iteration 157, loss = 0.02694980\n",
      "Iteration 158, loss = 0.02492516\n",
      "Iteration 159, loss = 0.02563669\n",
      "Iteration 160, loss = 0.02393689\n",
      "Iteration 161, loss = 0.02320879\n",
      "Iteration 162, loss = 0.02204275\n",
      "Iteration 163, loss = 0.02202323\n",
      "Iteration 164, loss = 0.02149116\n",
      "Iteration 165, loss = 0.02083296\n",
      "Iteration 166, loss = 0.02047586\n",
      "Iteration 167, loss = 0.02299858\n",
      "Iteration 168, loss = 0.02041819\n",
      "Iteration 169, loss = 0.02057470\n",
      "Iteration 170, loss = 0.02005562\n",
      "Iteration 171, loss = 0.01899938\n",
      "Iteration 172, loss = 0.01754353\n",
      "Iteration 173, loss = 0.01895655\n",
      "Iteration 174, loss = 0.01697426\n",
      "Iteration 175, loss = 0.01667271\n",
      "Iteration 176, loss = 0.01602661\n",
      "Iteration 177, loss = 0.01562054\n",
      "Iteration 178, loss = 0.01636116\n",
      "Iteration 179, loss = 0.01471516\n",
      "Iteration 180, loss = 0.01462974\n",
      "Iteration 181, loss = 0.01475868\n",
      "Iteration 182, loss = 0.01364815\n",
      "Iteration 183, loss = 0.01396773\n",
      "Iteration 184, loss = 0.01341571\n",
      "Iteration 185, loss = 0.01361622\n",
      "Iteration 186, loss = 0.01271619\n",
      "Iteration 187, loss = 0.01264877\n",
      "Iteration 188, loss = 0.01180113\n",
      "Iteration 189, loss = 0.01197049\n",
      "Iteration 190, loss = 0.01254305\n",
      "Iteration 191, loss = 0.01101776\n",
      "Iteration 192, loss = 0.01126982\n",
      "Iteration 193, loss = 0.01107101\n",
      "Iteration 194, loss = 0.01077396\n",
      "Iteration 195, loss = 0.01031505\n",
      "Iteration 196, loss = 0.00965935\n",
      "Iteration 197, loss = 0.00992990\n",
      "Iteration 198, loss = 0.00948006\n",
      "Iteration 199, loss = 0.00924426\n",
      "Iteration 200, loss = 0.00862646\n",
      "Iteration 201, loss = 0.00878069\n",
      "Iteration 202, loss = 0.00831356\n",
      "Iteration 203, loss = 0.00830375\n",
      "Iteration 204, loss = 0.00796361\n",
      "Iteration 205, loss = 0.00848245\n",
      "Iteration 206, loss = 0.00813988\n",
      "Iteration 207, loss = 0.00742139\n",
      "Iteration 208, loss = 0.00810230\n",
      "Iteration 209, loss = 0.00697142\n",
      "Iteration 210, loss = 0.00700864\n",
      "Iteration 211, loss = 0.00745642\n",
      "Iteration 212, loss = 0.00643892\n",
      "Iteration 213, loss = 0.00660358\n",
      "Iteration 214, loss = 0.00610256\n",
      "Iteration 215, loss = 0.00603671\n",
      "Iteration 216, loss = 0.00560546\n",
      "Iteration 217, loss = 0.00558359\n",
      "Iteration 218, loss = 0.00538437\n",
      "Iteration 219, loss = 0.00522561\n",
      "Iteration 220, loss = 0.00511692\n",
      "Iteration 221, loss = 0.00499622\n",
      "Iteration 222, loss = 0.00475015\n",
      "Iteration 223, loss = 0.00467278\n",
      "Iteration 224, loss = 0.00457308\n",
      "Iteration 225, loss = 0.00442736\n",
      "Iteration 226, loss = 0.00442484\n",
      "Iteration 227, loss = 0.00415976\n",
      "Iteration 228, loss = 0.00423125\n",
      "Iteration 229, loss = 0.00406212\n",
      "Iteration 230, loss = 0.00408561\n",
      "Iteration 231, loss = 0.00377074\n",
      "Iteration 232, loss = 0.00377065\n",
      "Iteration 233, loss = 0.00355467\n",
      "Iteration 234, loss = 0.00355582\n",
      "Iteration 235, loss = 0.00337361\n",
      "Iteration 236, loss = 0.00331641\n",
      "Iteration 237, loss = 0.00348084\n",
      "Iteration 238, loss = 0.00330861\n",
      "Iteration 239, loss = 0.00314366\n",
      "Iteration 240, loss = 0.00321117\n",
      "Iteration 241, loss = 0.00285562\n",
      "Iteration 242, loss = 0.00296941\n",
      "Iteration 243, loss = 0.00275983\n",
      "Iteration 244, loss = 0.00282416\n",
      "Iteration 245, loss = 0.00260893\n",
      "Iteration 246, loss = 0.00272921\n",
      "Iteration 247, loss = 0.00244106\n",
      "Iteration 248, loss = 0.00268924\n",
      "Iteration 249, loss = 0.00231476\n",
      "Iteration 250, loss = 0.00254170\n",
      "Iteration 251, loss = 0.00225987\n",
      "Iteration 252, loss = 0.00233079\n",
      "Iteration 253, loss = 0.00226914\n",
      "Iteration 254, loss = 0.00207267\n",
      "Iteration 255, loss = 0.00212005\n",
      "Iteration 256, loss = 0.00195770\n",
      "Iteration 257, loss = 0.00199037\n",
      "Iteration 258, loss = 0.00190915\n",
      "Iteration 259, loss = 0.00186193\n",
      "Iteration 260, loss = 0.00178660\n",
      "Iteration 261, loss = 0.00175221\n",
      "Iteration 262, loss = 0.00167996\n",
      "Iteration 263, loss = 0.00170174\n",
      "Iteration 264, loss = 0.00160636\n",
      "Iteration 265, loss = 0.00175705\n",
      "Iteration 266, loss = 0.00150512\n",
      "Iteration 267, loss = 0.00164897\n",
      "Iteration 268, loss = 0.00148501\n",
      "Iteration 269, loss = 0.00147502\n",
      "Iteration 270, loss = 0.00144895\n",
      "Iteration 271, loss = 0.00136459\n",
      "Iteration 272, loss = 0.00138255\n",
      "Iteration 273, loss = 0.00132495\n",
      "Iteration 274, loss = 0.00130156\n",
      "Iteration 275, loss = 0.00130081\n",
      "Iteration 276, loss = 0.00123635\n",
      "Iteration 277, loss = 0.00120203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8809523809523809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       1.00      0.72      0.84        18\n",
      "      Normal       0.83      1.00      0.91        24\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.91      0.86      0.87        42\n",
      "weighted avg       0.90      0.88      0.88        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv(\"Dataset_spine.csv\")\n",
    "data2 = data2.drop(['Unnamed: 13'], axis=1)\n",
    "data2.rename(columns = {\n",
    "    \"Col1\" : \"pelvic_incidence\", \n",
    "    \"Col2\" : \"pelvic_tilt\",\n",
    "    \"Col3\" : \"lumbar_lordosis_angle\",\n",
    "    \"Col4\" : \"sacral_slope\", \n",
    "    \"Col5\" : \"pelvic_radius\",\n",
    "    \"Col6\" : \"degree_spondylolisthesis\", \n",
    "    \"Col7\" : \"pelvic_slope\",\n",
    "    \"Col8\" : \"direct_tilt\",\n",
    "    \"Col9\" : \"thoracic_slope\", \n",
    "    \"Col10\" :\"cervical_tilt\", \n",
    "    \"Col11\" : \"sacrum_angle\",\n",
    "    \"Col12\" : \"scoliosis_slope\", \n",
    "    \"Class_att\" : \"class\"}, inplace=True)\n",
    "\n",
    "new_data_copy = data2\n",
    "\n",
    "new_data4 = new_data_copy.copy()\n",
    "new_data5 = new_data_copy.copy()\n",
    "new_data6 = new_data_copy.copy()\n",
    "\n",
    "for i in range(210):\n",
    "    new_data5.drop((i+1),axis=0,inplace=True)\n",
    "\n",
    "for i in range(298):\n",
    "    new_data6.drop((i+1),axis=0,inplace=True)\n",
    "    \n",
    "new_data4 = new_data4.append(new_data5, ignore_index=True)\n",
    "new_data4 = new_data4.append(new_data6, ignore_index=True)\n",
    "\n",
    "for i in range(2):\n",
    "    new_data4.drop((i+1),axis=0,inplace=True)\n",
    "\n",
    "y = new_data4['class']\n",
    "x = new_data4.drop(['class'], axis = 1)\n",
    "print(y.value_counts())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.1, random_state=13)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,300,500), max_iter=500, alpha=0.0001, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota-se uma melhoria nos resultados em relação ao teste anterior, tornando-o um conjunto de parâmetros bem sólido, porém ele também não atinge as melhores métricas de outros experimentos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, segue o exemplo excluindo 110 dados \"Abnormal\" de forma randômica, utilizando o dataset do primeiro experimento e o MLP classifier do primeiro experimento, ou seja, antes da aplicação de todas as melhorias propostas pelos experimentos acima. A intenção, assim como no experimento passado, é igualar o número de casos \"Abnormal\" e \"Normal\", mas agora de modo randômico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal      100\n",
      "Abnormal    100\n",
      "Name: class, dtype: int64\n",
      "Iteration 1, loss = 0.70527304\n",
      "Iteration 2, loss = 0.70203427\n",
      "Iteration 3, loss = 0.69920311\n",
      "Iteration 4, loss = 0.69677368\n",
      "Iteration 5, loss = 0.69473829\n",
      "Iteration 6, loss = 0.69308015\n",
      "Iteration 7, loss = 0.69176898\n",
      "Iteration 8, loss = 0.69076168\n",
      "Iteration 9, loss = 0.69000161\n",
      "Iteration 10, loss = 0.68941929\n",
      "Iteration 11, loss = 0.68893614\n",
      "Iteration 12, loss = 0.68847117\n",
      "Iteration 13, loss = 0.68795001\n",
      "Iteration 14, loss = 0.68731349\n",
      "Iteration 15, loss = 0.68652239\n",
      "Iteration 16, loss = 0.68555756\n",
      "Iteration 17, loss = 0.68441742\n",
      "Iteration 18, loss = 0.68311349\n",
      "Iteration 19, loss = 0.68166473\n",
      "Iteration 20, loss = 0.68009142\n",
      "Iteration 21, loss = 0.67840834\n",
      "Iteration 22, loss = 0.67662095\n",
      "Iteration 23, loss = 0.67472949\n",
      "Iteration 24, loss = 0.67273223\n",
      "Iteration 25, loss = 0.67062466\n",
      "Iteration 26, loss = 0.66839864\n",
      "Iteration 27, loss = 0.66604436\n",
      "Iteration 28, loss = 0.66355237\n",
      "Iteration 29, loss = 0.66091468\n",
      "Iteration 30, loss = 0.65812870\n",
      "Iteration 31, loss = 0.65519751\n",
      "Iteration 32, loss = 0.65212150\n",
      "Iteration 33, loss = 0.64888647\n",
      "Iteration 34, loss = 0.64545788\n",
      "Iteration 35, loss = 0.64178926\n",
      "Iteration 36, loss = 0.63783866\n",
      "Iteration 37, loss = 0.63357975\n",
      "Iteration 38, loss = 0.62900641\n",
      "Iteration 39, loss = 0.62413334\n",
      "Iteration 40, loss = 0.61899201\n",
      "Iteration 41, loss = 0.61362159\n",
      "Iteration 42, loss = 0.60805891\n",
      "Iteration 43, loss = 0.60233438\n",
      "Iteration 44, loss = 0.59647200\n",
      "Iteration 45, loss = 0.59048866\n",
      "Iteration 46, loss = 0.58439224\n",
      "Iteration 47, loss = 0.57818023\n",
      "Iteration 48, loss = 0.57184264\n",
      "Iteration 49, loss = 0.56537216\n",
      "Iteration 50, loss = 0.55877784\n",
      "Iteration 51, loss = 0.55208645\n",
      "Iteration 52, loss = 0.54532655\n",
      "Iteration 53, loss = 0.53851281\n",
      "Iteration 54, loss = 0.53164943\n",
      "Iteration 55, loss = 0.52473871\n",
      "Iteration 56, loss = 0.51778422\n",
      "Iteration 57, loss = 0.51079695\n",
      "Iteration 58, loss = 0.50380356\n",
      "Iteration 59, loss = 0.49684931\n",
      "Iteration 60, loss = 0.48998705\n",
      "Iteration 61, loss = 0.48325428\n",
      "Iteration 62, loss = 0.47665865\n",
      "Iteration 63, loss = 0.47018922\n",
      "Iteration 64, loss = 0.46383183\n",
      "Iteration 65, loss = 0.45756344\n",
      "Iteration 66, loss = 0.45135166\n",
      "Iteration 67, loss = 0.44518450\n",
      "Iteration 68, loss = 0.43908447\n",
      "Iteration 69, loss = 0.43309146\n",
      "Iteration 70, loss = 0.42723838\n",
      "Iteration 71, loss = 0.42153722\n",
      "Iteration 72, loss = 0.41598228\n",
      "Iteration 73, loss = 0.41055897\n",
      "Iteration 74, loss = 0.40524232\n",
      "Iteration 75, loss = 0.40001240\n",
      "Iteration 76, loss = 0.39488197\n",
      "Iteration 77, loss = 0.38989272\n",
      "Iteration 78, loss = 0.38508618\n",
      "Iteration 79, loss = 0.38046429\n",
      "Iteration 80, loss = 0.37599910\n",
      "Iteration 81, loss = 0.37167192\n",
      "Iteration 82, loss = 0.36747299\n",
      "Iteration 83, loss = 0.36339115\n",
      "Iteration 84, loss = 0.35941675\n",
      "Iteration 85, loss = 0.35554669\n",
      "Iteration 86, loss = 0.35178257\n",
      "Iteration 87, loss = 0.34812437\n",
      "Iteration 88, loss = 0.34456530\n",
      "Iteration 89, loss = 0.34110002\n",
      "Iteration 90, loss = 0.33773298\n",
      "Iteration 91, loss = 0.33447565\n",
      "Iteration 92, loss = 0.33132512\n",
      "Iteration 93, loss = 0.32825568\n",
      "Iteration 94, loss = 0.32525210\n",
      "Iteration 95, loss = 0.32231501\n",
      "Iteration 96, loss = 0.31944546\n",
      "Iteration 97, loss = 0.31664022\n",
      "Iteration 98, loss = 0.31388918\n",
      "Iteration 99, loss = 0.31118455\n",
      "Iteration 100, loss = 0.30853014\n",
      "Iteration 101, loss = 0.30593964\n",
      "Iteration 102, loss = 0.30341517\n",
      "Iteration 103, loss = 0.30093502\n",
      "Iteration 104, loss = 0.29848559\n",
      "Iteration 105, loss = 0.29606305\n",
      "Iteration 106, loss = 0.29366769\n",
      "Iteration 107, loss = 0.29130530\n",
      "Iteration 108, loss = 0.28898035\n",
      "Iteration 109, loss = 0.28668773\n",
      "Iteration 110, loss = 0.28441515\n",
      "Iteration 111, loss = 0.28214888\n",
      "Iteration 112, loss = 0.27988171\n",
      "Iteration 113, loss = 0.27761734\n",
      "Iteration 114, loss = 0.27536951\n",
      "Iteration 115, loss = 0.27315357\n",
      "Iteration 116, loss = 0.27095602\n",
      "Iteration 117, loss = 0.26875515\n",
      "Iteration 118, loss = 0.26655012\n",
      "Iteration 119, loss = 0.26435269\n",
      "Iteration 120, loss = 0.26216875\n",
      "Iteration 121, loss = 0.25999158\n",
      "Iteration 122, loss = 0.25780920\n",
      "Iteration 123, loss = 0.25561784\n",
      "Iteration 124, loss = 0.25342358\n",
      "Iteration 125, loss = 0.25123378\n",
      "Iteration 126, loss = 0.24904632\n",
      "Iteration 127, loss = 0.24685059\n",
      "Iteration 128, loss = 0.24464087\n",
      "Iteration 129, loss = 0.24242117\n",
      "Iteration 130, loss = 0.24019638\n",
      "Iteration 131, loss = 0.23796588\n",
      "Iteration 132, loss = 0.23572929\n",
      "Iteration 133, loss = 0.23349643\n",
      "Iteration 134, loss = 0.23125683\n",
      "Iteration 135, loss = 0.22900338\n",
      "Iteration 136, loss = 0.22674856\n",
      "Iteration 137, loss = 0.22450301\n",
      "Iteration 138, loss = 0.22226105\n",
      "Iteration 139, loss = 0.22001719\n",
      "Iteration 140, loss = 0.21777317\n",
      "Iteration 141, loss = 0.21553302\n",
      "Iteration 142, loss = 0.21330048\n",
      "Iteration 143, loss = 0.21107814\n",
      "Iteration 144, loss = 0.20886524\n",
      "Iteration 145, loss = 0.20665631\n",
      "Iteration 146, loss = 0.20444867\n",
      "Iteration 147, loss = 0.20224144\n",
      "Iteration 148, loss = 0.20003100\n",
      "Iteration 149, loss = 0.19781561\n",
      "Iteration 150, loss = 0.19559237\n",
      "Iteration 151, loss = 0.19336124\n",
      "Iteration 152, loss = 0.19112969\n",
      "Iteration 153, loss = 0.18893084\n",
      "Iteration 154, loss = 0.18676107\n",
      "Iteration 155, loss = 0.18461746\n",
      "Iteration 156, loss = 0.18247374\n",
      "Iteration 157, loss = 0.18026458\n",
      "Iteration 158, loss = 0.17801924\n",
      "Iteration 159, loss = 0.17586136\n",
      "Iteration 160, loss = 0.17380572\n",
      "Iteration 161, loss = 0.17174623\n",
      "Iteration 162, loss = 0.16963506\n",
      "Iteration 163, loss = 0.16756250\n",
      "Iteration 164, loss = 0.16557610\n",
      "Iteration 165, loss = 0.16360744\n",
      "Iteration 166, loss = 0.16160780\n",
      "Iteration 167, loss = 0.15963383\n",
      "Iteration 168, loss = 0.15773445\n",
      "Iteration 169, loss = 0.15586771\n",
      "Iteration 170, loss = 0.15397877\n",
      "Iteration 171, loss = 0.15208288\n",
      "Iteration 172, loss = 0.15022183\n",
      "Iteration 173, loss = 0.14839586\n",
      "Iteration 174, loss = 0.14657008\n",
      "Iteration 175, loss = 0.14473012\n",
      "Iteration 176, loss = 0.14289765\n",
      "Iteration 177, loss = 0.14108582\n",
      "Iteration 178, loss = 0.13928182\n",
      "Iteration 179, loss = 0.13746974\n",
      "Iteration 180, loss = 0.13565449\n",
      "Iteration 181, loss = 0.13384800\n",
      "Iteration 182, loss = 0.13205805\n",
      "Iteration 183, loss = 0.13028152\n",
      "Iteration 184, loss = 0.12851083\n",
      "Iteration 185, loss = 0.12673986\n",
      "Iteration 186, loss = 0.12496488\n",
      "Iteration 187, loss = 0.12318611\n",
      "Iteration 188, loss = 0.12140348\n",
      "Iteration 189, loss = 0.11961393\n",
      "Iteration 190, loss = 0.11780687\n",
      "Iteration 191, loss = 0.11597013\n",
      "Iteration 192, loss = 0.11410612\n",
      "Iteration 193, loss = 0.11223079\n",
      "Iteration 194, loss = 0.11038327\n",
      "Iteration 195, loss = 0.10858265\n",
      "Iteration 196, loss = 0.10682887\n",
      "Iteration 197, loss = 0.10510348\n",
      "Iteration 198, loss = 0.10339726\n",
      "Iteration 199, loss = 0.10171490\n",
      "Iteration 200, loss = 0.10007445\n",
      "Iteration 201, loss = 0.09849157\n",
      "Iteration 202, loss = 0.09696796\n",
      "Iteration 203, loss = 0.09549193\n",
      "Iteration 204, loss = 0.09404899\n",
      "Iteration 205, loss = 0.09263376\n",
      "Iteration 206, loss = 0.09125005\n",
      "Iteration 207, loss = 0.08990314\n",
      "Iteration 208, loss = 0.08859442\n",
      "Iteration 209, loss = 0.08732240\n",
      "Iteration 210, loss = 0.08608501\n",
      "Iteration 211, loss = 0.08488158\n",
      "Iteration 212, loss = 0.08371197\n",
      "Iteration 213, loss = 0.08257698\n",
      "Iteration 214, loss = 0.08147664\n",
      "Iteration 215, loss = 0.08040987\n",
      "Iteration 216, loss = 0.07937547\n",
      "Iteration 217, loss = 0.07837190\n",
      "Iteration 218, loss = 0.07739836\n",
      "Iteration 219, loss = 0.07645454\n",
      "Iteration 220, loss = 0.07553966\n",
      "Iteration 221, loss = 0.07465267\n",
      "Iteration 222, loss = 0.07379264\n",
      "Iteration 223, loss = 0.07295874\n",
      "Iteration 224, loss = 0.07215002\n",
      "Iteration 225, loss = 0.07136554\n",
      "Iteration 226, loss = 0.07060473\n",
      "Iteration 227, loss = 0.06986712\n",
      "Iteration 228, loss = 0.06915196\n",
      "Iteration 229, loss = 0.06845845\n",
      "Iteration 230, loss = 0.06778570\n",
      "Iteration 231, loss = 0.06713268\n",
      "Iteration 232, loss = 0.06649892\n",
      "Iteration 233, loss = 0.06588462\n",
      "Iteration 234, loss = 0.06528902\n",
      "Iteration 235, loss = 0.06471082\n",
      "Iteration 236, loss = 0.06414940\n",
      "Iteration 237, loss = 0.06360447\n",
      "Iteration 238, loss = 0.06307561\n",
      "Iteration 239, loss = 0.06256227\n",
      "Iteration 240, loss = 0.06206383\n",
      "Iteration 241, loss = 0.06157965\n",
      "Iteration 242, loss = 0.06110931\n",
      "Iteration 243, loss = 0.06065246\n",
      "Iteration 244, loss = 0.06020871\n",
      "Iteration 245, loss = 0.05977752\n",
      "Iteration 246, loss = 0.05935817\n",
      "Iteration 247, loss = 0.05895024\n",
      "Iteration 248, loss = 0.05855346\n",
      "Iteration 249, loss = 0.05816735\n",
      "Iteration 250, loss = 0.05779140\n",
      "Iteration 251, loss = 0.05742530\n",
      "Iteration 252, loss = 0.05706886\n",
      "Iteration 253, loss = 0.05672165\n",
      "Iteration 254, loss = 0.05638330\n",
      "Iteration 255, loss = 0.05605364\n",
      "Iteration 256, loss = 0.05573232\n",
      "Iteration 257, loss = 0.05541902\n",
      "Iteration 258, loss = 0.05511359\n",
      "Iteration 259, loss = 0.05481576\n",
      "Iteration 260, loss = 0.05452524\n",
      "Iteration 261, loss = 0.05424186\n",
      "Iteration 262, loss = 0.05396543\n",
      "Iteration 263, loss = 0.05369569\n",
      "Iteration 264, loss = 0.05343248\n",
      "Iteration 265, loss = 0.05317557\n",
      "Iteration 266, loss = 0.05292477\n",
      "Iteration 267, loss = 0.05267987\n",
      "Iteration 268, loss = 0.05244068\n",
      "Iteration 269, loss = 0.05220698\n",
      "Iteration 270, loss = 0.05197861\n",
      "Iteration 271, loss = 0.05175535\n",
      "Iteration 272, loss = 0.05153704\n",
      "Iteration 273, loss = 0.05132352\n",
      "Iteration 274, loss = 0.05111461\n",
      "Iteration 275, loss = 0.05091017\n",
      "Iteration 276, loss = 0.05071005\n",
      "Iteration 277, loss = 0.05051413\n",
      "Iteration 278, loss = 0.05032226\n",
      "Iteration 279, loss = 0.05013434\n",
      "Iteration 280, loss = 0.04995023\n",
      "Iteration 281, loss = 0.04976983\n",
      "Iteration 282, loss = 0.04959304\n",
      "Iteration 283, loss = 0.04941973\n",
      "Iteration 284, loss = 0.04924982\n",
      "Iteration 285, loss = 0.04908319\n",
      "Iteration 286, loss = 0.04891975\n",
      "Iteration 287, loss = 0.04875941\n",
      "Iteration 288, loss = 0.04860207\n",
      "Iteration 289, loss = 0.04844763\n",
      "Iteration 290, loss = 0.04829601\n",
      "Iteration 291, loss = 0.04814712\n",
      "Iteration 292, loss = 0.04800087\n",
      "Iteration 293, loss = 0.04785718\n",
      "Iteration 294, loss = 0.04771598\n",
      "Iteration 295, loss = 0.04757719\n",
      "Iteration 296, loss = 0.04744074\n",
      "Iteration 297, loss = 0.04730657\n",
      "Iteration 298, loss = 0.04717462\n",
      "Iteration 299, loss = 0.04704483\n",
      "Iteration 300, loss = 0.04691716\n",
      "Iteration 301, loss = 0.04679154\n",
      "Iteration 302, loss = 0.04666795\n",
      "Iteration 303, loss = 0.04654632\n",
      "Iteration 304, loss = 0.04642663\n",
      "Iteration 305, loss = 0.04630883\n",
      "Iteration 306, loss = 0.04619287\n",
      "Iteration 307, loss = 0.04607872\n",
      "Iteration 308, loss = 0.04596634\n",
      "Iteration 309, loss = 0.04585568\n",
      "Iteration 310, loss = 0.04574671\n",
      "Iteration 311, loss = 0.04563939\n",
      "Iteration 312, loss = 0.04553367\n",
      "Iteration 313, loss = 0.04542954\n",
      "Iteration 314, loss = 0.04532693\n",
      "Iteration 315, loss = 0.04522583\n",
      "Iteration 316, loss = 0.04512619\n",
      "Iteration 317, loss = 0.04502797\n",
      "Iteration 318, loss = 0.04493114\n",
      "Iteration 319, loss = 0.04483566\n",
      "Iteration 320, loss = 0.04474148\n",
      "Iteration 321, loss = 0.04464856\n",
      "Iteration 322, loss = 0.04455687\n",
      "Iteration 323, loss = 0.04446633\n",
      "Iteration 324, loss = 0.04437687\n",
      "Iteration 325, loss = 0.04428842\n",
      "Iteration 326, loss = 0.04420111\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.92      0.65      0.76        17\n",
      "      Normal       0.79      0.96      0.86        23\n",
      "\n",
      "    accuracy                           0.82        40\n",
      "   macro avg       0.85      0.80      0.81        40\n",
      "weighted avg       0.84      0.82      0.82        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "data1 = pd.read_csv(\"Dataset_spine.csv\")\n",
    "data1 = data1.drop(['Unnamed: 13'], axis=1)\n",
    "data1.rename(columns = {\n",
    "    \"Col1\" : \"pelvic_incidence\", \n",
    "    \"Col2\" : \"pelvic_tilt\",\n",
    "    \"Col3\" : \"lumbar_lordosis_angle\",\n",
    "    \"Col4\" : \"sacral_slope\", \n",
    "    \"Col5\" : \"pelvic_radius\",\n",
    "    \"Col6\" : \"degree_spondylolisthesis\", \n",
    "    \"Col7\" : \"pelvic_slope\",\n",
    "    \"Col8\" : \"direct_tilt\",\n",
    "    \"Col9\" : \"thoracic_slope\", \n",
    "    \"Col10\" :\"cervical_tilt\", \n",
    "    \"Col11\" : \"sacrum_angle\",\n",
    "    \"Col12\" : \"scoliosis_slope\", \n",
    "    \"Class_att\" : \"class\"}, inplace=True)\n",
    "\n",
    "new_data_copy = data1\n",
    "\n",
    "new_data3 = new_data_copy.copy()\n",
    "\n",
    "range_list = random.sample(range(210), 110)\n",
    "\n",
    "for i in range_list:\n",
    "    new_data3.drop(i,axis=0,inplace=True)\n",
    "\n",
    "y = new_data3['class']\n",
    "x = new_data3.drop(['class'], axis = 1)\n",
    "print(y.value_counts())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que as precisões e a acurácia aumentaram significativamente com relação aos primeiros testes deste ralatório, antes das melhorias sujeridas. Entretanto, apesar de os resultados serem satisfatórios, algumas das melhorias propostas acima, apresentam resultados melhores do que esta última abordagem.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, uniremos a melhor abordagem para igualar a quantidade de casos que é retirar os 110 primeiros casos \"Abnormal\" do dataset com o melhor resultado das melhorias sugeridas na manipulação de parâmetros do MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal      100\n",
      "Abnormal    100\n",
      "Name: class, dtype: int64\n",
      "Iteration 1, loss = 0.55375206\n",
      "Iteration 2, loss = 8.87932277\n",
      "Iteration 3, loss = 2.40768472\n",
      "Iteration 4, loss = 0.83024323\n",
      "Iteration 5, loss = 2.80373395\n",
      "Iteration 6, loss = 2.19747852\n",
      "Iteration 7, loss = 0.84871990\n",
      "Iteration 8, loss = 0.19363533\n",
      "Iteration 9, loss = 0.22319015\n",
      "Iteration 10, loss = 0.60768334\n",
      "Iteration 11, loss = 0.93623898\n",
      "Iteration 12, loss = 0.79453655\n",
      "Iteration 13, loss = 0.46104316\n",
      "Iteration 14, loss = 0.25078411\n",
      "Iteration 15, loss = 0.16912668\n",
      "Iteration 16, loss = 0.18179545\n",
      "Iteration 17, loss = 0.26101224\n",
      "Iteration 18, loss = 0.34544551\n",
      "Iteration 19, loss = 0.38223024\n",
      "Iteration 20, loss = 0.35676346\n",
      "Iteration 21, loss = 0.29218342\n",
      "Iteration 22, loss = 0.21975913\n",
      "Iteration 23, loss = 0.16907345\n",
      "Iteration 24, loss = 0.15155934\n",
      "Iteration 25, loss = 0.16478645\n",
      "Iteration 26, loss = 0.19392532\n",
      "Iteration 27, loss = 0.21190076\n",
      "Iteration 28, loss = 0.20715269\n",
      "Iteration 29, loss = 0.18248388\n",
      "Iteration 30, loss = 0.15071716\n",
      "Iteration 31, loss = 0.12686696\n",
      "Iteration 32, loss = 0.11732048\n",
      "Iteration 33, loss = 0.11627462\n",
      "Iteration 34, loss = 0.11834224\n",
      "Iteration 35, loss = 0.12059992\n",
      "Iteration 36, loss = 0.11997451\n",
      "Iteration 37, loss = 0.11454120\n",
      "Iteration 38, loss = 0.10440556\n",
      "Iteration 39, loss = 0.09170629\n",
      "Iteration 40, loss = 0.07969910\n",
      "Iteration 41, loss = 0.07275174\n",
      "Iteration 42, loss = 0.07238863\n",
      "Iteration 43, loss = 0.07560334\n",
      "Iteration 44, loss = 0.07635968\n",
      "Iteration 45, loss = 0.07092027\n",
      "Iteration 46, loss = 0.06200116\n",
      "Iteration 47, loss = 0.05432242\n",
      "Iteration 48, loss = 0.05170220\n",
      "Iteration 49, loss = 0.05275300\n",
      "Iteration 50, loss = 0.05405256\n",
      "Iteration 51, loss = 0.05165580\n",
      "Iteration 52, loss = 0.04593094\n",
      "Iteration 53, loss = 0.04110998\n",
      "Iteration 54, loss = 0.04025744\n",
      "Iteration 55, loss = 0.04205225\n",
      "Iteration 56, loss = 0.04173592\n",
      "Iteration 57, loss = 0.03804627\n",
      "Iteration 58, loss = 0.03442645\n",
      "Iteration 59, loss = 0.03359619\n",
      "Iteration 60, loss = 0.03451942\n",
      "Iteration 61, loss = 0.03404556\n",
      "Iteration 62, loss = 0.03162805\n",
      "Iteration 63, loss = 0.02967361\n",
      "Iteration 64, loss = 0.02970459\n",
      "Iteration 65, loss = 0.03015295\n",
      "Iteration 66, loss = 0.02903795\n",
      "Iteration 67, loss = 0.02707565\n",
      "Iteration 68, loss = 0.02620235\n",
      "Iteration 69, loss = 0.02623341\n",
      "Iteration 70, loss = 0.02579593\n",
      "Iteration 71, loss = 0.02444789\n",
      "Iteration 72, loss = 0.02321555\n",
      "Iteration 73, loss = 0.02283025\n",
      "Iteration 74, loss = 0.02259756\n",
      "Iteration 75, loss = 0.02169002\n",
      "Iteration 76, loss = 0.02062388\n",
      "Iteration 77, loss = 0.02002937\n",
      "Iteration 78, loss = 0.01973010\n",
      "Iteration 79, loss = 0.01917510\n",
      "Iteration 80, loss = 0.01846180\n",
      "Iteration 81, loss = 0.01791253\n",
      "Iteration 82, loss = 0.01755910\n",
      "Iteration 83, loss = 0.01713228\n",
      "Iteration 84, loss = 0.01653674\n",
      "Iteration 85, loss = 0.01595316\n",
      "Iteration 86, loss = 0.01552629\n",
      "Iteration 87, loss = 0.01511595\n",
      "Iteration 88, loss = 0.01463716\n",
      "Iteration 89, loss = 0.01417067\n",
      "Iteration 90, loss = 0.01374218\n",
      "Iteration 91, loss = 0.01344472\n",
      "Iteration 92, loss = 0.01301192\n",
      "Iteration 93, loss = 0.01255887\n",
      "Iteration 94, loss = 0.01214232\n",
      "Iteration 95, loss = 0.01172417\n",
      "Iteration 96, loss = 0.01128115\n",
      "Iteration 97, loss = 0.01089031\n",
      "Iteration 98, loss = 0.01066618\n",
      "Iteration 99, loss = 0.01043640\n",
      "Iteration 100, loss = 0.01013760\n",
      "Iteration 101, loss = 0.00983049\n",
      "Iteration 102, loss = 0.00958529\n",
      "Iteration 103, loss = 0.00930881\n",
      "Iteration 104, loss = 0.00899739\n",
      "Iteration 105, loss = 0.00872704\n",
      "Iteration 106, loss = 0.00851748\n",
      "Iteration 107, loss = 0.00829321\n",
      "Iteration 108, loss = 0.00804554\n",
      "Iteration 109, loss = 0.00783349\n",
      "Iteration 110, loss = 0.00764545\n",
      "Iteration 111, loss = 0.00743198\n",
      "Iteration 112, loss = 0.00723173\n",
      "Iteration 113, loss = 0.00704522\n",
      "Iteration 114, loss = 0.00686008\n",
      "Iteration 115, loss = 0.00670365\n",
      "Iteration 116, loss = 0.00650350\n",
      "Iteration 117, loss = 0.00632570\n",
      "Iteration 118, loss = 0.00616633\n",
      "Iteration 119, loss = 0.00600320\n",
      "Iteration 120, loss = 0.00581695\n",
      "Iteration 121, loss = 0.00566412\n",
      "Iteration 122, loss = 0.00552539\n",
      "Iteration 123, loss = 0.00537572\n",
      "Iteration 124, loss = 0.00522504\n",
      "Iteration 125, loss = 0.00508893\n",
      "Iteration 126, loss = 0.00494696\n",
      "Iteration 127, loss = 0.00481491\n",
      "Iteration 128, loss = 0.00468816\n",
      "Iteration 129, loss = 0.00456816\n",
      "Iteration 130, loss = 0.00445731\n",
      "Iteration 131, loss = 0.00434005\n",
      "Iteration 132, loss = 0.00421295\n",
      "Iteration 133, loss = 0.00409770\n",
      "Iteration 134, loss = 0.00399211\n",
      "Iteration 135, loss = 0.00388857\n",
      "Iteration 136, loss = 0.00379214\n",
      "Iteration 137, loss = 0.00369818\n",
      "Iteration 138, loss = 0.00358540\n",
      "Iteration 139, loss = 0.00349422\n",
      "Iteration 140, loss = 0.00340724\n",
      "Iteration 141, loss = 0.00332326\n",
      "Iteration 142, loss = 0.00323368\n",
      "Iteration 143, loss = 0.00315222\n",
      "Iteration 144, loss = 0.00304229\n",
      "Iteration 145, loss = 0.00298570\n",
      "Iteration 146, loss = 0.00290709\n",
      "Iteration 147, loss = 0.00282366\n",
      "Iteration 148, loss = 0.00274790\n",
      "Iteration 149, loss = 0.00267579\n",
      "Iteration 150, loss = 0.00259883\n",
      "Iteration 151, loss = 0.00253092\n",
      "Iteration 152, loss = 0.00246310\n",
      "Iteration 153, loss = 0.00238287\n",
      "Iteration 154, loss = 0.00233162\n",
      "Iteration 155, loss = 0.00227525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       1.00      1.00      1.00        10\n",
      "      Normal       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv(\"Dataset_spine.csv\")\n",
    "data1 = data1.drop(['Unnamed: 13'], axis=1)\n",
    "data1.rename(columns = {\n",
    "    \"Col1\" : \"pelvic_incidence\", \n",
    "    \"Col2\" : \"pelvic_tilt\",\n",
    "    \"Col3\" : \"lumbar_lordosis_angle\",\n",
    "    \"Col4\" : \"sacral_slope\", \n",
    "    \"Col5\" : \"pelvic_radius\",\n",
    "    \"Col6\" : \"degree_spondylolisthesis\", \n",
    "    \"Col7\" : \"pelvic_slope\",\n",
    "    \"Col8\" : \"direct_tilt\",\n",
    "    \"Col9\" : \"thoracic_slope\", \n",
    "    \"Col10\" :\"cervical_tilt\", \n",
    "    \"Col11\" : \"sacrum_angle\",\n",
    "    \"Col12\" : \"scoliosis_slope\", \n",
    "    \"Class_att\" : \"class\"}, inplace=True)\n",
    "\n",
    "new_data_copy = data1\n",
    "\n",
    "new_data3 = new_data_copy.copy()\n",
    "\n",
    "for i in range(110):\n",
    "    new_data3.drop((i+1),axis=0,inplace=True)\n",
    "\n",
    "y = new_data3['class']\n",
    "x = new_data3.drop(['class'], axis = 1)\n",
    "print(y.value_counts())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.1, random_state=13)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,300,500), max_iter=500, alpha=0.0001, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que, apesar de significante melhora com relação aos primeiros experimentos e de resultados similares aos experimentos que testaram apenas melhorias nos parâmetros do MLP, este experimento ainda está aquém do resultado do experimento 7, onde apenas o dataset foi manipulado não randomicamente de forma a igualar o número de casos \"Abnormal\" e \"Normal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTO 14**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora tentaremos uma abordagem similar à do experimento anterior, unindo as melhorias nos parâmetros MLP com o ato de igualar o número de casos, mas agora de modo randômico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Normal      100\n",
      "Abnormal    100\n",
      "Name: class, dtype: int64\n",
      "Iteration 1, loss = 1.92475204\n",
      "Iteration 2, loss = 9.75004652\n",
      "Iteration 3, loss = 6.27743985\n",
      "Iteration 4, loss = 0.81780664\n",
      "Iteration 5, loss = 2.62682503\n",
      "Iteration 6, loss = 3.83733028\n",
      "Iteration 7, loss = 3.14033956\n",
      "Iteration 8, loss = 1.75878495\n",
      "Iteration 9, loss = 0.74547971\n",
      "Iteration 10, loss = 0.97991562\n",
      "Iteration 11, loss = 1.82507418\n",
      "Iteration 12, loss = 1.56744052\n",
      "Iteration 13, loss = 0.82245317\n",
      "Iteration 14, loss = 0.58697168\n",
      "Iteration 15, loss = 0.77811539\n",
      "Iteration 16, loss = 0.97663093\n",
      "Iteration 17, loss = 1.05262099\n",
      "Iteration 18, loss = 0.98623994\n",
      "Iteration 19, loss = 0.81701065\n",
      "Iteration 20, loss = 0.61119316\n",
      "Iteration 21, loss = 0.45456961\n",
      "Iteration 22, loss = 0.45969173\n",
      "Iteration 23, loss = 0.60597912\n",
      "Iteration 24, loss = 0.66032198\n",
      "Iteration 25, loss = 0.53749211\n",
      "Iteration 26, loss = 0.39426231\n",
      "Iteration 27, loss = 0.37884415\n",
      "Iteration 28, loss = 0.44192004\n",
      "Iteration 29, loss = 0.48962869\n",
      "Iteration 30, loss = 0.48381553\n",
      "Iteration 31, loss = 0.42947305\n",
      "Iteration 32, loss = 0.36605166\n",
      "Iteration 33, loss = 0.34467190\n",
      "Iteration 34, loss = 0.37886557\n",
      "Iteration 35, loss = 0.41561521\n",
      "Iteration 36, loss = 0.40306248\n",
      "Iteration 37, loss = 0.35930621\n",
      "Iteration 38, loss = 0.33640558\n",
      "Iteration 39, loss = 0.34898274\n",
      "Iteration 40, loss = 0.36825698\n",
      "Iteration 41, loss = 0.36903619\n",
      "Iteration 42, loss = 0.34983243\n",
      "Iteration 43, loss = 0.32802565\n",
      "Iteration 44, loss = 0.32100164\n",
      "Iteration 45, loss = 0.33020860\n",
      "Iteration 46, loss = 0.33845762\n",
      "Iteration 47, loss = 0.33248863\n",
      "Iteration 48, loss = 0.31890240\n",
      "Iteration 49, loss = 0.31177775\n",
      "Iteration 50, loss = 0.31462509\n",
      "Iteration 51, loss = 0.31963826\n",
      "Iteration 52, loss = 0.31873144\n",
      "Iteration 53, loss = 0.31165913\n",
      "Iteration 54, loss = 0.30475418\n",
      "Iteration 55, loss = 0.30325943\n",
      "Iteration 56, loss = 0.30524561\n",
      "Iteration 57, loss = 0.30495442\n",
      "Iteration 58, loss = 0.29992595\n",
      "Iteration 59, loss = 0.29481389\n",
      "Iteration 60, loss = 0.29322268\n",
      "Iteration 61, loss = 0.29384357\n",
      "Iteration 62, loss = 0.29283463\n",
      "Iteration 63, loss = 0.28907608\n",
      "Iteration 64, loss = 0.28597928\n",
      "Iteration 65, loss = 0.28506503\n",
      "Iteration 66, loss = 0.28465449\n",
      "Iteration 67, loss = 0.28262674\n",
      "Iteration 68, loss = 0.27955994\n",
      "Iteration 69, loss = 0.27736250\n",
      "Iteration 70, loss = 0.27610948\n",
      "Iteration 71, loss = 0.27481155\n",
      "Iteration 72, loss = 0.27257443\n",
      "Iteration 73, loss = 0.26990798\n",
      "Iteration 74, loss = 0.26860459\n",
      "Iteration 75, loss = 0.26779048\n",
      "Iteration 76, loss = 0.26592972\n",
      "Iteration 77, loss = 0.26351522\n",
      "Iteration 78, loss = 0.26184157\n",
      "Iteration 79, loss = 0.26041549\n",
      "Iteration 80, loss = 0.25877310\n",
      "Iteration 81, loss = 0.25680079\n",
      "Iteration 82, loss = 0.25507394\n",
      "Iteration 83, loss = 0.25367295\n",
      "Iteration 84, loss = 0.25199849\n",
      "Iteration 85, loss = 0.25007094\n",
      "Iteration 86, loss = 0.24835146\n",
      "Iteration 87, loss = 0.24666851\n",
      "Iteration 88, loss = 0.24477724\n",
      "Iteration 89, loss = 0.24311996\n",
      "Iteration 90, loss = 0.24171683\n",
      "Iteration 91, loss = 0.24004148\n",
      "Iteration 92, loss = 0.23818080\n",
      "Iteration 93, loss = 0.23643899\n",
      "Iteration 94, loss = 0.23483703\n",
      "Iteration 95, loss = 0.23306514\n",
      "Iteration 96, loss = 0.23127567\n",
      "Iteration 97, loss = 0.22936248\n",
      "Iteration 98, loss = 0.22771181\n",
      "Iteration 99, loss = 0.22586029\n",
      "Iteration 100, loss = 0.22387782\n",
      "Iteration 101, loss = 0.22211852\n",
      "Iteration 102, loss = 0.22041968\n",
      "Iteration 103, loss = 0.21856037\n",
      "Iteration 104, loss = 0.21670743\n",
      "Iteration 105, loss = 0.21486746\n",
      "Iteration 106, loss = 0.21299123\n",
      "Iteration 107, loss = 0.21125162\n",
      "Iteration 108, loss = 0.20928327\n",
      "Iteration 109, loss = 0.20726157\n",
      "Iteration 110, loss = 0.20526207\n",
      "Iteration 111, loss = 0.20326266\n",
      "Iteration 112, loss = 0.20133507\n",
      "Iteration 113, loss = 0.19963093\n",
      "Iteration 114, loss = 0.19788451\n",
      "Iteration 115, loss = 0.19610889\n",
      "Iteration 116, loss = 0.19429789\n",
      "Iteration 117, loss = 0.19247616\n",
      "Iteration 118, loss = 0.19070932\n",
      "Iteration 119, loss = 0.18871191\n",
      "Iteration 120, loss = 0.18723632\n",
      "Iteration 121, loss = 0.18528700\n",
      "Iteration 122, loss = 0.18353826\n",
      "Iteration 123, loss = 0.18197128\n",
      "Iteration 124, loss = 0.18028174\n",
      "Iteration 125, loss = 0.17851322\n",
      "Iteration 126, loss = 0.17650479\n",
      "Iteration 127, loss = 0.17480881\n",
      "Iteration 128, loss = 0.17295188\n",
      "Iteration 129, loss = 0.17125472\n",
      "Iteration 130, loss = 0.16948453\n",
      "Iteration 131, loss = 0.16773448\n",
      "Iteration 132, loss = 0.16583812\n",
      "Iteration 133, loss = 0.16439552\n",
      "Iteration 134, loss = 0.16241296\n",
      "Iteration 135, loss = 0.16059549\n",
      "Iteration 136, loss = 0.15906116\n",
      "Iteration 137, loss = 0.15717196\n",
      "Iteration 138, loss = 0.15517649\n",
      "Iteration 139, loss = 0.15374057\n",
      "Iteration 140, loss = 0.15185922\n",
      "Iteration 141, loss = 0.15011072\n",
      "Iteration 142, loss = 0.14821000\n",
      "Iteration 143, loss = 0.14670355\n",
      "Iteration 144, loss = 0.14475909\n",
      "Iteration 145, loss = 0.14286031\n",
      "Iteration 146, loss = 0.14109778\n",
      "Iteration 147, loss = 0.13950917\n",
      "Iteration 148, loss = 0.13742923\n",
      "Iteration 149, loss = 0.13582597\n",
      "Iteration 150, loss = 0.13400323\n",
      "Iteration 151, loss = 0.13232161\n",
      "Iteration 152, loss = 0.13015240\n",
      "Iteration 153, loss = 0.12847289\n",
      "Iteration 154, loss = 0.12662178\n",
      "Iteration 155, loss = 0.12472537\n",
      "Iteration 156, loss = 0.12287245\n",
      "Iteration 157, loss = 0.12120078\n",
      "Iteration 158, loss = 0.11913546\n",
      "Iteration 159, loss = 0.11727033\n",
      "Iteration 160, loss = 0.11597417\n",
      "Iteration 161, loss = 0.11363828\n",
      "Iteration 162, loss = 0.11207102\n",
      "Iteration 163, loss = 0.11047098\n",
      "Iteration 164, loss = 0.10847675\n",
      "Iteration 165, loss = 0.10670712\n",
      "Iteration 166, loss = 0.10483289\n",
      "Iteration 167, loss = 0.10313963\n",
      "Iteration 168, loss = 0.10148388\n",
      "Iteration 169, loss = 0.09948277\n",
      "Iteration 170, loss = 0.09809201\n",
      "Iteration 171, loss = 0.09667926\n",
      "Iteration 172, loss = 0.09507539\n",
      "Iteration 173, loss = 0.09348683\n",
      "Iteration 174, loss = 0.09167135\n",
      "Iteration 175, loss = 0.09007892\n",
      "Iteration 176, loss = 0.08833807\n",
      "Iteration 177, loss = 0.08691932\n",
      "Iteration 178, loss = 0.08549651\n",
      "Iteration 179, loss = 0.08367902\n",
      "Iteration 180, loss = 0.08222718\n",
      "Iteration 181, loss = 0.08065454\n",
      "Iteration 182, loss = 0.07924874\n",
      "Iteration 183, loss = 0.07793125\n",
      "Iteration 184, loss = 0.07667265\n",
      "Iteration 185, loss = 0.07555165\n",
      "Iteration 186, loss = 0.07410589\n",
      "Iteration 187, loss = 0.07261036\n",
      "Iteration 188, loss = 0.07091517\n",
      "Iteration 189, loss = 0.06942657\n",
      "Iteration 190, loss = 0.06847058\n",
      "Iteration 191, loss = 0.06717514\n",
      "Iteration 192, loss = 0.06580806\n",
      "Iteration 193, loss = 0.06448459\n",
      "Iteration 194, loss = 0.06334689\n",
      "Iteration 195, loss = 0.06190991\n",
      "Iteration 196, loss = 0.06062716\n",
      "Iteration 197, loss = 0.05951645\n",
      "Iteration 198, loss = 0.05819993\n",
      "Iteration 199, loss = 0.05720883\n",
      "Iteration 200, loss = 0.05589379\n",
      "Iteration 201, loss = 0.05477713\n",
      "Iteration 202, loss = 0.05354677\n",
      "Iteration 203, loss = 0.05296134\n",
      "Iteration 204, loss = 0.05244245\n",
      "Iteration 205, loss = 0.05118236\n",
      "Iteration 206, loss = 0.04928562\n",
      "Iteration 207, loss = 0.04840928\n",
      "Iteration 208, loss = 0.04793073\n",
      "Iteration 209, loss = 0.04687995\n",
      "Iteration 210, loss = 0.04498121\n",
      "Iteration 211, loss = 0.04424278\n",
      "Iteration 212, loss = 0.04348980\n",
      "Iteration 213, loss = 0.04208432\n",
      "Iteration 214, loss = 0.04107653\n",
      "Iteration 215, loss = 0.04022328\n",
      "Iteration 216, loss = 0.03912477\n",
      "Iteration 217, loss = 0.03819981\n",
      "Iteration 218, loss = 0.03744726\n",
      "Iteration 219, loss = 0.03648064\n",
      "Iteration 220, loss = 0.03561818\n",
      "Iteration 221, loss = 0.03487744\n",
      "Iteration 222, loss = 0.03393164\n",
      "Iteration 223, loss = 0.03308652\n",
      "Iteration 224, loss = 0.03238167\n",
      "Iteration 225, loss = 0.03170396\n",
      "Iteration 226, loss = 0.03091584\n",
      "Iteration 227, loss = 0.03006006\n",
      "Iteration 228, loss = 0.02944977\n",
      "Iteration 229, loss = 0.02890661\n",
      "Iteration 230, loss = 0.02793797\n",
      "Iteration 231, loss = 0.02725868\n",
      "Iteration 232, loss = 0.02671524\n",
      "Iteration 233, loss = 0.02600044\n",
      "Iteration 234, loss = 0.02540936\n",
      "Iteration 235, loss = 0.02473956\n",
      "Iteration 236, loss = 0.02409180\n",
      "Iteration 237, loss = 0.02354606\n",
      "Iteration 238, loss = 0.02295405\n",
      "Iteration 239, loss = 0.02245650\n",
      "Iteration 240, loss = 0.02180013\n",
      "Iteration 241, loss = 0.02124459\n",
      "Iteration 242, loss = 0.02069625\n",
      "Iteration 243, loss = 0.02024828\n",
      "Iteration 244, loss = 0.01968648\n",
      "Iteration 245, loss = 0.01918124\n",
      "Iteration 246, loss = 0.01867627\n",
      "Iteration 247, loss = 0.01819280\n",
      "Iteration 248, loss = 0.01773742\n",
      "Iteration 249, loss = 0.01730227\n",
      "Iteration 250, loss = 0.01693807\n",
      "Iteration 251, loss = 0.01644515\n",
      "Iteration 252, loss = 0.01599060\n",
      "Iteration 253, loss = 0.01566161\n",
      "Iteration 254, loss = 0.01524985\n",
      "Iteration 255, loss = 0.01490245\n",
      "Iteration 256, loss = 0.01450358\n",
      "Iteration 257, loss = 0.01414142\n",
      "Iteration 258, loss = 0.01383894\n",
      "Iteration 259, loss = 0.01351473\n",
      "Iteration 260, loss = 0.01313350\n",
      "Iteration 261, loss = 0.01284229\n",
      "Iteration 262, loss = 0.01258224\n",
      "Iteration 263, loss = 0.01225203\n",
      "Iteration 264, loss = 0.01199760\n",
      "Iteration 265, loss = 0.01169836\n",
      "Iteration 266, loss = 0.01136533\n",
      "Iteration 267, loss = 0.01114203\n",
      "Iteration 268, loss = 0.01086996\n",
      "Iteration 269, loss = 0.01060460\n",
      "Iteration 270, loss = 0.01037647\n",
      "Iteration 271, loss = 0.01016987\n",
      "Iteration 272, loss = 0.00993463\n",
      "Iteration 273, loss = 0.00968462\n",
      "Iteration 274, loss = 0.00945429\n",
      "Iteration 275, loss = 0.00930506\n",
      "Iteration 276, loss = 0.00908265\n",
      "Iteration 277, loss = 0.00888435\n",
      "Iteration 278, loss = 0.00869391\n",
      "Iteration 279, loss = 0.00852405\n",
      "Iteration 280, loss = 0.00834921\n",
      "Iteration 281, loss = 0.00817948\n",
      "Iteration 282, loss = 0.00800845\n",
      "Iteration 283, loss = 0.00785424\n",
      "Iteration 284, loss = 0.00769786\n",
      "Iteration 285, loss = 0.00754311\n",
      "Iteration 286, loss = 0.00739617\n",
      "Iteration 287, loss = 0.00725265\n",
      "Iteration 288, loss = 0.00712472\n",
      "Iteration 289, loss = 0.00699009\n",
      "Iteration 290, loss = 0.00683472\n",
      "Iteration 291, loss = 0.00671428\n",
      "Iteration 292, loss = 0.00659767\n",
      "Iteration 293, loss = 0.00648099\n",
      "Iteration 294, loss = 0.00634279\n",
      "Iteration 295, loss = 0.00624014\n",
      "Iteration 296, loss = 0.00611311\n",
      "Iteration 297, loss = 0.00601009\n",
      "Iteration 298, loss = 0.00589586\n",
      "Iteration 299, loss = 0.00578843\n",
      "Iteration 300, loss = 0.00568159\n",
      "Iteration 301, loss = 0.00557580\n",
      "Iteration 302, loss = 0.00546739\n",
      "Iteration 303, loss = 0.00537023\n",
      "Iteration 304, loss = 0.00526982\n",
      "Iteration 305, loss = 0.00517094\n",
      "Iteration 306, loss = 0.00507128\n",
      "Iteration 307, loss = 0.00498359\n",
      "Iteration 308, loss = 0.00488598\n",
      "Iteration 309, loss = 0.00479751\n",
      "Iteration 310, loss = 0.00471126\n",
      "Iteration 311, loss = 0.00462431\n",
      "Iteration 312, loss = 0.00453379\n",
      "Iteration 313, loss = 0.00445544\n",
      "Iteration 314, loss = 0.00436840\n",
      "Iteration 315, loss = 0.00429643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.83      1.00      0.91        10\n",
      "      Normal       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.92      0.90      0.90        20\n",
      "weighted avg       0.92      0.90      0.90        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv(\"Dataset_spine.csv\")\n",
    "data1 = data1.drop(['Unnamed: 13'], axis=1)\n",
    "data1.rename(columns = {\n",
    "    \"Col1\" : \"pelvic_incidence\", \n",
    "    \"Col2\" : \"pelvic_tilt\",\n",
    "    \"Col3\" : \"lumbar_lordosis_angle\",\n",
    "    \"Col4\" : \"sacral_slope\", \n",
    "    \"Col5\" : \"pelvic_radius\",\n",
    "    \"Col6\" : \"degree_spondylolisthesis\", \n",
    "    \"Col7\" : \"pelvic_slope\",\n",
    "    \"Col8\" : \"direct_tilt\",\n",
    "    \"Col9\" : \"thoracic_slope\", \n",
    "    \"Col10\" :\"cervical_tilt\", \n",
    "    \"Col11\" : \"sacrum_angle\",\n",
    "    \"Col12\" : \"scoliosis_slope\", \n",
    "    \"Class_att\" : \"class\"}, inplace=True)\n",
    "\n",
    "new_data_copy = data1\n",
    "\n",
    "new_data3 = new_data_copy.copy()\n",
    "\n",
    "range_list = random.sample(range(210), 110)\n",
    "\n",
    "for i in range_list:\n",
    "    new_data3.drop(i,axis=0,inplace=True)\n",
    "\n",
    "y = new_data3['class']\n",
    "x = new_data3.drop(['class'], axis = 1)\n",
    "print(y.value_counts())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.1, random_state=13)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,300,500), max_iter=500, alpha=0.0001, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "Agora iremos verificar como ficam os valores de precision, accurary e recall, se mudarmos a proporção entre o set_train e set_test no caso em considerarços o DataSet original, o DataSet após a exclusão das 6 primeiras variáveis, e o DataSet após a exclusão de 110 casos anormais."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Mudando a proporção entre a quantidade de dados no conjunto de treinamento e a quantidade de dados no conjunto de teste. Deixaremos 40% para o conjunto de treinamento e 60% para o de teste."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "No caso com o DataSet original:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**EXPERIMENTO15**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['class']\n",
    "x = data.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.67084220\n",
      "Iteration 2, loss = 0.65056247\n",
      "Iteration 3, loss = 0.63627528\n",
      "Iteration 4, loss = 0.62758501\n",
      "Iteration 5, loss = 0.62371144\n",
      "Iteration 6, loss = 0.62340548\n",
      "Iteration 7, loss = 0.62505864\n",
      "Iteration 8, loss = 0.62707060\n",
      "Iteration 9, loss = 0.62831042\n",
      "Iteration 10, loss = 0.62829727\n",
      "Iteration 11, loss = 0.62706532\n",
      "Iteration 12, loss = 0.62493334\n",
      "Iteration 13, loss = 0.62231877\n",
      "Iteration 14, loss = 0.61961870\n",
      "Iteration 15, loss = 0.61714005\n",
      "Iteration 16, loss = 0.61506189\n",
      "Iteration 17, loss = 0.61342285\n",
      "Iteration 18, loss = 0.61213405\n",
      "Iteration 19, loss = 0.61101758\n",
      "Iteration 20, loss = 0.60986145\n",
      "Iteration 21, loss = 0.60847490\n",
      "Iteration 22, loss = 0.60673110\n",
      "Iteration 23, loss = 0.60458702\n",
      "Iteration 24, loss = 0.60207441\n",
      "Iteration 25, loss = 0.59927404\n",
      "Iteration 26, loss = 0.59628608\n",
      "Iteration 27, loss = 0.59319626\n",
      "Iteration 28, loss = 0.59004313\n",
      "Iteration 29, loss = 0.58680274\n",
      "Iteration 30, loss = 0.58339684\n",
      "Iteration 31, loss = 0.57971684\n",
      "Iteration 32, loss = 0.57565784\n",
      "Iteration 33, loss = 0.57115385\n",
      "Iteration 34, loss = 0.56619640\n",
      "Iteration 35, loss = 0.56082630\n",
      "Iteration 36, loss = 0.55510394\n",
      "Iteration 37, loss = 0.54907655\n",
      "Iteration 38, loss = 0.54275753\n",
      "Iteration 39, loss = 0.53612829\n",
      "Iteration 40, loss = 0.52916048\n",
      "Iteration 41, loss = 0.52183809\n",
      "Iteration 42, loss = 0.51416718\n",
      "Iteration 43, loss = 0.50617300\n",
      "Iteration 44, loss = 0.49790927\n",
      "Iteration 45, loss = 0.48946722\n",
      "Iteration 46, loss = 0.48095922\n",
      "Iteration 47, loss = 0.47248640\n",
      "Iteration 48, loss = 0.46412886\n",
      "Iteration 49, loss = 0.45596174\n",
      "Iteration 50, loss = 0.44805840\n",
      "Iteration 51, loss = 0.44047074\n",
      "Iteration 52, loss = 0.43321148\n",
      "Iteration 53, loss = 0.42626026\n",
      "Iteration 54, loss = 0.41958024\n",
      "Iteration 55, loss = 0.41312741\n",
      "Iteration 56, loss = 0.40686967\n",
      "Iteration 57, loss = 0.40081064\n",
      "Iteration 58, loss = 0.39498932\n",
      "Iteration 59, loss = 0.38945350\n",
      "Iteration 60, loss = 0.38422287\n",
      "Iteration 61, loss = 0.37928123\n",
      "Iteration 62, loss = 0.37460823\n",
      "Iteration 63, loss = 0.37019765\n",
      "Iteration 64, loss = 0.36602289\n",
      "Iteration 65, loss = 0.36197991\n",
      "Iteration 66, loss = 0.35794995\n",
      "Iteration 67, loss = 0.35386916\n",
      "Iteration 68, loss = 0.34970920\n",
      "Iteration 69, loss = 0.34547856\n",
      "Iteration 70, loss = 0.34121807\n",
      "Iteration 71, loss = 0.33696570\n",
      "Iteration 72, loss = 0.33275744\n",
      "Iteration 73, loss = 0.32863233\n",
      "Iteration 74, loss = 0.32460410\n",
      "Iteration 75, loss = 0.32066784\n",
      "Iteration 76, loss = 0.31682411\n",
      "Iteration 77, loss = 0.31306525\n",
      "Iteration 78, loss = 0.30936285\n",
      "Iteration 79, loss = 0.30569976\n",
      "Iteration 80, loss = 0.30207109\n",
      "Iteration 81, loss = 0.29847876\n",
      "Iteration 82, loss = 0.29493578\n",
      "Iteration 83, loss = 0.29145309\n",
      "Iteration 84, loss = 0.28805019\n",
      "Iteration 85, loss = 0.28473946\n",
      "Iteration 86, loss = 0.28152010\n",
      "Iteration 87, loss = 0.27837392\n",
      "Iteration 88, loss = 0.27528401\n",
      "Iteration 89, loss = 0.27224140\n",
      "Iteration 90, loss = 0.26923643\n",
      "Iteration 91, loss = 0.26627013\n",
      "Iteration 92, loss = 0.26334987\n",
      "Iteration 93, loss = 0.26047922\n",
      "Iteration 94, loss = 0.25764086\n",
      "Iteration 95, loss = 0.25481240\n",
      "Iteration 96, loss = 0.25198257\n",
      "Iteration 97, loss = 0.24915169\n",
      "Iteration 98, loss = 0.24631944\n",
      "Iteration 99, loss = 0.24348416\n",
      "Iteration 100, loss = 0.24064555\n",
      "Iteration 101, loss = 0.23780259\n",
      "Iteration 102, loss = 0.23495157\n",
      "Iteration 103, loss = 0.23209270\n",
      "Iteration 104, loss = 0.22923339\n",
      "Iteration 105, loss = 0.22638158\n",
      "Iteration 106, loss = 0.22353943\n",
      "Iteration 107, loss = 0.22070207\n",
      "Iteration 108, loss = 0.21785559\n",
      "Iteration 109, loss = 0.21500025\n",
      "Iteration 110, loss = 0.21214899\n",
      "Iteration 111, loss = 0.20931629\n",
      "Iteration 112, loss = 0.20650687\n",
      "Iteration 113, loss = 0.20371603\n",
      "Iteration 114, loss = 0.20093894\n",
      "Iteration 115, loss = 0.19817342\n",
      "Iteration 116, loss = 0.19541854\n",
      "Iteration 117, loss = 0.19267064\n",
      "Iteration 118, loss = 0.18992783\n",
      "Iteration 119, loss = 0.18719622\n",
      "Iteration 120, loss = 0.18448311\n",
      "Iteration 121, loss = 0.18178424\n",
      "Iteration 122, loss = 0.17908847\n",
      "Iteration 123, loss = 0.17639906\n",
      "Iteration 124, loss = 0.17376170\n",
      "Iteration 125, loss = 0.17131509\n",
      "Iteration 126, loss = 0.16896344\n",
      "Iteration 127, loss = 0.16622645\n",
      "Iteration 128, loss = 0.16340416\n",
      "Iteration 129, loss = 0.16129762\n",
      "Iteration 130, loss = 0.15889894\n",
      "Iteration 131, loss = 0.15610498\n",
      "Iteration 132, loss = 0.15398445\n",
      "Iteration 133, loss = 0.15158842\n",
      "Iteration 134, loss = 0.14894064\n",
      "Iteration 135, loss = 0.14686866\n",
      "Iteration 136, loss = 0.14445595\n",
      "Iteration 137, loss = 0.14192263\n",
      "Iteration 138, loss = 0.13983734\n",
      "Iteration 139, loss = 0.13741411\n",
      "Iteration 140, loss = 0.13494473\n",
      "Iteration 141, loss = 0.13284074\n",
      "Iteration 142, loss = 0.13047135\n",
      "Iteration 143, loss = 0.12803696\n",
      "Iteration 144, loss = 0.12591309\n",
      "Iteration 145, loss = 0.12363062\n",
      "Iteration 146, loss = 0.12119680\n",
      "Iteration 147, loss = 0.11902039\n",
      "Iteration 148, loss = 0.11685699\n",
      "Iteration 149, loss = 0.11448555\n",
      "Iteration 150, loss = 0.11221594\n",
      "Iteration 151, loss = 0.11010047\n",
      "Iteration 152, loss = 0.10789228\n",
      "Iteration 153, loss = 0.10559541\n",
      "Iteration 154, loss = 0.10338004\n",
      "Iteration 155, loss = 0.10127019\n",
      "Iteration 156, loss = 0.09915039\n",
      "Iteration 157, loss = 0.09695487\n",
      "Iteration 158, loss = 0.09475149\n",
      "Iteration 159, loss = 0.09260815\n",
      "Iteration 160, loss = 0.09053409\n",
      "Iteration 161, loss = 0.08850445\n",
      "Iteration 162, loss = 0.08649394\n",
      "Iteration 163, loss = 0.08450391\n",
      "Iteration 164, loss = 0.08248818\n",
      "Iteration 165, loss = 0.08046937\n",
      "Iteration 166, loss = 0.07844041\n",
      "Iteration 167, loss = 0.07645442\n",
      "Iteration 168, loss = 0.07453189\n",
      "Iteration 169, loss = 0.07267264\n",
      "Iteration 170, loss = 0.07087043\n",
      "Iteration 171, loss = 0.06913112\n",
      "Iteration 172, loss = 0.06748704\n",
      "Iteration 173, loss = 0.06591531\n",
      "Iteration 174, loss = 0.06439953\n",
      "Iteration 175, loss = 0.06263708\n",
      "Iteration 176, loss = 0.06074682\n",
      "Iteration 177, loss = 0.05901565\n",
      "Iteration 178, loss = 0.05762663\n",
      "Iteration 179, loss = 0.05633656\n",
      "Iteration 180, loss = 0.05482174\n",
      "Iteration 181, loss = 0.05319219\n",
      "Iteration 182, loss = 0.05173895\n",
      "Iteration 183, loss = 0.05051514\n",
      "Iteration 184, loss = 0.04927581\n",
      "Iteration 185, loss = 0.04787180\n",
      "Iteration 186, loss = 0.04649628\n",
      "Iteration 187, loss = 0.04530554\n",
      "Iteration 188, loss = 0.04419254\n",
      "Iteration 189, loss = 0.04300223\n",
      "Iteration 190, loss = 0.04176369\n",
      "Iteration 191, loss = 0.04063018\n",
      "Iteration 192, loss = 0.03960489\n",
      "Iteration 193, loss = 0.03856476\n",
      "Iteration 194, loss = 0.03747936\n",
      "Iteration 195, loss = 0.03643677\n",
      "Iteration 196, loss = 0.03548659\n",
      "Iteration 197, loss = 0.03456422\n",
      "Iteration 198, loss = 0.03361306\n",
      "Iteration 199, loss = 0.03267562\n",
      "Iteration 200, loss = 0.03180469\n",
      "Iteration 201, loss = 0.03097488\n",
      "Iteration 202, loss = 0.03013592\n",
      "Iteration 203, loss = 0.02930267\n",
      "Iteration 204, loss = 0.02851556\n",
      "Iteration 205, loss = 0.02776654\n",
      "Iteration 206, loss = 0.02702719\n",
      "Iteration 207, loss = 0.02629658\n",
      "Iteration 208, loss = 0.02558549\n",
      "Iteration 209, loss = 0.02490806\n",
      "Iteration 210, loss = 0.02425815\n",
      "Iteration 211, loss = 0.02361993\n",
      "Iteration 212, loss = 0.02299103\n",
      "Iteration 213, loss = 0.02238218\n",
      "Iteration 214, loss = 0.02180090\n",
      "Iteration 215, loss = 0.02123912\n",
      "Iteration 216, loss = 0.02068690\n",
      "Iteration 217, loss = 0.02014802\n",
      "Iteration 218, loss = 0.01963052\n",
      "Iteration 219, loss = 0.01913322\n",
      "Iteration 220, loss = 0.01864869\n",
      "Iteration 221, loss = 0.01817484\n",
      "Iteration 222, loss = 0.01771678\n",
      "Iteration 223, loss = 0.01727703\n",
      "Iteration 224, loss = 0.01685218\n",
      "Iteration 225, loss = 0.01643843\n",
      "Iteration 226, loss = 0.01603591\n",
      "Iteration 227, loss = 0.01564775\n",
      "Iteration 228, loss = 0.01527437\n",
      "Iteration 229, loss = 0.01491285\n",
      "Iteration 230, loss = 0.01456112\n",
      "Iteration 231, loss = 0.01422002\n",
      "Iteration 232, loss = 0.01389117\n",
      "Iteration 233, loss = 0.01357359\n",
      "Iteration 234, loss = 0.01326489\n",
      "Iteration 235, loss = 0.01296453\n",
      "Iteration 236, loss = 0.01267324\n",
      "Iteration 237, loss = 0.01239034\n",
      "Iteration 238, loss = 0.01211440\n",
      "Iteration 239, loss = 0.01184582\n",
      "Iteration 240, loss = 0.01159284\n",
      "Iteration 241, loss = 0.01134678\n",
      "Iteration 242, loss = 0.01110585\n",
      "Iteration 243, loss = 0.01087329\n",
      "Iteration 244, loss = 0.01064745\n",
      "Iteration 245, loss = 0.01042709\n",
      "Iteration 246, loss = 0.01021443\n",
      "Iteration 247, loss = 0.01001008\n",
      "Iteration 248, loss = 0.00981139\n",
      "Iteration 249, loss = 0.00961687\n",
      "Iteration 250, loss = 0.00942777\n",
      "Iteration 251, loss = 0.00924512\n",
      "Iteration 252, loss = 0.00906826\n",
      "Iteration 253, loss = 0.00889646\n",
      "Iteration 254, loss = 0.00872958\n",
      "Iteration 255, loss = 0.00856714\n",
      "Iteration 256, loss = 0.00840886\n",
      "Iteration 257, loss = 0.00825531\n",
      "Iteration 258, loss = 0.00810670\n",
      "Iteration 259, loss = 0.00796222\n",
      "Iteration 260, loss = 0.00782128\n",
      "Iteration 261, loss = 0.00768404\n",
      "Iteration 262, loss = 0.00755077\n",
      "Iteration 263, loss = 0.00742121\n",
      "Iteration 264, loss = 0.00729502\n",
      "Iteration 265, loss = 0.00717220\n",
      "Iteration 266, loss = 0.00705266\n",
      "Iteration 267, loss = 0.00693611\n",
      "Iteration 268, loss = 0.00682250\n",
      "Iteration 269, loss = 0.00671195\n",
      "Iteration 270, loss = 0.00660428\n",
      "Iteration 271, loss = 0.00649918\n",
      "Iteration 272, loss = 0.00639658\n",
      "Iteration 273, loss = 0.00629650\n",
      "Iteration 274, loss = 0.00619881\n",
      "Iteration 275, loss = 0.00610360\n",
      "Iteration 276, loss = 0.00601055\n",
      "Iteration 277, loss = 0.00591946\n",
      "Iteration 278, loss = 0.00583056\n",
      "Iteration 279, loss = 0.00574368\n",
      "Iteration 280, loss = 0.00565867\n",
      "Iteration 281, loss = 0.00557565\n",
      "Iteration 282, loss = 0.00549454\n",
      "Iteration 283, loss = 0.00541509\n",
      "Iteration 284, loss = 0.00533750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.782258064516129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.85      0.82      0.83        83\n",
      "      Normal       0.66      0.71      0.68        41\n",
      "\n",
      "    accuracy                           0.78       124\n",
      "   macro avg       0.75      0.76      0.76       124\n",
      "weighted avg       0.79      0.78      0.78       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "**EXPERIMENTO16**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "Usando a mesma proporção de 40% e 60%, só que agora considerando a exclusão das 6 primeiras variáveis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.64213469\n",
      "Iteration 2, loss = 0.63841274\n",
      "Iteration 3, loss = 0.63509025\n",
      "Iteration 4, loss = 0.63216771\n",
      "Iteration 5, loss = 0.62964246\n",
      "Iteration 6, loss = 0.62750346\n",
      "Iteration 7, loss = 0.62573137\n",
      "Iteration 8, loss = 0.62429749\n",
      "Iteration 9, loss = 0.62316382\n",
      "Iteration 10, loss = 0.62228613\n",
      "Iteration 11, loss = 0.62161599\n",
      "Iteration 12, loss = 0.62110181\n",
      "Iteration 13, loss = 0.62068996\n",
      "Iteration 14, loss = 0.62032672\n",
      "Iteration 15, loss = 0.61996112\n",
      "Iteration 16, loss = 0.61954754\n",
      "Iteration 17, loss = 0.61904813\n",
      "Iteration 18, loss = 0.61843453\n",
      "Iteration 19, loss = 0.61768853\n",
      "Iteration 20, loss = 0.61680243\n",
      "Iteration 21, loss = 0.61577889\n",
      "Iteration 22, loss = 0.61462996\n",
      "Iteration 23, loss = 0.61337491\n",
      "Iteration 24, loss = 0.61203705\n",
      "Iteration 25, loss = 0.61063964\n",
      "Iteration 26, loss = 0.60920158\n",
      "Iteration 27, loss = 0.60773337\n",
      "Iteration 28, loss = 0.60623522\n",
      "Iteration 29, loss = 0.60469717\n",
      "Iteration 30, loss = 0.60309919\n",
      "Iteration 31, loss = 0.60141503\n",
      "Iteration 32, loss = 0.59962019\n",
      "Iteration 33, loss = 0.59769525\n",
      "Iteration 34, loss = 0.59562172\n",
      "Iteration 35, loss = 0.59338269\n",
      "Iteration 36, loss = 0.59096950\n",
      "Iteration 37, loss = 0.58838207\n",
      "Iteration 38, loss = 0.58562289\n",
      "Iteration 39, loss = 0.58269180\n",
      "Iteration 40, loss = 0.57958242\n",
      "Iteration 41, loss = 0.57628312\n",
      "Iteration 42, loss = 0.57278388\n",
      "Iteration 43, loss = 0.56907861\n",
      "Iteration 44, loss = 0.56516812\n",
      "Iteration 45, loss = 0.56106293\n",
      "Iteration 46, loss = 0.55678167\n",
      "Iteration 47, loss = 0.55234089\n",
      "Iteration 48, loss = 0.54774753\n",
      "Iteration 49, loss = 0.54300576\n",
      "Iteration 50, loss = 0.53812703\n",
      "Iteration 51, loss = 0.53312262\n",
      "Iteration 52, loss = 0.52798710\n",
      "Iteration 53, loss = 0.52270362\n",
      "Iteration 54, loss = 0.51726774\n",
      "Iteration 55, loss = 0.51170551\n",
      "Iteration 56, loss = 0.50606969\n",
      "Iteration 57, loss = 0.50041994\n",
      "Iteration 58, loss = 0.49480118\n",
      "Iteration 59, loss = 0.48922793\n",
      "Iteration 60, loss = 0.48368996\n",
      "Iteration 61, loss = 0.47817526\n",
      "Iteration 62, loss = 0.47268032\n",
      "Iteration 63, loss = 0.46721361\n",
      "Iteration 64, loss = 0.46180616\n",
      "Iteration 65, loss = 0.45651185\n",
      "Iteration 66, loss = 0.45139185\n",
      "Iteration 67, loss = 0.44647603\n",
      "Iteration 68, loss = 0.44174209\n",
      "Iteration 69, loss = 0.43714617\n",
      "Iteration 70, loss = 0.43265488\n",
      "Iteration 71, loss = 0.42824474\n",
      "Iteration 72, loss = 0.42389840\n",
      "Iteration 73, loss = 0.41960700\n",
      "Iteration 74, loss = 0.41536905\n",
      "Iteration 75, loss = 0.41118568\n",
      "Iteration 76, loss = 0.40705565\n",
      "Iteration 77, loss = 0.40297939\n",
      "Iteration 78, loss = 0.39896419\n",
      "Iteration 79, loss = 0.39501382\n",
      "Iteration 80, loss = 0.39112483\n",
      "Iteration 81, loss = 0.38730471\n",
      "Iteration 82, loss = 0.38357576\n",
      "Iteration 83, loss = 0.37994813\n",
      "Iteration 84, loss = 0.37639508\n",
      "Iteration 85, loss = 0.37289419\n",
      "Iteration 86, loss = 0.36944919\n",
      "Iteration 87, loss = 0.36607274\n",
      "Iteration 88, loss = 0.36276701\n",
      "Iteration 89, loss = 0.35952769\n",
      "Iteration 90, loss = 0.35635251\n",
      "Iteration 91, loss = 0.35323749\n",
      "Iteration 92, loss = 0.35017738\n",
      "Iteration 93, loss = 0.34716906\n",
      "Iteration 94, loss = 0.34421096\n",
      "Iteration 95, loss = 0.34129432\n",
      "Iteration 96, loss = 0.33841209\n",
      "Iteration 97, loss = 0.33556183\n",
      "Iteration 98, loss = 0.33274841\n",
      "Iteration 99, loss = 0.32998475\n",
      "Iteration 100, loss = 0.32728336\n",
      "Iteration 101, loss = 0.32463862\n",
      "Iteration 102, loss = 0.32203517\n",
      "Iteration 103, loss = 0.31946164\n",
      "Iteration 104, loss = 0.31691188\n",
      "Iteration 105, loss = 0.31438306\n",
      "Iteration 106, loss = 0.31187717\n",
      "Iteration 107, loss = 0.30939705\n",
      "Iteration 108, loss = 0.30694273\n",
      "Iteration 109, loss = 0.30451708\n",
      "Iteration 110, loss = 0.30212250\n",
      "Iteration 111, loss = 0.29975428\n",
      "Iteration 112, loss = 0.29740214\n",
      "Iteration 113, loss = 0.29505574\n",
      "Iteration 114, loss = 0.29270711\n",
      "Iteration 115, loss = 0.29035251\n",
      "Iteration 116, loss = 0.28799825\n",
      "Iteration 117, loss = 0.28565248\n",
      "Iteration 118, loss = 0.28331204\n",
      "Iteration 119, loss = 0.28097226\n",
      "Iteration 120, loss = 0.27863400\n",
      "Iteration 121, loss = 0.27630452\n",
      "Iteration 122, loss = 0.27397761\n",
      "Iteration 123, loss = 0.27163359\n",
      "Iteration 124, loss = 0.26927345\n",
      "Iteration 125, loss = 0.26690568\n",
      "Iteration 126, loss = 0.26453255\n",
      "Iteration 127, loss = 0.26215848\n",
      "Iteration 128, loss = 0.25979027\n",
      "Iteration 129, loss = 0.25742744\n",
      "Iteration 130, loss = 0.25507103\n",
      "Iteration 131, loss = 0.25272461\n",
      "Iteration 132, loss = 0.25038605\n",
      "Iteration 133, loss = 0.24804680\n",
      "Iteration 134, loss = 0.24569971\n",
      "Iteration 135, loss = 0.24334643\n",
      "Iteration 136, loss = 0.24099528\n",
      "Iteration 137, loss = 0.23865126\n",
      "Iteration 138, loss = 0.23631570\n",
      "Iteration 139, loss = 0.23398883\n",
      "Iteration 140, loss = 0.23166940\n",
      "Iteration 141, loss = 0.22935632\n",
      "Iteration 142, loss = 0.22704743\n",
      "Iteration 143, loss = 0.22474415\n",
      "Iteration 144, loss = 0.22245045\n",
      "Iteration 145, loss = 0.22016648\n",
      "Iteration 146, loss = 0.21788892\n",
      "Iteration 147, loss = 0.21561503\n",
      "Iteration 148, loss = 0.21334411\n",
      "Iteration 149, loss = 0.21107438\n",
      "Iteration 150, loss = 0.20880405\n",
      "Iteration 151, loss = 0.20653735\n",
      "Iteration 152, loss = 0.20428437\n",
      "Iteration 153, loss = 0.20203991\n",
      "Iteration 154, loss = 0.19980372\n",
      "Iteration 155, loss = 0.19759695\n",
      "Iteration 156, loss = 0.19543664\n",
      "Iteration 157, loss = 0.19328644\n",
      "Iteration 158, loss = 0.19102851\n",
      "Iteration 159, loss = 0.18882735\n",
      "Iteration 160, loss = 0.18674562\n",
      "Iteration 161, loss = 0.18462238\n",
      "Iteration 162, loss = 0.18244515\n",
      "Iteration 163, loss = 0.18034723\n",
      "Iteration 164, loss = 0.17831207\n",
      "Iteration 165, loss = 0.17623061\n",
      "Iteration 166, loss = 0.17412885\n",
      "Iteration 167, loss = 0.17210569\n",
      "Iteration 168, loss = 0.17011271\n",
      "Iteration 169, loss = 0.16807663\n",
      "Iteration 170, loss = 0.16605366\n",
      "Iteration 171, loss = 0.16408945\n",
      "Iteration 172, loss = 0.16214207\n",
      "Iteration 173, loss = 0.16017710\n",
      "Iteration 174, loss = 0.15821674\n",
      "Iteration 175, loss = 0.15629531\n",
      "Iteration 176, loss = 0.15440106\n",
      "Iteration 177, loss = 0.15250489\n",
      "Iteration 178, loss = 0.15060466\n",
      "Iteration 179, loss = 0.14871484\n",
      "Iteration 180, loss = 0.14684698\n",
      "Iteration 181, loss = 0.14499486\n",
      "Iteration 182, loss = 0.14314582\n",
      "Iteration 183, loss = 0.14129028\n",
      "Iteration 184, loss = 0.13942078\n",
      "Iteration 185, loss = 0.13753751\n",
      "Iteration 186, loss = 0.13564592\n",
      "Iteration 187, loss = 0.13376469\n",
      "Iteration 188, loss = 0.13191432\n",
      "Iteration 189, loss = 0.13010976\n",
      "Iteration 190, loss = 0.12833291\n",
      "Iteration 191, loss = 0.12652439\n",
      "Iteration 192, loss = 0.12469993\n",
      "Iteration 193, loss = 0.12291517\n",
      "Iteration 194, loss = 0.12125307\n",
      "Iteration 195, loss = 0.11966532\n",
      "Iteration 196, loss = 0.11798229\n",
      "Iteration 197, loss = 0.11597014\n",
      "Iteration 198, loss = 0.11424275\n",
      "Iteration 199, loss = 0.11277738\n",
      "Iteration 200, loss = 0.11099298\n",
      "Iteration 201, loss = 0.10915828\n",
      "Iteration 202, loss = 0.10762178\n",
      "Iteration 203, loss = 0.10601708\n",
      "Iteration 204, loss = 0.10422530\n",
      "Iteration 205, loss = 0.10257898\n",
      "Iteration 206, loss = 0.10103634\n",
      "Iteration 207, loss = 0.09933710\n",
      "Iteration 208, loss = 0.09762215\n",
      "Iteration 209, loss = 0.09606194\n",
      "Iteration 210, loss = 0.09446350\n",
      "Iteration 211, loss = 0.09276859\n",
      "Iteration 212, loss = 0.09116994\n",
      "Iteration 213, loss = 0.08964162\n",
      "Iteration 214, loss = 0.08803781\n",
      "Iteration 215, loss = 0.08642137\n",
      "Iteration 216, loss = 0.08489559\n",
      "Iteration 217, loss = 0.08340568\n",
      "Iteration 218, loss = 0.08187821\n",
      "Iteration 219, loss = 0.08035530\n",
      "Iteration 220, loss = 0.07890163\n",
      "Iteration 221, loss = 0.07748554\n",
      "Iteration 222, loss = 0.07605442\n",
      "Iteration 223, loss = 0.07462658\n",
      "Iteration 224, loss = 0.07324627\n",
      "Iteration 225, loss = 0.07190935\n",
      "Iteration 226, loss = 0.07058102\n",
      "Iteration 227, loss = 0.06925394\n",
      "Iteration 228, loss = 0.06795435\n",
      "Iteration 229, loss = 0.06669812\n",
      "Iteration 230, loss = 0.06547190\n",
      "Iteration 231, loss = 0.06425861\n",
      "Iteration 232, loss = 0.06306016\n",
      "Iteration 233, loss = 0.06189302\n",
      "Iteration 234, loss = 0.06076384\n",
      "Iteration 235, loss = 0.05966329\n",
      "Iteration 236, loss = 0.05858051\n",
      "Iteration 237, loss = 0.05751640\n",
      "Iteration 238, loss = 0.05647986\n",
      "Iteration 239, loss = 0.05547436\n",
      "Iteration 240, loss = 0.05449390\n",
      "Iteration 241, loss = 0.05353259\n",
      "Iteration 242, loss = 0.05259261\n",
      "Iteration 243, loss = 0.05167884\n",
      "Iteration 244, loss = 0.05079023\n",
      "Iteration 245, loss = 0.04992120\n",
      "Iteration 246, loss = 0.04906991\n",
      "Iteration 247, loss = 0.04824016\n",
      "Iteration 248, loss = 0.04743376\n",
      "Iteration 249, loss = 0.04664736\n",
      "Iteration 250, loss = 0.04587828\n",
      "Iteration 251, loss = 0.04512824\n",
      "Iteration 252, loss = 0.04439854\n",
      "Iteration 253, loss = 0.04368669\n",
      "Iteration 254, loss = 0.04299086\n",
      "Iteration 255, loss = 0.04231273\n",
      "Iteration 256, loss = 0.04165275\n",
      "Iteration 257, loss = 0.04100860\n",
      "Iteration 258, loss = 0.04037967\n",
      "Iteration 259, loss = 0.03976700\n",
      "Iteration 260, loss = 0.03916959\n",
      "Iteration 261, loss = 0.03858601\n",
      "Iteration 262, loss = 0.03801692\n",
      "Iteration 263, loss = 0.03746226\n",
      "Iteration 264, loss = 0.03692066\n",
      "Iteration 265, loss = 0.03639226\n",
      "Iteration 266, loss = 0.03587710\n",
      "Iteration 267, loss = 0.03537405\n",
      "Iteration 268, loss = 0.03488303\n",
      "Iteration 269, loss = 0.03440408\n",
      "Iteration 270, loss = 0.03393635\n",
      "Iteration 271, loss = 0.03347983\n",
      "Iteration 272, loss = 0.03303436\n",
      "Iteration 273, loss = 0.03259923\n",
      "Iteration 274, loss = 0.03217444\n",
      "Iteration 275, loss = 0.03175962\n",
      "Iteration 276, loss = 0.03135432\n",
      "Iteration 277, loss = 0.03095855\n",
      "Iteration 278, loss = 0.03057185\n",
      "Iteration 279, loss = 0.03019398\n",
      "Iteration 280, loss = 0.02982477\n",
      "Iteration 281, loss = 0.02946380\n",
      "Iteration 282, loss = 0.02911100\n",
      "Iteration 283, loss = 0.02876603\n",
      "Iteration 284, loss = 0.02842869\n",
      "Iteration 285, loss = 0.02809881\n",
      "Iteration 286, loss = 0.02777609\n",
      "Iteration 287, loss = 0.02746039\n",
      "Iteration 288, loss = 0.02715145\n",
      "Iteration 289, loss = 0.02684912\n",
      "Iteration 290, loss = 0.02655320\n",
      "Iteration 291, loss = 0.02626348\n",
      "Iteration 292, loss = 0.02597983\n",
      "Iteration 293, loss = 0.02570202\n",
      "Iteration 294, loss = 0.02542993\n",
      "Iteration 295, loss = 0.02516336\n",
      "Iteration 296, loss = 0.02490219\n",
      "Iteration 297, loss = 0.02464624\n",
      "Iteration 298, loss = 0.02439539\n",
      "Iteration 299, loss = 0.02414947\n",
      "Iteration 300, loss = 0.02390837\n",
      "Iteration 301, loss = 0.02367194\n",
      "Iteration 302, loss = 0.02344006\n",
      "Iteration 303, loss = 0.02321260\n",
      "Iteration 304, loss = 0.02298945\n",
      "Iteration 305, loss = 0.02277049\n",
      "Iteration 306, loss = 0.02255560\n",
      "Iteration 307, loss = 0.02234468\n",
      "Iteration 308, loss = 0.02213762\n",
      "Iteration 309, loss = 0.02193432\n",
      "Iteration 310, loss = 0.02173469\n",
      "Iteration 311, loss = 0.02153862\n",
      "Iteration 312, loss = 0.02134604\n",
      "Iteration 313, loss = 0.02115683\n",
      "Iteration 314, loss = 0.02097093\n",
      "Iteration 315, loss = 0.02078824\n",
      "Iteration 316, loss = 0.02060869\n",
      "Iteration 317, loss = 0.02043220\n",
      "Iteration 318, loss = 0.02025868\n",
      "Iteration 319, loss = 0.02008807\n",
      "Iteration 320, loss = 0.01992030\n",
      "Iteration 321, loss = 0.01975529\n",
      "Iteration 322, loss = 0.01959298\n",
      "Iteration 323, loss = 0.01943330\n",
      "Iteration 324, loss = 0.01927620\n",
      "Iteration 325, loss = 0.01912160\n",
      "Iteration 326, loss = 0.01896945\n",
      "Iteration 327, loss = 0.01881969\n",
      "Iteration 328, loss = 0.01867227\n",
      "Iteration 329, loss = 0.01852712\n",
      "Iteration 330, loss = 0.01838421\n",
      "Iteration 331, loss = 0.01824348\n",
      "Iteration 332, loss = 0.01810487\n",
      "Iteration 333, loss = 0.01796834\n",
      "Iteration 334, loss = 0.01783384\n",
      "Iteration 335, loss = 0.01770134\n",
      "Iteration 336, loss = 0.01757077\n",
      "Iteration 337, loss = 0.01744211\n",
      "Iteration 338, loss = 0.01731530\n",
      "Iteration 339, loss = 0.01719031\n",
      "Iteration 340, loss = 0.01706710\n",
      "Iteration 341, loss = 0.01694563\n",
      "Iteration 342, loss = 0.01682587\n",
      "Iteration 343, loss = 0.01670777\n",
      "Iteration 344, loss = 0.01659130\n",
      "Iteration 345, loss = 0.01647644\n",
      "Iteration 346, loss = 0.01636313\n",
      "Iteration 347, loss = 0.01625136\n",
      "Iteration 348, loss = 0.01614109\n",
      "Iteration 349, loss = 0.01603229\n",
      "Iteration 350, loss = 0.01592493\n",
      "Iteration 351, loss = 0.01581898\n",
      "Iteration 352, loss = 0.01571442\n",
      "Iteration 353, loss = 0.01561121\n",
      "Iteration 354, loss = 0.01550933\n",
      "Iteration 355, loss = 0.01540875\n",
      "Iteration 356, loss = 0.01530945\n",
      "Iteration 357, loss = 0.01521140\n",
      "Iteration 358, loss = 0.01511458\n",
      "Iteration 359, loss = 0.01501897\n",
      "Iteration 360, loss = 0.01492454\n",
      "Iteration 361, loss = 0.01483126\n",
      "Iteration 362, loss = 0.01473913\n",
      "Iteration 363, loss = 0.01464811\n",
      "Iteration 364, loss = 0.01455819\n",
      "Iteration 365, loss = 0.01446934\n",
      "Iteration 366, loss = 0.01438155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7903225806451613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.87      0.81      0.84        83\n",
      "      Normal       0.66      0.76      0.70        41\n",
      "\n",
      "    accuracy                           0.79       124\n",
      "   macro avg       0.76      0.78      0.77       124\n",
      "weighted avg       0.80      0.79      0.79       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "No caso anterior, temos também a sinalização de warning que diz que o número limite de interações foi alcançado, porém a otmização ainda não convergiu. Iremos dobrar o número de interações e verificaremos como o MPL se comporta com os mesmos valores de hidden_layer_sizes e após com os valores dobrados hidden_layer_sizes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**EXPERIMENTO17**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Primeiro iremos considerar apenas o caso em que o número de interações dobra:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.64213469\n",
      "Iteration 2, loss = 0.63841274\n",
      "Iteration 3, loss = 0.63509025\n",
      "Iteration 4, loss = 0.63216771\n",
      "Iteration 5, loss = 0.62964246\n",
      "Iteration 6, loss = 0.62750346\n",
      "Iteration 7, loss = 0.62573137\n",
      "Iteration 8, loss = 0.62429749\n",
      "Iteration 9, loss = 0.62316382\n",
      "Iteration 10, loss = 0.62228613\n",
      "Iteration 11, loss = 0.62161599\n",
      "Iteration 12, loss = 0.62110181\n",
      "Iteration 13, loss = 0.62068996\n",
      "Iteration 14, loss = 0.62032672\n",
      "Iteration 15, loss = 0.61996112\n",
      "Iteration 16, loss = 0.61954754\n",
      "Iteration 17, loss = 0.61904813\n",
      "Iteration 18, loss = 0.61843453\n",
      "Iteration 19, loss = 0.61768853\n",
      "Iteration 20, loss = 0.61680243\n",
      "Iteration 21, loss = 0.61577889\n",
      "Iteration 22, loss = 0.61462996\n",
      "Iteration 23, loss = 0.61337491\n",
      "Iteration 24, loss = 0.61203705\n",
      "Iteration 25, loss = 0.61063964\n",
      "Iteration 26, loss = 0.60920158\n",
      "Iteration 27, loss = 0.60773337\n",
      "Iteration 28, loss = 0.60623522\n",
      "Iteration 29, loss = 0.60469717\n",
      "Iteration 30, loss = 0.60309919\n",
      "Iteration 31, loss = 0.60141503\n",
      "Iteration 32, loss = 0.59962019\n",
      "Iteration 33, loss = 0.59769525\n",
      "Iteration 34, loss = 0.59562172\n",
      "Iteration 35, loss = 0.59338269\n",
      "Iteration 36, loss = 0.59096950\n",
      "Iteration 37, loss = 0.58838207\n",
      "Iteration 38, loss = 0.58562289\n",
      "Iteration 39, loss = 0.58269180\n",
      "Iteration 40, loss = 0.57958242\n",
      "Iteration 41, loss = 0.57628312\n",
      "Iteration 42, loss = 0.57278388\n",
      "Iteration 43, loss = 0.56907861\n",
      "Iteration 44, loss = 0.56516812\n",
      "Iteration 45, loss = 0.56106293\n",
      "Iteration 46, loss = 0.55678167\n",
      "Iteration 47, loss = 0.55234089\n",
      "Iteration 48, loss = 0.54774753\n",
      "Iteration 49, loss = 0.54300576\n",
      "Iteration 50, loss = 0.53812703\n",
      "Iteration 51, loss = 0.53312262\n",
      "Iteration 52, loss = 0.52798710\n",
      "Iteration 53, loss = 0.52270362\n",
      "Iteration 54, loss = 0.51726774\n",
      "Iteration 55, loss = 0.51170551\n",
      "Iteration 56, loss = 0.50606969\n",
      "Iteration 57, loss = 0.50041994\n",
      "Iteration 58, loss = 0.49480118\n",
      "Iteration 59, loss = 0.48922793\n",
      "Iteration 60, loss = 0.48368996\n",
      "Iteration 61, loss = 0.47817526\n",
      "Iteration 62, loss = 0.47268032\n",
      "Iteration 63, loss = 0.46721361\n",
      "Iteration 64, loss = 0.46180616\n",
      "Iteration 65, loss = 0.45651185\n",
      "Iteration 66, loss = 0.45139185\n",
      "Iteration 67, loss = 0.44647603\n",
      "Iteration 68, loss = 0.44174209\n",
      "Iteration 69, loss = 0.43714617\n",
      "Iteration 70, loss = 0.43265488\n",
      "Iteration 71, loss = 0.42824474\n",
      "Iteration 72, loss = 0.42389840\n",
      "Iteration 73, loss = 0.41960700\n",
      "Iteration 74, loss = 0.41536905\n",
      "Iteration 75, loss = 0.41118568\n",
      "Iteration 76, loss = 0.40705565\n",
      "Iteration 77, loss = 0.40297939\n",
      "Iteration 78, loss = 0.39896419\n",
      "Iteration 79, loss = 0.39501382\n",
      "Iteration 80, loss = 0.39112483\n",
      "Iteration 81, loss = 0.38730471\n",
      "Iteration 82, loss = 0.38357576\n",
      "Iteration 83, loss = 0.37994813\n",
      "Iteration 84, loss = 0.37639508\n",
      "Iteration 85, loss = 0.37289419\n",
      "Iteration 86, loss = 0.36944919\n",
      "Iteration 87, loss = 0.36607274\n",
      "Iteration 88, loss = 0.36276701\n",
      "Iteration 89, loss = 0.35952769\n",
      "Iteration 90, loss = 0.35635251\n",
      "Iteration 91, loss = 0.35323749\n",
      "Iteration 92, loss = 0.35017738\n",
      "Iteration 93, loss = 0.34716906\n",
      "Iteration 94, loss = 0.34421096\n",
      "Iteration 95, loss = 0.34129432\n",
      "Iteration 96, loss = 0.33841209\n",
      "Iteration 97, loss = 0.33556183\n",
      "Iteration 98, loss = 0.33274841\n",
      "Iteration 99, loss = 0.32998475\n",
      "Iteration 100, loss = 0.32728336\n",
      "Iteration 101, loss = 0.32463862\n",
      "Iteration 102, loss = 0.32203517\n",
      "Iteration 103, loss = 0.31946164\n",
      "Iteration 104, loss = 0.31691188\n",
      "Iteration 105, loss = 0.31438306\n",
      "Iteration 106, loss = 0.31187717\n",
      "Iteration 107, loss = 0.30939705\n",
      "Iteration 108, loss = 0.30694273\n",
      "Iteration 109, loss = 0.30451708\n",
      "Iteration 110, loss = 0.30212250\n",
      "Iteration 111, loss = 0.29975428\n",
      "Iteration 112, loss = 0.29740214\n",
      "Iteration 113, loss = 0.29505574\n",
      "Iteration 114, loss = 0.29270711\n",
      "Iteration 115, loss = 0.29035251\n",
      "Iteration 116, loss = 0.28799825\n",
      "Iteration 117, loss = 0.28565248\n",
      "Iteration 118, loss = 0.28331204\n",
      "Iteration 119, loss = 0.28097226\n",
      "Iteration 120, loss = 0.27863400\n",
      "Iteration 121, loss = 0.27630452\n",
      "Iteration 122, loss = 0.27397761\n",
      "Iteration 123, loss = 0.27163359\n",
      "Iteration 124, loss = 0.26927345\n",
      "Iteration 125, loss = 0.26690568\n",
      "Iteration 126, loss = 0.26453255\n",
      "Iteration 127, loss = 0.26215848\n",
      "Iteration 128, loss = 0.25979027\n",
      "Iteration 129, loss = 0.25742744\n",
      "Iteration 130, loss = 0.25507103\n",
      "Iteration 131, loss = 0.25272461\n",
      "Iteration 132, loss = 0.25038605\n",
      "Iteration 133, loss = 0.24804680\n",
      "Iteration 134, loss = 0.24569971\n",
      "Iteration 135, loss = 0.24334643\n",
      "Iteration 136, loss = 0.24099528\n",
      "Iteration 137, loss = 0.23865126\n",
      "Iteration 138, loss = 0.23631570\n",
      "Iteration 139, loss = 0.23398883\n",
      "Iteration 140, loss = 0.23166940\n",
      "Iteration 141, loss = 0.22935632\n",
      "Iteration 142, loss = 0.22704743\n",
      "Iteration 143, loss = 0.22474415\n",
      "Iteration 144, loss = 0.22245045\n",
      "Iteration 145, loss = 0.22016648\n",
      "Iteration 146, loss = 0.21788892\n",
      "Iteration 147, loss = 0.21561503\n",
      "Iteration 148, loss = 0.21334411\n",
      "Iteration 149, loss = 0.21107438\n",
      "Iteration 150, loss = 0.20880405\n",
      "Iteration 151, loss = 0.20653735\n",
      "Iteration 152, loss = 0.20428437\n",
      "Iteration 153, loss = 0.20203991\n",
      "Iteration 154, loss = 0.19980372\n",
      "Iteration 155, loss = 0.19759695\n",
      "Iteration 156, loss = 0.19543664\n",
      "Iteration 157, loss = 0.19328644\n",
      "Iteration 158, loss = 0.19102851\n",
      "Iteration 159, loss = 0.18882735\n",
      "Iteration 160, loss = 0.18674562\n",
      "Iteration 161, loss = 0.18462238\n",
      "Iteration 162, loss = 0.18244515\n",
      "Iteration 163, loss = 0.18034723\n",
      "Iteration 164, loss = 0.17831207\n",
      "Iteration 165, loss = 0.17623061\n",
      "Iteration 166, loss = 0.17412885\n",
      "Iteration 167, loss = 0.17210569\n",
      "Iteration 168, loss = 0.17011271\n",
      "Iteration 169, loss = 0.16807663\n",
      "Iteration 170, loss = 0.16605366\n",
      "Iteration 171, loss = 0.16408945\n",
      "Iteration 172, loss = 0.16214207\n",
      "Iteration 173, loss = 0.16017710\n",
      "Iteration 174, loss = 0.15821674\n",
      "Iteration 175, loss = 0.15629531\n",
      "Iteration 176, loss = 0.15440106\n",
      "Iteration 177, loss = 0.15250489\n",
      "Iteration 178, loss = 0.15060466\n",
      "Iteration 179, loss = 0.14871484\n",
      "Iteration 180, loss = 0.14684698\n",
      "Iteration 181, loss = 0.14499486\n",
      "Iteration 182, loss = 0.14314582\n",
      "Iteration 183, loss = 0.14129028\n",
      "Iteration 184, loss = 0.13942078\n",
      "Iteration 185, loss = 0.13753751\n",
      "Iteration 186, loss = 0.13564592\n",
      "Iteration 187, loss = 0.13376469\n",
      "Iteration 188, loss = 0.13191432\n",
      "Iteration 189, loss = 0.13010976\n",
      "Iteration 190, loss = 0.12833291\n",
      "Iteration 191, loss = 0.12652439\n",
      "Iteration 192, loss = 0.12469993\n",
      "Iteration 193, loss = 0.12291517\n",
      "Iteration 194, loss = 0.12125307\n",
      "Iteration 195, loss = 0.11966532\n",
      "Iteration 196, loss = 0.11798229\n",
      "Iteration 197, loss = 0.11597014\n",
      "Iteration 198, loss = 0.11424275\n",
      "Iteration 199, loss = 0.11277738\n",
      "Iteration 200, loss = 0.11099298\n",
      "Iteration 201, loss = 0.10915828\n",
      "Iteration 202, loss = 0.10762178\n",
      "Iteration 203, loss = 0.10601708\n",
      "Iteration 204, loss = 0.10422530\n",
      "Iteration 205, loss = 0.10257898\n",
      "Iteration 206, loss = 0.10103634\n",
      "Iteration 207, loss = 0.09933710\n",
      "Iteration 208, loss = 0.09762215\n",
      "Iteration 209, loss = 0.09606194\n",
      "Iteration 210, loss = 0.09446350\n",
      "Iteration 211, loss = 0.09276859\n",
      "Iteration 212, loss = 0.09116994\n",
      "Iteration 213, loss = 0.08964162\n",
      "Iteration 214, loss = 0.08803781\n",
      "Iteration 215, loss = 0.08642137\n",
      "Iteration 216, loss = 0.08489559\n",
      "Iteration 217, loss = 0.08340568\n",
      "Iteration 218, loss = 0.08187821\n",
      "Iteration 219, loss = 0.08035530\n",
      "Iteration 220, loss = 0.07890163\n",
      "Iteration 221, loss = 0.07748554\n",
      "Iteration 222, loss = 0.07605442\n",
      "Iteration 223, loss = 0.07462658\n",
      "Iteration 224, loss = 0.07324627\n",
      "Iteration 225, loss = 0.07190935\n",
      "Iteration 226, loss = 0.07058102\n",
      "Iteration 227, loss = 0.06925394\n",
      "Iteration 228, loss = 0.06795435\n",
      "Iteration 229, loss = 0.06669812\n",
      "Iteration 230, loss = 0.06547190\n",
      "Iteration 231, loss = 0.06425861\n",
      "Iteration 232, loss = 0.06306016\n",
      "Iteration 233, loss = 0.06189302\n",
      "Iteration 234, loss = 0.06076384\n",
      "Iteration 235, loss = 0.05966329\n",
      "Iteration 236, loss = 0.05858051\n",
      "Iteration 237, loss = 0.05751640\n",
      "Iteration 238, loss = 0.05647986\n",
      "Iteration 239, loss = 0.05547436\n",
      "Iteration 240, loss = 0.05449390\n",
      "Iteration 241, loss = 0.05353259\n",
      "Iteration 242, loss = 0.05259261\n",
      "Iteration 243, loss = 0.05167884\n",
      "Iteration 244, loss = 0.05079023\n",
      "Iteration 245, loss = 0.04992120\n",
      "Iteration 246, loss = 0.04906991\n",
      "Iteration 247, loss = 0.04824016\n",
      "Iteration 248, loss = 0.04743376\n",
      "Iteration 249, loss = 0.04664736\n",
      "Iteration 250, loss = 0.04587828\n",
      "Iteration 251, loss = 0.04512824\n",
      "Iteration 252, loss = 0.04439854\n",
      "Iteration 253, loss = 0.04368669\n",
      "Iteration 254, loss = 0.04299086\n",
      "Iteration 255, loss = 0.04231273\n",
      "Iteration 256, loss = 0.04165275\n",
      "Iteration 257, loss = 0.04100860\n",
      "Iteration 258, loss = 0.04037967\n",
      "Iteration 259, loss = 0.03976700\n",
      "Iteration 260, loss = 0.03916959\n",
      "Iteration 261, loss = 0.03858601\n",
      "Iteration 262, loss = 0.03801692\n",
      "Iteration 263, loss = 0.03746226\n",
      "Iteration 264, loss = 0.03692066\n",
      "Iteration 265, loss = 0.03639226\n",
      "Iteration 266, loss = 0.03587710\n",
      "Iteration 267, loss = 0.03537405\n",
      "Iteration 268, loss = 0.03488303\n",
      "Iteration 269, loss = 0.03440408\n",
      "Iteration 270, loss = 0.03393635\n",
      "Iteration 271, loss = 0.03347983\n",
      "Iteration 272, loss = 0.03303436\n",
      "Iteration 273, loss = 0.03259923\n",
      "Iteration 274, loss = 0.03217444\n",
      "Iteration 275, loss = 0.03175962\n",
      "Iteration 276, loss = 0.03135432\n",
      "Iteration 277, loss = 0.03095855\n",
      "Iteration 278, loss = 0.03057185\n",
      "Iteration 279, loss = 0.03019398\n",
      "Iteration 280, loss = 0.02982477\n",
      "Iteration 281, loss = 0.02946380\n",
      "Iteration 282, loss = 0.02911100\n",
      "Iteration 283, loss = 0.02876603\n",
      "Iteration 284, loss = 0.02842869\n",
      "Iteration 285, loss = 0.02809881\n",
      "Iteration 286, loss = 0.02777609\n",
      "Iteration 287, loss = 0.02746039\n",
      "Iteration 288, loss = 0.02715145\n",
      "Iteration 289, loss = 0.02684912\n",
      "Iteration 290, loss = 0.02655320\n",
      "Iteration 291, loss = 0.02626348\n",
      "Iteration 292, loss = 0.02597983\n",
      "Iteration 293, loss = 0.02570202\n",
      "Iteration 294, loss = 0.02542993\n",
      "Iteration 295, loss = 0.02516336\n",
      "Iteration 296, loss = 0.02490219\n",
      "Iteration 297, loss = 0.02464624\n",
      "Iteration 298, loss = 0.02439539\n",
      "Iteration 299, loss = 0.02414947\n",
      "Iteration 300, loss = 0.02390837\n",
      "Iteration 301, loss = 0.02367194\n",
      "Iteration 302, loss = 0.02344006\n",
      "Iteration 303, loss = 0.02321260\n",
      "Iteration 304, loss = 0.02298945\n",
      "Iteration 305, loss = 0.02277049\n",
      "Iteration 306, loss = 0.02255560\n",
      "Iteration 307, loss = 0.02234468\n",
      "Iteration 308, loss = 0.02213762\n",
      "Iteration 309, loss = 0.02193432\n",
      "Iteration 310, loss = 0.02173469\n",
      "Iteration 311, loss = 0.02153862\n",
      "Iteration 312, loss = 0.02134604\n",
      "Iteration 313, loss = 0.02115683\n",
      "Iteration 314, loss = 0.02097093\n",
      "Iteration 315, loss = 0.02078824\n",
      "Iteration 316, loss = 0.02060869\n",
      "Iteration 317, loss = 0.02043220\n",
      "Iteration 318, loss = 0.02025868\n",
      "Iteration 319, loss = 0.02008807\n",
      "Iteration 320, loss = 0.01992030\n",
      "Iteration 321, loss = 0.01975529\n",
      "Iteration 322, loss = 0.01959298\n",
      "Iteration 323, loss = 0.01943330\n",
      "Iteration 324, loss = 0.01927620\n",
      "Iteration 325, loss = 0.01912160\n",
      "Iteration 326, loss = 0.01896945\n",
      "Iteration 327, loss = 0.01881969\n",
      "Iteration 328, loss = 0.01867227\n",
      "Iteration 329, loss = 0.01852712\n",
      "Iteration 330, loss = 0.01838421\n",
      "Iteration 331, loss = 0.01824348\n",
      "Iteration 332, loss = 0.01810487\n",
      "Iteration 333, loss = 0.01796834\n",
      "Iteration 334, loss = 0.01783384\n",
      "Iteration 335, loss = 0.01770134\n",
      "Iteration 336, loss = 0.01757077\n",
      "Iteration 337, loss = 0.01744211\n",
      "Iteration 338, loss = 0.01731530\n",
      "Iteration 339, loss = 0.01719031\n",
      "Iteration 340, loss = 0.01706710\n",
      "Iteration 341, loss = 0.01694563\n",
      "Iteration 342, loss = 0.01682587\n",
      "Iteration 343, loss = 0.01670777\n",
      "Iteration 344, loss = 0.01659130\n",
      "Iteration 345, loss = 0.01647644\n",
      "Iteration 346, loss = 0.01636313\n",
      "Iteration 347, loss = 0.01625136\n",
      "Iteration 348, loss = 0.01614109\n",
      "Iteration 349, loss = 0.01603229\n",
      "Iteration 350, loss = 0.01592493\n",
      "Iteration 351, loss = 0.01581898\n",
      "Iteration 352, loss = 0.01571442\n",
      "Iteration 353, loss = 0.01561121\n",
      "Iteration 354, loss = 0.01550933\n",
      "Iteration 355, loss = 0.01540875\n",
      "Iteration 356, loss = 0.01530945\n",
      "Iteration 357, loss = 0.01521140\n",
      "Iteration 358, loss = 0.01511458\n",
      "Iteration 359, loss = 0.01501897\n",
      "Iteration 360, loss = 0.01492454\n",
      "Iteration 361, loss = 0.01483126\n",
      "Iteration 362, loss = 0.01473913\n",
      "Iteration 363, loss = 0.01464811\n",
      "Iteration 364, loss = 0.01455819\n",
      "Iteration 365, loss = 0.01446934\n",
      "Iteration 366, loss = 0.01438155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7903225806451613\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.87      0.81      0.84        83\n",
      "      Normal       0.66      0.76      0.70        41\n",
      "\n",
      "    accuracy                           0.79       124\n",
      "   macro avg       0.76      0.78      0.77       124\n",
      "weighted avg       0.80      0.79      0.79       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=1000, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "\n",
    "Tivemos uma melhora de precision e recall para ambaos os casos e também de accuracy, se comporado com o experimento anterior."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**EXPERIMENTO18**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Agora iremos verificar o comportamento do MPL quando dobrandos os valores para o hidden_layer_sizes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.65205510\n",
      "Iteration 2, loss = 0.64048746\n",
      "Iteration 3, loss = 0.63174406\n",
      "Iteration 4, loss = 0.62564321\n",
      "Iteration 5, loss = 0.62185780\n",
      "Iteration 6, loss = 0.61987823\n",
      "Iteration 7, loss = 0.61904405\n",
      "Iteration 8, loss = 0.61863318\n",
      "Iteration 9, loss = 0.61802537\n",
      "Iteration 10, loss = 0.61683834\n",
      "Iteration 11, loss = 0.61494447\n",
      "Iteration 12, loss = 0.61240025\n",
      "Iteration 13, loss = 0.60935619\n",
      "Iteration 14, loss = 0.60598610\n",
      "Iteration 15, loss = 0.60244391\n",
      "Iteration 16, loss = 0.59883641\n",
      "Iteration 17, loss = 0.59520863\n",
      "Iteration 18, loss = 0.59154549\n",
      "Iteration 19, loss = 0.58778303\n",
      "Iteration 20, loss = 0.58382321\n",
      "Iteration 21, loss = 0.57955850\n",
      "Iteration 22, loss = 0.57490051\n",
      "Iteration 23, loss = 0.56979795\n",
      "Iteration 24, loss = 0.56423445\n",
      "Iteration 25, loss = 0.55821807\n",
      "Iteration 26, loss = 0.55176925\n",
      "Iteration 27, loss = 0.54490779\n",
      "Iteration 28, loss = 0.53765310\n",
      "Iteration 29, loss = 0.53003509\n",
      "Iteration 30, loss = 0.52210682\n",
      "Iteration 31, loss = 0.51393628\n",
      "Iteration 32, loss = 0.50558786\n",
      "Iteration 33, loss = 0.49712434\n",
      "Iteration 34, loss = 0.48862011\n",
      "Iteration 35, loss = 0.48016851\n",
      "Iteration 36, loss = 0.47188452\n",
      "Iteration 37, loss = 0.46388937\n",
      "Iteration 38, loss = 0.45627487\n",
      "Iteration 39, loss = 0.44906507\n",
      "Iteration 40, loss = 0.44222167\n",
      "Iteration 41, loss = 0.43569202\n",
      "Iteration 42, loss = 0.42944323\n",
      "Iteration 43, loss = 0.42346439\n",
      "Iteration 44, loss = 0.41775818\n",
      "Iteration 45, loss = 0.41232198\n",
      "Iteration 46, loss = 0.40712043\n",
      "Iteration 47, loss = 0.40209268\n",
      "Iteration 48, loss = 0.39718077\n",
      "Iteration 49, loss = 0.39234126\n",
      "Iteration 50, loss = 0.38754173\n",
      "Iteration 51, loss = 0.38275988\n",
      "Iteration 52, loss = 0.37798179\n",
      "Iteration 53, loss = 0.37319520\n",
      "Iteration 54, loss = 0.36838647\n",
      "Iteration 55, loss = 0.36356467\n",
      "Iteration 56, loss = 0.35875713\n",
      "Iteration 57, loss = 0.35398222\n",
      "Iteration 58, loss = 0.34926801\n",
      "Iteration 59, loss = 0.34464905\n",
      "Iteration 60, loss = 0.34015103\n",
      "Iteration 61, loss = 0.33577938\n",
      "Iteration 62, loss = 0.33152547\n",
      "Iteration 63, loss = 0.32737815\n",
      "Iteration 64, loss = 0.32332286\n",
      "Iteration 65, loss = 0.31934686\n",
      "Iteration 66, loss = 0.31544978\n",
      "Iteration 67, loss = 0.31164366\n",
      "Iteration 68, loss = 0.30794217\n",
      "Iteration 69, loss = 0.30435840\n",
      "Iteration 70, loss = 0.30089306\n",
      "Iteration 71, loss = 0.29752055\n",
      "Iteration 72, loss = 0.29420892\n",
      "Iteration 73, loss = 0.29093862\n",
      "Iteration 74, loss = 0.28770339\n",
      "Iteration 75, loss = 0.28450497\n",
      "Iteration 76, loss = 0.28134391\n",
      "Iteration 77, loss = 0.27820873\n",
      "Iteration 78, loss = 0.27509110\n",
      "Iteration 79, loss = 0.27198512\n",
      "Iteration 80, loss = 0.26888759\n",
      "Iteration 81, loss = 0.26580388\n",
      "Iteration 82, loss = 0.26273345\n",
      "Iteration 83, loss = 0.25968257\n",
      "Iteration 84, loss = 0.25666206\n",
      "Iteration 85, loss = 0.25365231\n",
      "Iteration 86, loss = 0.25064439\n",
      "Iteration 87, loss = 0.24764648\n",
      "Iteration 88, loss = 0.24467471\n",
      "Iteration 89, loss = 0.24174244\n",
      "Iteration 90, loss = 0.23883634\n",
      "Iteration 91, loss = 0.23592621\n",
      "Iteration 92, loss = 0.23299560\n",
      "Iteration 93, loss = 0.23004578\n",
      "Iteration 94, loss = 0.22709248\n",
      "Iteration 95, loss = 0.22416570\n",
      "Iteration 96, loss = 0.22129939\n",
      "Iteration 97, loss = 0.21853422\n",
      "Iteration 98, loss = 0.21586634\n",
      "Iteration 99, loss = 0.21301878\n",
      "Iteration 100, loss = 0.21002077\n",
      "Iteration 101, loss = 0.20721929\n",
      "Iteration 102, loss = 0.20463983\n",
      "Iteration 103, loss = 0.20195902\n",
      "Iteration 104, loss = 0.19903281\n",
      "Iteration 105, loss = 0.19622121\n",
      "Iteration 106, loss = 0.19359836\n",
      "Iteration 107, loss = 0.19090235\n",
      "Iteration 108, loss = 0.18804053\n",
      "Iteration 109, loss = 0.18515864\n",
      "Iteration 110, loss = 0.18242096\n",
      "Iteration 111, loss = 0.17974450\n",
      "Iteration 112, loss = 0.17699277\n",
      "Iteration 113, loss = 0.17415360\n",
      "Iteration 114, loss = 0.17129613\n",
      "Iteration 115, loss = 0.16851210\n",
      "Iteration 116, loss = 0.16580640\n",
      "Iteration 117, loss = 0.16315192\n",
      "Iteration 118, loss = 0.16054875\n",
      "Iteration 119, loss = 0.15792135\n",
      "Iteration 120, loss = 0.15526595\n",
      "Iteration 121, loss = 0.15243704\n",
      "Iteration 122, loss = 0.14959840\n",
      "Iteration 123, loss = 0.14685663\n",
      "Iteration 124, loss = 0.14424414\n",
      "Iteration 125, loss = 0.14173894\n",
      "Iteration 126, loss = 0.13931550\n",
      "Iteration 127, loss = 0.13700909\n",
      "Iteration 128, loss = 0.13439324\n",
      "Iteration 129, loss = 0.13152978\n",
      "Iteration 130, loss = 0.12863276\n",
      "Iteration 131, loss = 0.12616788\n",
      "Iteration 132, loss = 0.12399281\n",
      "Iteration 133, loss = 0.12165844\n",
      "Iteration 134, loss = 0.11905328\n",
      "Iteration 135, loss = 0.11622561\n",
      "Iteration 136, loss = 0.11367257\n",
      "Iteration 137, loss = 0.11145032\n",
      "Iteration 138, loss = 0.10931910\n",
      "Iteration 139, loss = 0.10711711\n",
      "Iteration 140, loss = 0.10456361\n",
      "Iteration 141, loss = 0.10194291\n",
      "Iteration 142, loss = 0.09949583\n",
      "Iteration 143, loss = 0.09733435\n",
      "Iteration 144, loss = 0.09535057\n",
      "Iteration 145, loss = 0.09336220\n",
      "Iteration 146, loss = 0.09132034\n",
      "Iteration 147, loss = 0.08897761\n",
      "Iteration 148, loss = 0.08658931\n",
      "Iteration 149, loss = 0.08436423\n",
      "Iteration 150, loss = 0.08242213\n",
      "Iteration 151, loss = 0.08069208\n",
      "Iteration 152, loss = 0.07903017\n",
      "Iteration 153, loss = 0.07739726\n",
      "Iteration 154, loss = 0.07540198\n",
      "Iteration 155, loss = 0.07324341\n",
      "Iteration 156, loss = 0.07119147\n",
      "Iteration 157, loss = 0.06954026\n",
      "Iteration 158, loss = 0.06814071\n",
      "Iteration 159, loss = 0.06664979\n",
      "Iteration 160, loss = 0.06495067\n",
      "Iteration 161, loss = 0.06308108\n",
      "Iteration 162, loss = 0.06138173\n",
      "Iteration 163, loss = 0.05996379\n",
      "Iteration 164, loss = 0.05867546\n",
      "Iteration 165, loss = 0.05732928\n",
      "Iteration 166, loss = 0.05580636\n",
      "Iteration 167, loss = 0.05424541\n",
      "Iteration 168, loss = 0.05280556\n",
      "Iteration 169, loss = 0.05154526\n",
      "Iteration 170, loss = 0.05037742\n",
      "Iteration 171, loss = 0.04917710\n",
      "Iteration 172, loss = 0.04790513\n",
      "Iteration 173, loss = 0.04659222\n",
      "Iteration 174, loss = 0.04534370\n",
      "Iteration 175, loss = 0.04421152\n",
      "Iteration 176, loss = 0.04316532\n",
      "Iteration 177, loss = 0.04213660\n",
      "Iteration 178, loss = 0.04107436\n",
      "Iteration 179, loss = 0.03999003\n",
      "Iteration 180, loss = 0.03893113\n",
      "Iteration 181, loss = 0.03794330\n",
      "Iteration 182, loss = 0.03702526\n",
      "Iteration 183, loss = 0.03613812\n",
      "Iteration 184, loss = 0.03524564\n",
      "Iteration 185, loss = 0.03434485\n",
      "Iteration 186, loss = 0.03346649\n",
      "Iteration 187, loss = 0.03263948\n",
      "Iteration 188, loss = 0.03186048\n",
      "Iteration 189, loss = 0.03110085\n",
      "Iteration 190, loss = 0.03034117\n",
      "Iteration 191, loss = 0.02959155\n",
      "Iteration 192, loss = 0.02887598\n",
      "Iteration 193, loss = 0.02819927\n",
      "Iteration 194, loss = 0.02754224\n",
      "Iteration 195, loss = 0.02689052\n",
      "Iteration 196, loss = 0.02625332\n",
      "Iteration 197, loss = 0.02564649\n",
      "Iteration 198, loss = 0.02506580\n",
      "Iteration 199, loss = 0.02449552\n",
      "Iteration 200, loss = 0.02393583\n",
      "Iteration 201, loss = 0.02339908\n",
      "Iteration 202, loss = 0.02288498\n",
      "Iteration 203, loss = 0.02238194\n",
      "Iteration 204, loss = 0.02189013\n",
      "Iteration 205, loss = 0.02141854\n",
      "Iteration 206, loss = 0.02096390\n",
      "Iteration 207, loss = 0.02051853\n",
      "Iteration 208, loss = 0.02008695\n",
      "Iteration 209, loss = 0.01967255\n",
      "Iteration 210, loss = 0.01926895\n",
      "Iteration 211, loss = 0.01887572\n",
      "Iteration 212, loss = 0.01849722\n",
      "Iteration 213, loss = 0.01812966\n",
      "Iteration 214, loss = 0.01777104\n",
      "Iteration 215, loss = 0.01742455\n",
      "Iteration 216, loss = 0.01708747\n",
      "Iteration 217, loss = 0.01676242\n",
      "Iteration 218, loss = 0.01644612\n",
      "Iteration 219, loss = 0.01613780\n",
      "Iteration 220, loss = 0.01583932\n",
      "Iteration 221, loss = 0.01555056\n",
      "Iteration 222, loss = 0.01526965\n",
      "Iteration 223, loss = 0.01499626\n",
      "Iteration 224, loss = 0.01473082\n",
      "Iteration 225, loss = 0.01447322\n",
      "Iteration 226, loss = 0.01422264\n",
      "Iteration 227, loss = 0.01397898\n",
      "Iteration 228, loss = 0.01374198\n",
      "Iteration 229, loss = 0.01351163\n",
      "Iteration 230, loss = 0.01328767\n",
      "Iteration 231, loss = 0.01306964\n",
      "Iteration 232, loss = 0.01285730\n",
      "Iteration 233, loss = 0.01265074\n",
      "Iteration 234, loss = 0.01244956\n",
      "Iteration 235, loss = 0.01225372\n",
      "Iteration 236, loss = 0.01206233\n",
      "Iteration 237, loss = 0.01187565\n",
      "Iteration 238, loss = 0.01169371\n",
      "Iteration 239, loss = 0.01151667\n",
      "Iteration 240, loss = 0.01134430\n",
      "Iteration 241, loss = 0.01117610\n",
      "Iteration 242, loss = 0.01101195\n",
      "Iteration 243, loss = 0.01085090\n",
      "Iteration 244, loss = 0.01069368\n",
      "Iteration 245, loss = 0.01054060\n",
      "Iteration 246, loss = 0.01039155\n",
      "Iteration 247, loss = 0.01024599\n",
      "Iteration 248, loss = 0.01010323\n",
      "Iteration 249, loss = 0.00996345\n",
      "Iteration 250, loss = 0.00982688\n",
      "Iteration 251, loss = 0.00969376\n",
      "Iteration 252, loss = 0.00956381\n",
      "Iteration 253, loss = 0.00943659\n",
      "Iteration 254, loss = 0.00931200\n",
      "Iteration 255, loss = 0.00918985\n",
      "Iteration 256, loss = 0.00907038\n",
      "Iteration 257, loss = 0.00895364\n",
      "Iteration 258, loss = 0.00883956\n",
      "Iteration 259, loss = 0.00872800\n",
      "Iteration 260, loss = 0.00861875\n",
      "Iteration 261, loss = 0.00851178\n",
      "Iteration 262, loss = 0.00840685\n",
      "Iteration 263, loss = 0.00830406\n",
      "Iteration 264, loss = 0.00820341\n",
      "Iteration 265, loss = 0.00810494\n",
      "Iteration 266, loss = 0.00800862\n",
      "Iteration 267, loss = 0.00791432\n",
      "Iteration 268, loss = 0.00782197\n",
      "Iteration 269, loss = 0.00773142\n",
      "Iteration 270, loss = 0.00764265\n",
      "Iteration 271, loss = 0.00755554\n",
      "Iteration 272, loss = 0.00747016\n",
      "Iteration 273, loss = 0.00738652\n",
      "Iteration 274, loss = 0.00730463\n",
      "Iteration 275, loss = 0.00722441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7983870967741935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.85      0.84      0.85        83\n",
      "      Normal       0.69      0.71      0.70        41\n",
      "\n",
      "    accuracy                           0.80       124\n",
      "   macro avg       0.77      0.78      0.77       124\n",
      "weighted avg       0.80      0.80      0.80       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(200,100,50,), activation='logistic', max_iter=1000, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "Tivemos uma melhora de precision e recall para ambaos os casos, se comporado com o experimento anterior."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ao dobrar os valores da hidden_layer_sizes nós tivemos uma diminuição de cerca de 2% na acurácia, porém tivemos uma piora de precision para os casos anormais, enquanto o recall melhorou tanto para os casos anormais como para os normais."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**EXPERIMENTO19**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "E agora considerando a mesma porporção de 40% e 60%, só que considerando a exclusão de 110 casos anormais."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_data3['class']\n",
    "x = new_data3.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.70289048\n",
      "Iteration 2, loss = 0.69986866\n",
      "Iteration 3, loss = 0.69725069\n",
      "Iteration 4, loss = 0.69503252\n",
      "Iteration 5, loss = 0.69320618\n",
      "Iteration 6, loss = 0.69175200\n",
      "Iteration 7, loss = 0.69063436\n",
      "Iteration 8, loss = 0.68980190\n",
      "Iteration 9, loss = 0.68918882\n",
      "Iteration 10, loss = 0.68871661\n",
      "Iteration 11, loss = 0.68830028\n",
      "Iteration 12, loss = 0.68785888\n",
      "Iteration 13, loss = 0.68732631\n",
      "Iteration 14, loss = 0.68665749\n",
      "Iteration 15, loss = 0.68582850\n",
      "Iteration 16, loss = 0.68483164\n",
      "Iteration 17, loss = 0.68367007\n",
      "Iteration 18, loss = 0.68235404\n",
      "Iteration 19, loss = 0.68089671\n",
      "Iteration 20, loss = 0.67930910\n",
      "Iteration 21, loss = 0.67759752\n",
      "Iteration 22, loss = 0.67576493\n",
      "Iteration 23, loss = 0.67381299\n",
      "Iteration 24, loss = 0.67174236\n",
      "Iteration 25, loss = 0.66955174\n",
      "Iteration 26, loss = 0.66723810\n",
      "Iteration 27, loss = 0.66479773\n",
      "Iteration 28, loss = 0.66222582\n",
      "Iteration 29, loss = 0.65951464\n",
      "Iteration 30, loss = 0.65665086\n",
      "Iteration 31, loss = 0.65361563\n",
      "Iteration 32, loss = 0.65038635\n",
      "Iteration 33, loss = 0.64693698\n",
      "Iteration 34, loss = 0.64323580\n",
      "Iteration 35, loss = 0.63924612\n",
      "Iteration 36, loss = 0.63494170\n",
      "Iteration 37, loss = 0.63032398\n",
      "Iteration 38, loss = 0.62541668\n",
      "Iteration 39, loss = 0.62024981\n",
      "Iteration 40, loss = 0.61484851\n",
      "Iteration 41, loss = 0.60922801\n",
      "Iteration 42, loss = 0.60339767\n",
      "Iteration 43, loss = 0.59736473\n",
      "Iteration 44, loss = 0.59113422\n",
      "Iteration 45, loss = 0.58470950\n",
      "Iteration 46, loss = 0.57809294\n",
      "Iteration 47, loss = 0.57129110\n",
      "Iteration 48, loss = 0.56431603\n",
      "Iteration 49, loss = 0.55717974\n",
      "Iteration 50, loss = 0.54989592\n",
      "Iteration 51, loss = 0.54248241\n",
      "Iteration 52, loss = 0.53496553\n",
      "Iteration 53, loss = 0.52739864\n",
      "Iteration 54, loss = 0.51986406\n",
      "Iteration 55, loss = 0.51243676\n",
      "Iteration 56, loss = 0.50516414\n",
      "Iteration 57, loss = 0.49807534\n",
      "Iteration 58, loss = 0.49117672\n",
      "Iteration 59, loss = 0.48445329\n",
      "Iteration 60, loss = 0.47789312\n",
      "Iteration 61, loss = 0.47150775\n",
      "Iteration 62, loss = 0.46531496\n",
      "Iteration 63, loss = 0.45931220\n",
      "Iteration 64, loss = 0.45347732\n",
      "Iteration 65, loss = 0.44778261\n",
      "Iteration 66, loss = 0.44221436\n",
      "Iteration 67, loss = 0.43677817\n",
      "Iteration 68, loss = 0.43147077\n",
      "Iteration 69, loss = 0.42627877\n",
      "Iteration 70, loss = 0.42120052\n",
      "Iteration 71, loss = 0.41624297\n",
      "Iteration 72, loss = 0.41144132\n",
      "Iteration 73, loss = 0.40686717\n",
      "Iteration 74, loss = 0.40256103\n",
      "Iteration 75, loss = 0.39847302\n",
      "Iteration 76, loss = 0.39452620\n",
      "Iteration 77, loss = 0.39070120\n",
      "Iteration 78, loss = 0.38701550\n",
      "Iteration 79, loss = 0.38344262\n",
      "Iteration 80, loss = 0.37989982\n",
      "Iteration 81, loss = 0.37635869\n",
      "Iteration 82, loss = 0.37284305\n",
      "Iteration 83, loss = 0.36939291\n",
      "Iteration 84, loss = 0.36603697\n",
      "Iteration 85, loss = 0.36277920\n",
      "Iteration 86, loss = 0.35960556\n",
      "Iteration 87, loss = 0.35649617\n",
      "Iteration 88, loss = 0.35343636\n",
      "Iteration 89, loss = 0.35040672\n",
      "Iteration 90, loss = 0.34738077\n",
      "Iteration 91, loss = 0.34435452\n",
      "Iteration 92, loss = 0.34135124\n",
      "Iteration 93, loss = 0.33839667\n",
      "Iteration 94, loss = 0.33550917\n",
      "Iteration 95, loss = 0.33268425\n",
      "Iteration 96, loss = 0.32989089\n",
      "Iteration 97, loss = 0.32710903\n",
      "Iteration 98, loss = 0.32433958\n",
      "Iteration 99, loss = 0.32159744\n",
      "Iteration 100, loss = 0.31889841\n",
      "Iteration 101, loss = 0.31624356\n",
      "Iteration 102, loss = 0.31362708\n",
      "Iteration 103, loss = 0.31103655\n",
      "Iteration 104, loss = 0.30846360\n",
      "Iteration 105, loss = 0.30591312\n",
      "Iteration 106, loss = 0.30339522\n",
      "Iteration 107, loss = 0.30091415\n",
      "Iteration 108, loss = 0.29846388\n",
      "Iteration 109, loss = 0.29603715\n",
      "Iteration 110, loss = 0.29362953\n",
      "Iteration 111, loss = 0.29124186\n",
      "Iteration 112, loss = 0.28887849\n",
      "Iteration 113, loss = 0.28654230\n",
      "Iteration 114, loss = 0.28423540\n",
      "Iteration 115, loss = 0.28195725\n",
      "Iteration 116, loss = 0.27969970\n",
      "Iteration 117, loss = 0.27745912\n",
      "Iteration 118, loss = 0.27524157\n",
      "Iteration 119, loss = 0.27305266\n",
      "Iteration 120, loss = 0.27089704\n",
      "Iteration 121, loss = 0.26877862\n",
      "Iteration 122, loss = 0.26669817\n",
      "Iteration 123, loss = 0.26465151\n",
      "Iteration 124, loss = 0.26263208\n",
      "Iteration 125, loss = 0.26063645\n",
      "Iteration 126, loss = 0.25866529\n",
      "Iteration 127, loss = 0.25672170\n",
      "Iteration 128, loss = 0.25480595\n",
      "Iteration 129, loss = 0.25291550\n",
      "Iteration 130, loss = 0.25104853\n",
      "Iteration 131, loss = 0.24920563\n",
      "Iteration 132, loss = 0.24738500\n",
      "Iteration 133, loss = 0.24558170\n",
      "Iteration 134, loss = 0.24379122\n",
      "Iteration 135, loss = 0.24201241\n",
      "Iteration 136, loss = 0.24024515\n",
      "Iteration 137, loss = 0.23848819\n",
      "Iteration 138, loss = 0.23674020\n",
      "Iteration 139, loss = 0.23500232\n",
      "Iteration 140, loss = 0.23327609\n",
      "Iteration 141, loss = 0.23155685\n",
      "Iteration 142, loss = 0.22983752\n",
      "Iteration 143, loss = 0.22811929\n",
      "Iteration 144, loss = 0.22640674\n",
      "Iteration 145, loss = 0.22470126\n",
      "Iteration 146, loss = 0.22300463\n",
      "Iteration 147, loss = 0.22132360\n",
      "Iteration 148, loss = 0.21964108\n",
      "Iteration 149, loss = 0.21795651\n",
      "Iteration 150, loss = 0.21627714\n",
      "Iteration 151, loss = 0.21460239\n",
      "Iteration 152, loss = 0.21292917\n",
      "Iteration 153, loss = 0.21125465\n",
      "Iteration 154, loss = 0.20957335\n",
      "Iteration 155, loss = 0.20788668\n",
      "Iteration 156, loss = 0.20619980\n",
      "Iteration 157, loss = 0.20450706\n",
      "Iteration 158, loss = 0.20279972\n",
      "Iteration 159, loss = 0.20108353\n",
      "Iteration 160, loss = 0.19936389\n",
      "Iteration 161, loss = 0.19763123\n",
      "Iteration 162, loss = 0.19588149\n",
      "Iteration 163, loss = 0.19412002\n",
      "Iteration 164, loss = 0.19234290\n",
      "Iteration 165, loss = 0.19054452\n",
      "Iteration 166, loss = 0.18872588\n",
      "Iteration 167, loss = 0.18688620\n",
      "Iteration 168, loss = 0.18502227\n",
      "Iteration 169, loss = 0.18313303\n",
      "Iteration 170, loss = 0.18122164\n",
      "Iteration 171, loss = 0.17928977\n",
      "Iteration 172, loss = 0.17733683\n",
      "Iteration 173, loss = 0.17536474\n",
      "Iteration 174, loss = 0.17337741\n",
      "Iteration 175, loss = 0.17139053\n",
      "Iteration 176, loss = 0.16942283\n",
      "Iteration 177, loss = 0.16749702\n",
      "Iteration 178, loss = 0.16545426\n",
      "Iteration 179, loss = 0.16334692\n",
      "Iteration 180, loss = 0.16131821\n",
      "Iteration 181, loss = 0.15938100\n",
      "Iteration 182, loss = 0.15738539\n",
      "Iteration 183, loss = 0.15528634\n",
      "Iteration 184, loss = 0.15325709\n",
      "Iteration 185, loss = 0.15130021\n",
      "Iteration 186, loss = 0.14927641\n",
      "Iteration 187, loss = 0.14721564\n",
      "Iteration 188, loss = 0.14522627\n",
      "Iteration 189, loss = 0.14328372\n",
      "Iteration 190, loss = 0.14130765\n",
      "Iteration 191, loss = 0.13932608\n",
      "Iteration 192, loss = 0.13741099\n",
      "Iteration 193, loss = 0.13552776\n",
      "Iteration 194, loss = 0.13362614\n",
      "Iteration 195, loss = 0.13174800\n",
      "Iteration 196, loss = 0.12992840\n",
      "Iteration 197, loss = 0.12813045\n",
      "Iteration 198, loss = 0.12633031\n",
      "Iteration 199, loss = 0.12455982\n",
      "Iteration 200, loss = 0.12283514\n",
      "Iteration 201, loss = 0.12112776\n",
      "Iteration 202, loss = 0.11942967\n",
      "Iteration 203, loss = 0.11776583\n",
      "Iteration 204, loss = 0.11613711\n",
      "Iteration 205, loss = 0.11452199\n",
      "Iteration 206, loss = 0.11292332\n",
      "Iteration 207, loss = 0.11135673\n",
      "Iteration 208, loss = 0.10981315\n",
      "Iteration 209, loss = 0.10827999\n",
      "Iteration 210, loss = 0.10676614\n",
      "Iteration 211, loss = 0.10527535\n",
      "Iteration 212, loss = 0.10379525\n",
      "Iteration 213, loss = 0.10232344\n",
      "Iteration 214, loss = 0.10086547\n",
      "Iteration 215, loss = 0.09941389\n",
      "Iteration 216, loss = 0.09795840\n",
      "Iteration 217, loss = 0.09649773\n",
      "Iteration 218, loss = 0.09502509\n",
      "Iteration 219, loss = 0.09352571\n",
      "Iteration 220, loss = 0.09199282\n",
      "Iteration 221, loss = 0.09043039\n",
      "Iteration 222, loss = 0.08884953\n",
      "Iteration 223, loss = 0.08726490\n",
      "Iteration 224, loss = 0.08568031\n",
      "Iteration 225, loss = 0.08407969\n",
      "Iteration 226, loss = 0.08245500\n",
      "Iteration 227, loss = 0.08082000\n",
      "Iteration 228, loss = 0.07919350\n",
      "Iteration 229, loss = 0.07758406\n",
      "Iteration 230, loss = 0.07599090\n",
      "Iteration 231, loss = 0.07441109\n",
      "Iteration 232, loss = 0.07284500\n",
      "Iteration 233, loss = 0.07129826\n",
      "Iteration 234, loss = 0.06977945\n",
      "Iteration 235, loss = 0.06829710\n",
      "Iteration 236, loss = 0.06685478\n",
      "Iteration 237, loss = 0.06545123\n",
      "Iteration 238, loss = 0.06408391\n",
      "Iteration 239, loss = 0.06275208\n",
      "Iteration 240, loss = 0.06145730\n",
      "Iteration 241, loss = 0.06020193\n",
      "Iteration 242, loss = 0.05898698\n",
      "Iteration 243, loss = 0.05781153\n",
      "Iteration 244, loss = 0.05667493\n",
      "Iteration 245, loss = 0.05557707\n",
      "Iteration 246, loss = 0.05451650\n",
      "Iteration 247, loss = 0.05349046\n",
      "Iteration 248, loss = 0.05249645\n",
      "Iteration 249, loss = 0.05153251\n",
      "Iteration 250, loss = 0.05059781\n",
      "Iteration 251, loss = 0.04969251\n",
      "Iteration 252, loss = 0.04881638\n",
      "Iteration 253, loss = 0.04796850\n",
      "Iteration 254, loss = 0.04714695\n",
      "Iteration 255, loss = 0.04634860\n",
      "Iteration 256, loss = 0.04557009\n",
      "Iteration 257, loss = 0.04480877\n",
      "Iteration 258, loss = 0.04406476\n",
      "Iteration 259, loss = 0.04334211\n",
      "Iteration 260, loss = 0.04264396\n",
      "Iteration 261, loss = 0.04196697\n",
      "Iteration 262, loss = 0.04130054\n",
      "Iteration 263, loss = 0.04064459\n",
      "Iteration 264, loss = 0.04003613\n",
      "Iteration 265, loss = 0.03943868\n",
      "Iteration 266, loss = 0.03883147\n",
      "Iteration 267, loss = 0.03825466\n",
      "Iteration 268, loss = 0.03769648\n",
      "Iteration 269, loss = 0.03715753\n",
      "Iteration 270, loss = 0.03662562\n",
      "Iteration 271, loss = 0.03611023\n",
      "Iteration 272, loss = 0.03560853\n",
      "Iteration 273, loss = 0.03512486\n",
      "Iteration 274, loss = 0.03465147\n",
      "Iteration 275, loss = 0.03418607\n",
      "Iteration 276, loss = 0.03373344\n",
      "Iteration 277, loss = 0.03329494\n",
      "Iteration 278, loss = 0.03286688\n",
      "Iteration 279, loss = 0.03244600\n",
      "Iteration 280, loss = 0.03203385\n",
      "Iteration 281, loss = 0.03163173\n",
      "Iteration 282, loss = 0.03123752\n",
      "Iteration 283, loss = 0.03084444\n",
      "Iteration 284, loss = 0.03044656\n",
      "Iteration 285, loss = 0.03003970\n",
      "Iteration 286, loss = 0.02963138\n",
      "Iteration 287, loss = 0.02927280\n",
      "Iteration 288, loss = 0.02894447\n",
      "Iteration 289, loss = 0.02858891\n",
      "Iteration 290, loss = 0.02823576\n",
      "Iteration 291, loss = 0.02791452\n",
      "Iteration 292, loss = 0.02760339\n",
      "Iteration 293, loss = 0.02729187\n",
      "Iteration 294, loss = 0.02697883\n",
      "Iteration 295, loss = 0.02667420\n",
      "Iteration 296, loss = 0.02638135\n",
      "Iteration 297, loss = 0.02609427\n",
      "Iteration 298, loss = 0.02581236\n",
      "Iteration 299, loss = 0.02553276\n",
      "Iteration 300, loss = 0.02526052\n",
      "Iteration 301, loss = 0.02499353\n",
      "Iteration 302, loss = 0.02473318\n",
      "Iteration 303, loss = 0.02447781\n",
      "Iteration 304, loss = 0.02422544\n",
      "Iteration 305, loss = 0.02397786\n",
      "Iteration 306, loss = 0.02373472\n",
      "Iteration 307, loss = 0.02349799\n",
      "Iteration 308, loss = 0.02326569\n",
      "Iteration 309, loss = 0.02303667\n",
      "Iteration 310, loss = 0.02281160\n",
      "Iteration 311, loss = 0.02259040\n",
      "Iteration 312, loss = 0.02237414\n",
      "Iteration 313, loss = 0.02216153\n",
      "Iteration 314, loss = 0.02195200\n",
      "Iteration 315, loss = 0.02174607\n",
      "Iteration 316, loss = 0.02154368\n",
      "Iteration 317, loss = 0.02134538\n",
      "Iteration 318, loss = 0.02115045\n",
      "Iteration 319, loss = 0.02095843\n",
      "Iteration 320, loss = 0.02076968\n",
      "Iteration 321, loss = 0.02058415\n",
      "Iteration 322, loss = 0.02040204\n",
      "Iteration 323, loss = 0.02022300\n",
      "Iteration 324, loss = 0.02004660\n",
      "Iteration 325, loss = 0.01987307\n",
      "Iteration 326, loss = 0.01970240\n",
      "Iteration 327, loss = 0.01953456\n",
      "Iteration 328, loss = 0.01936948\n",
      "Iteration 329, loss = 0.01920688\n",
      "Iteration 330, loss = 0.01904683\n",
      "Iteration 331, loss = 0.01888938\n",
      "Iteration 332, loss = 0.01873438\n",
      "Iteration 333, loss = 0.01858176\n",
      "Iteration 334, loss = 0.01843140\n",
      "Iteration 335, loss = 0.01828327\n",
      "Iteration 336, loss = 0.01813745\n",
      "Iteration 337, loss = 0.01799387\n",
      "Iteration 338, loss = 0.01785243\n",
      "Iteration 339, loss = 0.01771305\n",
      "Iteration 340, loss = 0.01757567\n",
      "Iteration 341, loss = 0.01744029\n",
      "Iteration 342, loss = 0.01730691\n",
      "Iteration 343, loss = 0.01717542\n",
      "Iteration 344, loss = 0.01704577\n",
      "Iteration 345, loss = 0.01691789\n",
      "Iteration 346, loss = 0.01679176\n",
      "Iteration 347, loss = 0.01666733\n",
      "Iteration 348, loss = 0.01654455\n",
      "Iteration 349, loss = 0.01642333\n",
      "Iteration 350, loss = 0.01630360\n",
      "Iteration 351, loss = 0.01618529\n",
      "Iteration 352, loss = 0.01606833\n",
      "Iteration 353, loss = 0.01595269\n",
      "Iteration 354, loss = 0.01583830\n",
      "Iteration 355, loss = 0.01572509\n",
      "Iteration 356, loss = 0.01561303\n",
      "Iteration 357, loss = 0.01550210\n",
      "Iteration 358, loss = 0.01539230\n",
      "Iteration 359, loss = 0.01528364\n",
      "Iteration 360, loss = 0.01517614\n",
      "Iteration 361, loss = 0.01506982\n",
      "Iteration 362, loss = 0.01496468\n",
      "Iteration 363, loss = 0.01486074\n",
      "Iteration 364, loss = 0.01475801\n",
      "Iteration 365, loss = 0.01465651\n",
      "Iteration 366, loss = 0.01455624\n",
      "Iteration 367, loss = 0.01445722\n",
      "Iteration 368, loss = 0.01435945\n",
      "Iteration 369, loss = 0.01426294\n",
      "Iteration 370, loss = 0.01416771\n",
      "Iteration 371, loss = 0.01407377\n",
      "Iteration 372, loss = 0.01398113\n",
      "Iteration 373, loss = 0.01388981\n",
      "Iteration 374, loss = 0.01379979\n",
      "Iteration 375, loss = 0.01371108\n",
      "Iteration 376, loss = 0.01362366\n",
      "Iteration 377, loss = 0.01353752\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.80      0.65      0.72        37\n",
      "      Normal       0.74      0.86      0.80        43\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.77      0.75      0.76        80\n",
      "weighted avg       0.77      0.76      0.76        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.4, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "Checando os valores de precision, recall e accuracy para os três casos, podemos observar que a mudança de proporção de 20% para 40% do set_train, não resultou em melhoras significativas. Tivemos um pequeno aumento da accurary de 78% para 81%, porém no caso da precision dos casos anormais, por exemplo, tivemos uma diminuição de 1%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**EXPERIMENTO20**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Irei realizar o mesmo experimento, porém agora o set_train terá 90% dos dados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Com o dataSet original:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['class']\n",
    "x = data.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.65852627\n",
      "Iteration 2, loss = 0.62411831\n",
      "Iteration 3, loss = 0.59582311\n",
      "Iteration 4, loss = 0.57342462\n",
      "Iteration 5, loss = 0.55656374\n",
      "Iteration 6, loss = 0.54473558\n",
      "Iteration 7, loss = 0.53729024\n",
      "Iteration 8, loss = 0.53344647\n",
      "Iteration 9, loss = 0.53233454\n",
      "Iteration 10, loss = 0.53305675\n",
      "Iteration 11, loss = 0.53475935\n",
      "Iteration 12, loss = 0.53671007\n",
      "Iteration 13, loss = 0.53835658\n",
      "Iteration 14, loss = 0.53934625\n",
      "Iteration 15, loss = 0.53950655\n",
      "Iteration 16, loss = 0.53880771\n",
      "Iteration 17, loss = 0.53732649\n",
      "Iteration 18, loss = 0.53521152\n",
      "Iteration 19, loss = 0.53265077\n",
      "Iteration 20, loss = 0.52984265\n",
      "Iteration 21, loss = 0.52697282\n",
      "Iteration 22, loss = 0.52419829\n",
      "Iteration 23, loss = 0.52163592\n",
      "Iteration 24, loss = 0.51935306\n",
      "Iteration 25, loss = 0.51736518\n",
      "Iteration 26, loss = 0.51564305\n",
      "Iteration 27, loss = 0.51412704\n",
      "Iteration 28, loss = 0.51273852\n",
      "Iteration 29, loss = 0.51138606\n",
      "Iteration 30, loss = 0.50998662\n",
      "Iteration 31, loss = 0.50848312\n",
      "Iteration 32, loss = 0.50684253\n",
      "Iteration 33, loss = 0.50503801\n",
      "Iteration 34, loss = 0.50304408\n",
      "Iteration 35, loss = 0.50084614\n",
      "Iteration 36, loss = 0.49843869\n",
      "Iteration 37, loss = 0.49582615\n",
      "Iteration 38, loss = 0.49302571\n",
      "Iteration 39, loss = 0.49006269\n",
      "Iteration 40, loss = 0.48696254\n",
      "Iteration 41, loss = 0.48374215\n",
      "Iteration 42, loss = 0.48040286\n",
      "Iteration 43, loss = 0.47693295\n",
      "Iteration 44, loss = 0.47332012\n",
      "Iteration 45, loss = 0.46956269\n",
      "Iteration 46, loss = 0.46566900\n",
      "Iteration 47, loss = 0.46164596\n",
      "Iteration 48, loss = 0.45749184\n",
      "Iteration 49, loss = 0.45319295\n",
      "Iteration 50, loss = 0.44872550\n",
      "Iteration 51, loss = 0.44406900\n",
      "Iteration 52, loss = 0.43921696\n",
      "Iteration 53, loss = 0.43417481\n",
      "Iteration 54, loss = 0.42894452\n",
      "Iteration 55, loss = 0.42352551\n",
      "Iteration 56, loss = 0.41793641\n",
      "Iteration 57, loss = 0.41221976\n",
      "Iteration 58, loss = 0.40641546\n",
      "Iteration 59, loss = 0.40055213\n",
      "Iteration 60, loss = 0.39464295\n",
      "Iteration 61, loss = 0.38867899\n",
      "Iteration 62, loss = 0.38263506\n",
      "Iteration 63, loss = 0.37648765\n",
      "Iteration 64, loss = 0.37023611\n",
      "Iteration 65, loss = 0.36390945\n",
      "Iteration 66, loss = 0.35753898\n",
      "Iteration 67, loss = 0.35115223\n",
      "Iteration 68, loss = 0.34476268\n",
      "Iteration 69, loss = 0.33831909\n",
      "Iteration 70, loss = 0.33176604\n",
      "Iteration 71, loss = 0.32511628\n",
      "Iteration 72, loss = 0.31839556\n",
      "Iteration 73, loss = 0.31160527\n",
      "Iteration 74, loss = 0.30477163\n",
      "Iteration 75, loss = 0.29796123\n",
      "Iteration 76, loss = 0.29119403\n",
      "Iteration 77, loss = 0.28439679\n",
      "Iteration 78, loss = 0.27751472\n",
      "Iteration 79, loss = 0.27054307\n",
      "Iteration 80, loss = 0.26349236\n",
      "Iteration 81, loss = 0.25638348\n",
      "Iteration 82, loss = 0.24925369\n",
      "Iteration 83, loss = 0.24213900\n",
      "Iteration 84, loss = 0.23506325\n",
      "Iteration 85, loss = 0.22803982\n",
      "Iteration 86, loss = 0.22106504\n",
      "Iteration 87, loss = 0.21411931\n",
      "Iteration 88, loss = 0.20719223\n",
      "Iteration 89, loss = 0.20028790\n",
      "Iteration 90, loss = 0.19341590\n",
      "Iteration 91, loss = 0.18658525\n",
      "Iteration 92, loss = 0.17981040\n",
      "Iteration 93, loss = 0.17311458\n",
      "Iteration 94, loss = 0.16651571\n",
      "Iteration 95, loss = 0.16001607\n",
      "Iteration 96, loss = 0.15361050\n",
      "Iteration 97, loss = 0.14729901\n",
      "Iteration 98, loss = 0.14108239\n",
      "Iteration 99, loss = 0.13494771\n",
      "Iteration 100, loss = 0.12888948\n",
      "Iteration 101, loss = 0.12293120\n",
      "Iteration 102, loss = 0.11710032\n",
      "Iteration 103, loss = 0.11141369\n",
      "Iteration 104, loss = 0.10589040\n",
      "Iteration 105, loss = 0.10055322\n",
      "Iteration 106, loss = 0.09540432\n",
      "Iteration 107, loss = 0.09043585\n",
      "Iteration 108, loss = 0.08564674\n",
      "Iteration 109, loss = 0.08103920\n",
      "Iteration 110, loss = 0.07660793\n",
      "Iteration 111, loss = 0.07237352\n",
      "Iteration 112, loss = 0.06836080\n",
      "Iteration 113, loss = 0.06457873\n",
      "Iteration 114, loss = 0.06102830\n",
      "Iteration 115, loss = 0.05770241\n",
      "Iteration 116, loss = 0.05458951\n",
      "Iteration 117, loss = 0.05167888\n",
      "Iteration 118, loss = 0.04896083\n",
      "Iteration 119, loss = 0.04642273\n",
      "Iteration 120, loss = 0.04404696\n",
      "Iteration 121, loss = 0.04181910\n",
      "Iteration 122, loss = 0.03973245\n",
      "Iteration 123, loss = 0.03778421\n",
      "Iteration 124, loss = 0.03596938\n",
      "Iteration 125, loss = 0.03427643\n",
      "Iteration 126, loss = 0.03269126\n",
      "Iteration 127, loss = 0.03120468\n",
      "Iteration 128, loss = 0.02981263\n",
      "Iteration 129, loss = 0.02851253\n",
      "Iteration 130, loss = 0.02729996\n",
      "Iteration 131, loss = 0.02616858\n",
      "Iteration 132, loss = 0.02511154\n",
      "Iteration 133, loss = 0.02412235\n",
      "Iteration 134, loss = 0.02319470\n",
      "Iteration 135, loss = 0.02232321\n",
      "Iteration 136, loss = 0.02150462\n",
      "Iteration 137, loss = 0.02073824\n",
      "Iteration 138, loss = 0.02002083\n",
      "Iteration 139, loss = 0.01934650\n",
      "Iteration 140, loss = 0.01871167\n",
      "Iteration 141, loss = 0.01811442\n",
      "Iteration 142, loss = 0.01755345\n",
      "Iteration 143, loss = 0.01702645\n",
      "Iteration 144, loss = 0.01653003\n",
      "Iteration 145, loss = 0.01606049\n",
      "Iteration 146, loss = 0.01561524\n",
      "Iteration 147, loss = 0.01519304\n",
      "Iteration 148, loss = 0.01479324\n",
      "Iteration 149, loss = 0.01441510\n",
      "Iteration 150, loss = 0.01405671\n",
      "Iteration 151, loss = 0.01371538\n",
      "Iteration 152, loss = 0.01338958\n",
      "Iteration 153, loss = 0.01307925\n",
      "Iteration 154, loss = 0.01278423\n",
      "Iteration 155, loss = 0.01250333\n",
      "Iteration 156, loss = 0.01223469\n",
      "Iteration 157, loss = 0.01197722\n",
      "Iteration 158, loss = 0.01173079\n",
      "Iteration 159, loss = 0.01149531\n",
      "Iteration 160, loss = 0.01126981\n",
      "Iteration 161, loss = 0.01105301\n",
      "Iteration 162, loss = 0.01084436\n",
      "Iteration 163, loss = 0.01064385\n",
      "Iteration 164, loss = 0.01045117\n",
      "Iteration 165, loss = 0.01026559\n",
      "Iteration 166, loss = 0.01008653\n",
      "Iteration 167, loss = 0.00991391\n",
      "Iteration 168, loss = 0.00974763\n",
      "Iteration 169, loss = 0.00958729\n",
      "Iteration 170, loss = 0.00943239\n",
      "Iteration 171, loss = 0.00928278\n",
      "Iteration 172, loss = 0.00913838\n",
      "Iteration 173, loss = 0.00899891\n",
      "Iteration 174, loss = 0.00886398\n",
      "Iteration 175, loss = 0.00873337\n",
      "Iteration 176, loss = 0.00860694\n",
      "Iteration 177, loss = 0.00848444\n",
      "Iteration 178, loss = 0.00836560\n",
      "Iteration 179, loss = 0.00825024\n",
      "Iteration 180, loss = 0.00813822\n",
      "Iteration 181, loss = 0.00802939\n",
      "Iteration 182, loss = 0.00792358\n",
      "Iteration 183, loss = 0.00782065\n",
      "Iteration 184, loss = 0.00772050\n",
      "Iteration 185, loss = 0.00762304\n",
      "Iteration 186, loss = 0.00752817\n",
      "Iteration 187, loss = 0.00743577\n",
      "Iteration 188, loss = 0.00734574\n",
      "Iteration 189, loss = 0.00725804\n",
      "Iteration 190, loss = 0.00717257\n",
      "Iteration 191, loss = 0.00708922\n",
      "Iteration 192, loss = 0.00700792\n",
      "Iteration 193, loss = 0.00692863\n",
      "Iteration 194, loss = 0.00685125\n",
      "Iteration 195, loss = 0.00677569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7956989247311828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.80      0.92      0.86       186\n",
      "      Normal       0.78      0.54      0.64        93\n",
      "\n",
      "    accuracy                           0.80       279\n",
      "   macro avg       0.79      0.73      0.75       279\n",
      "weighted avg       0.79      0.80      0.78       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "**EXPERIMENTO21**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Usando a mesma proporção de 90% e 10%, só que agora considerando a exclusão das 6 primerias variáveis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_data['class']\n",
    "x = new_data.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.70801206\n",
      "Iteration 2, loss = 0.69644288\n",
      "Iteration 3, loss = 0.68526906\n",
      "Iteration 4, loss = 0.67448786\n",
      "Iteration 5, loss = 0.66410456\n",
      "Iteration 6, loss = 0.65412720\n",
      "Iteration 7, loss = 0.64456431\n",
      "Iteration 8, loss = 0.63542316\n",
      "Iteration 9, loss = 0.62670896\n",
      "Iteration 10, loss = 0.61842515\n",
      "Iteration 11, loss = 0.61057376\n",
      "Iteration 12, loss = 0.60315510\n",
      "Iteration 13, loss = 0.59616731\n",
      "Iteration 14, loss = 0.58960624\n",
      "Iteration 15, loss = 0.58346632\n",
      "Iteration 16, loss = 0.57774171\n",
      "Iteration 17, loss = 0.57242696\n",
      "Iteration 18, loss = 0.56751646\n",
      "Iteration 19, loss = 0.56300327\n",
      "Iteration 20, loss = 0.55887758\n",
      "Iteration 21, loss = 0.55512508\n",
      "Iteration 22, loss = 0.55172671\n",
      "Iteration 23, loss = 0.54865970\n",
      "Iteration 24, loss = 0.54589884\n",
      "Iteration 25, loss = 0.54341777\n",
      "Iteration 26, loss = 0.54119020\n",
      "Iteration 27, loss = 0.53919122\n",
      "Iteration 28, loss = 0.53739801\n",
      "Iteration 29, loss = 0.53578966\n",
      "Iteration 30, loss = 0.53434636\n",
      "Iteration 31, loss = 0.53304866\n",
      "Iteration 32, loss = 0.53187730\n",
      "Iteration 33, loss = 0.53081398\n",
      "Iteration 34, loss = 0.52984240\n",
      "Iteration 35, loss = 0.52894878\n",
      "Iteration 36, loss = 0.52812175\n",
      "Iteration 37, loss = 0.52735175\n",
      "Iteration 38, loss = 0.52663052\n",
      "Iteration 39, loss = 0.52595073\n",
      "Iteration 40, loss = 0.52530642\n",
      "Iteration 41, loss = 0.52469472\n",
      "Iteration 42, loss = 0.52411585\n",
      "Iteration 43, loss = 0.52357059\n",
      "Iteration 44, loss = 0.52305560\n",
      "Iteration 45, loss = 0.52255751\n",
      "Iteration 46, loss = 0.52205443\n",
      "Iteration 47, loss = 0.52152697\n",
      "Iteration 48, loss = 0.52096253\n",
      "Iteration 49, loss = 0.52035281\n",
      "Iteration 50, loss = 0.51969197\n",
      "Iteration 51, loss = 0.51897676\n",
      "Iteration 52, loss = 0.51820370\n",
      "Iteration 53, loss = 0.51736736\n",
      "Iteration 54, loss = 0.51646419\n",
      "Iteration 55, loss = 0.51549727\n",
      "Iteration 56, loss = 0.51447708\n",
      "Iteration 57, loss = 0.51341885\n",
      "Iteration 58, loss = 0.51233651\n",
      "Iteration 59, loss = 0.51123550\n",
      "Iteration 60, loss = 0.51011015\n",
      "Iteration 61, loss = 0.50894797\n",
      "Iteration 62, loss = 0.50773473\n",
      "Iteration 63, loss = 0.50645690\n",
      "Iteration 64, loss = 0.50510846\n",
      "Iteration 65, loss = 0.50369421\n",
      "Iteration 66, loss = 0.50222228\n",
      "Iteration 67, loss = 0.50069623\n",
      "Iteration 68, loss = 0.49911337\n",
      "Iteration 69, loss = 0.49746803\n",
      "Iteration 70, loss = 0.49575583\n",
      "Iteration 71, loss = 0.49397739\n",
      "Iteration 72, loss = 0.49214074\n",
      "Iteration 73, loss = 0.49025926\n",
      "Iteration 74, loss = 0.48834367\n",
      "Iteration 75, loss = 0.48639745\n",
      "Iteration 76, loss = 0.48441820\n",
      "Iteration 77, loss = 0.48239966\n",
      "Iteration 78, loss = 0.48033687\n",
      "Iteration 79, loss = 0.47822684\n",
      "Iteration 80, loss = 0.47606750\n",
      "Iteration 81, loss = 0.47385812\n",
      "Iteration 82, loss = 0.47159865\n",
      "Iteration 83, loss = 0.46928825\n",
      "Iteration 84, loss = 0.46692338\n",
      "Iteration 85, loss = 0.46449751\n",
      "Iteration 86, loss = 0.46200740\n",
      "Iteration 87, loss = 0.45945853\n",
      "Iteration 88, loss = 0.45686267\n",
      "Iteration 89, loss = 0.45423043\n",
      "Iteration 90, loss = 0.45156620\n",
      "Iteration 91, loss = 0.44886888\n",
      "Iteration 92, loss = 0.44613508\n",
      "Iteration 93, loss = 0.44336066\n",
      "Iteration 94, loss = 0.44054016\n",
      "Iteration 95, loss = 0.43766856\n",
      "Iteration 96, loss = 0.43474298\n",
      "Iteration 97, loss = 0.43176406\n",
      "Iteration 98, loss = 0.42874147\n",
      "Iteration 99, loss = 0.42569313\n",
      "Iteration 100, loss = 0.42262232\n",
      "Iteration 101, loss = 0.41951120\n",
      "Iteration 102, loss = 0.41632982\n",
      "Iteration 103, loss = 0.41308293\n",
      "Iteration 104, loss = 0.40978853\n",
      "Iteration 105, loss = 0.40645509\n",
      "Iteration 106, loss = 0.40308226\n",
      "Iteration 107, loss = 0.39966254\n",
      "Iteration 108, loss = 0.39619106\n",
      "Iteration 109, loss = 0.39266547\n",
      "Iteration 110, loss = 0.38908520\n",
      "Iteration 111, loss = 0.38544824\n",
      "Iteration 112, loss = 0.38175027\n",
      "Iteration 113, loss = 0.37798928\n",
      "Iteration 114, loss = 0.37416839\n",
      "Iteration 115, loss = 0.37030509\n",
      "Iteration 116, loss = 0.36642136\n",
      "Iteration 117, loss = 0.36252829\n",
      "Iteration 118, loss = 0.35860983\n",
      "Iteration 119, loss = 0.35464751\n",
      "Iteration 120, loss = 0.35064933\n",
      "Iteration 121, loss = 0.34664239\n",
      "Iteration 122, loss = 0.34263484\n",
      "Iteration 123, loss = 0.33860547\n",
      "Iteration 124, loss = 0.33454036\n",
      "Iteration 125, loss = 0.33044631\n",
      "Iteration 126, loss = 0.32633217\n",
      "Iteration 127, loss = 0.32220296\n",
      "Iteration 128, loss = 0.31805854\n",
      "Iteration 129, loss = 0.31389349\n",
      "Iteration 130, loss = 0.30970562\n",
      "Iteration 131, loss = 0.30549939\n",
      "Iteration 132, loss = 0.30128838\n",
      "Iteration 133, loss = 0.29708691\n",
      "Iteration 134, loss = 0.29290578\n",
      "Iteration 135, loss = 0.28875351\n",
      "Iteration 136, loss = 0.28464403\n",
      "Iteration 137, loss = 0.28059564\n",
      "Iteration 138, loss = 0.27660195\n",
      "Iteration 139, loss = 0.27263508\n",
      "Iteration 140, loss = 0.26869484\n",
      "Iteration 141, loss = 0.26478934\n",
      "Iteration 142, loss = 0.26092072\n",
      "Iteration 143, loss = 0.25709392\n",
      "Iteration 144, loss = 0.25331395\n",
      "Iteration 145, loss = 0.24958009\n",
      "Iteration 146, loss = 0.24589053\n",
      "Iteration 147, loss = 0.24224644\n",
      "Iteration 148, loss = 0.23865160\n",
      "Iteration 149, loss = 0.23511368\n",
      "Iteration 150, loss = 0.23164095\n",
      "Iteration 151, loss = 0.22823314\n",
      "Iteration 152, loss = 0.22488127\n",
      "Iteration 153, loss = 0.22158173\n",
      "Iteration 154, loss = 0.21832812\n",
      "Iteration 155, loss = 0.21511921\n",
      "Iteration 156, loss = 0.21195858\n",
      "Iteration 157, loss = 0.20884518\n",
      "Iteration 158, loss = 0.20577275\n",
      "Iteration 159, loss = 0.20273403\n",
      "Iteration 160, loss = 0.19972422\n",
      "Iteration 161, loss = 0.19674071\n",
      "Iteration 162, loss = 0.19378127\n",
      "Iteration 163, loss = 0.19084346\n",
      "Iteration 164, loss = 0.18792442\n",
      "Iteration 165, loss = 0.18502208\n",
      "Iteration 166, loss = 0.18213661\n",
      "Iteration 167, loss = 0.17926656\n",
      "Iteration 168, loss = 0.17640594\n",
      "Iteration 169, loss = 0.17355311\n",
      "Iteration 170, loss = 0.17070981\n",
      "Iteration 171, loss = 0.16788012\n",
      "Iteration 172, loss = 0.16507144\n",
      "Iteration 173, loss = 0.16229498\n",
      "Iteration 174, loss = 0.15956588\n",
      "Iteration 175, loss = 0.15688519\n",
      "Iteration 176, loss = 0.15423535\n",
      "Iteration 177, loss = 0.15162571\n",
      "Iteration 178, loss = 0.14907503\n",
      "Iteration 179, loss = 0.14661534\n",
      "Iteration 180, loss = 0.14420555\n",
      "Iteration 181, loss = 0.14183994\n",
      "Iteration 182, loss = 0.13952639\n",
      "Iteration 183, loss = 0.13726565\n",
      "Iteration 184, loss = 0.13504294\n",
      "Iteration 185, loss = 0.13284785\n",
      "Iteration 186, loss = 0.13070760\n",
      "Iteration 187, loss = 0.12862784\n",
      "Iteration 188, loss = 0.12658879\n",
      "Iteration 189, loss = 0.12457931\n",
      "Iteration 190, loss = 0.12260648\n",
      "Iteration 191, loss = 0.12067646\n",
      "Iteration 192, loss = 0.11878011\n",
      "Iteration 193, loss = 0.11691203\n",
      "Iteration 194, loss = 0.11508322\n",
      "Iteration 195, loss = 0.11329662\n",
      "Iteration 196, loss = 0.11154305\n",
      "Iteration 197, loss = 0.10982123\n",
      "Iteration 198, loss = 0.10813878\n",
      "Iteration 199, loss = 0.10649484\n",
      "Iteration 200, loss = 0.10488301\n",
      "Iteration 201, loss = 0.10330480\n",
      "Iteration 202, loss = 0.10176357\n",
      "Iteration 203, loss = 0.10025539\n",
      "Iteration 204, loss = 0.09877824\n",
      "Iteration 205, loss = 0.09733575\n",
      "Iteration 206, loss = 0.09592633\n",
      "Iteration 207, loss = 0.09454688\n",
      "Iteration 208, loss = 0.09319947\n",
      "Iteration 209, loss = 0.09188364\n",
      "Iteration 210, loss = 0.09059618\n",
      "Iteration 211, loss = 0.08933799\n",
      "Iteration 212, loss = 0.08810864\n",
      "Iteration 213, loss = 0.08690558\n",
      "Iteration 214, loss = 0.08572945\n",
      "Iteration 215, loss = 0.08457960\n",
      "Iteration 216, loss = 0.08345463\n",
      "Iteration 217, loss = 0.08235533\n",
      "Iteration 218, loss = 0.08128051\n",
      "Iteration 219, loss = 0.08022933\n",
      "Iteration 220, loss = 0.07920202\n",
      "Iteration 221, loss = 0.07819726\n",
      "Iteration 222, loss = 0.07721499\n",
      "Iteration 223, loss = 0.07625469\n",
      "Iteration 224, loss = 0.07531536\n",
      "Iteration 225, loss = 0.07439684\n",
      "Iteration 226, loss = 0.07349795\n",
      "Iteration 227, loss = 0.07261834\n",
      "Iteration 228, loss = 0.07175740\n",
      "Iteration 229, loss = 0.07091462\n",
      "Iteration 230, loss = 0.07008982\n",
      "Iteration 231, loss = 0.06928234\n",
      "Iteration 232, loss = 0.06849205\n",
      "Iteration 233, loss = 0.06771829\n",
      "Iteration 234, loss = 0.06696074\n",
      "Iteration 235, loss = 0.06621879\n",
      "Iteration 236, loss = 0.06549202\n",
      "Iteration 237, loss = 0.06477994\n",
      "Iteration 238, loss = 0.06408211\n",
      "Iteration 239, loss = 0.06339816\n",
      "Iteration 240, loss = 0.06272765\n",
      "Iteration 241, loss = 0.06207030\n",
      "Iteration 242, loss = 0.06142570\n",
      "Iteration 243, loss = 0.06079358\n",
      "Iteration 244, loss = 0.06017356\n",
      "Iteration 245, loss = 0.05956538\n",
      "Iteration 246, loss = 0.05896867\n",
      "Iteration 247, loss = 0.05838314\n",
      "Iteration 248, loss = 0.05780844\n",
      "Iteration 249, loss = 0.05724431\n",
      "Iteration 250, loss = 0.05669043\n",
      "Iteration 251, loss = 0.05614654\n",
      "Iteration 252, loss = 0.05561237\n",
      "Iteration 253, loss = 0.05508769\n",
      "Iteration 254, loss = 0.05457224\n",
      "Iteration 255, loss = 0.05406580\n",
      "Iteration 256, loss = 0.05356813\n",
      "Iteration 257, loss = 0.05307902\n",
      "Iteration 258, loss = 0.05259824\n",
      "Iteration 259, loss = 0.05212558\n",
      "Iteration 260, loss = 0.05166083\n",
      "Iteration 261, loss = 0.05120378\n",
      "Iteration 262, loss = 0.05075426\n",
      "Iteration 263, loss = 0.05031206\n",
      "Iteration 264, loss = 0.04987701\n",
      "Iteration 265, loss = 0.04944893\n",
      "Iteration 266, loss = 0.04902765\n",
      "Iteration 267, loss = 0.04861301\n",
      "Iteration 268, loss = 0.04820485\n",
      "Iteration 269, loss = 0.04780302\n",
      "Iteration 270, loss = 0.04740735\n",
      "Iteration 271, loss = 0.04701771\n",
      "Iteration 272, loss = 0.04663396\n",
      "Iteration 273, loss = 0.04625594\n",
      "Iteration 274, loss = 0.04588353\n",
      "Iteration 275, loss = 0.04551660\n",
      "Iteration 276, loss = 0.04515501\n",
      "Iteration 277, loss = 0.04479865\n",
      "Iteration 278, loss = 0.04444739\n",
      "Iteration 279, loss = 0.04410113\n",
      "Iteration 280, loss = 0.04375974\n",
      "Iteration 281, loss = 0.04342312\n",
      "Iteration 282, loss = 0.04309115\n",
      "Iteration 283, loss = 0.04276375\n",
      "Iteration 284, loss = 0.04244079\n",
      "Iteration 285, loss = 0.04212220\n",
      "Iteration 286, loss = 0.04180787\n",
      "Iteration 287, loss = 0.04149771\n",
      "Iteration 288, loss = 0.04119162\n",
      "Iteration 289, loss = 0.04088954\n",
      "Iteration 290, loss = 0.04059136\n",
      "Iteration 291, loss = 0.04029701\n",
      "Iteration 292, loss = 0.04000642\n",
      "Iteration 293, loss = 0.03971949\n",
      "Iteration 294, loss = 0.03943617\n",
      "Iteration 295, loss = 0.03915638\n",
      "Iteration 296, loss = 0.03888005\n",
      "Iteration 297, loss = 0.03860712\n",
      "Iteration 298, loss = 0.03833752\n",
      "Iteration 299, loss = 0.03807119\n",
      "Iteration 300, loss = 0.03780807\n",
      "Iteration 301, loss = 0.03754811\n",
      "Iteration 302, loss = 0.03729124\n",
      "Iteration 303, loss = 0.03703743\n",
      "Iteration 304, loss = 0.03678661\n",
      "Iteration 305, loss = 0.03653874\n",
      "Iteration 306, loss = 0.03629377\n",
      "Iteration 307, loss = 0.03605165\n",
      "Iteration 308, loss = 0.03581234\n",
      "Iteration 309, loss = 0.03557580\n",
      "Iteration 310, loss = 0.03534199\n",
      "Iteration 311, loss = 0.03511086\n",
      "Iteration 312, loss = 0.03488238\n",
      "Iteration 313, loss = 0.03465651\n",
      "Iteration 314, loss = 0.03443321\n",
      "Iteration 315, loss = 0.03421244\n",
      "Iteration 316, loss = 0.03399417\n",
      "Iteration 317, loss = 0.03377837\n",
      "Iteration 318, loss = 0.03356499\n",
      "Iteration 319, loss = 0.03335400\n",
      "Iteration 320, loss = 0.03314538\n",
      "Iteration 321, loss = 0.03293908\n",
      "Iteration 322, loss = 0.03273508\n",
      "Iteration 323, loss = 0.03253333\n",
      "Iteration 324, loss = 0.03233382\n",
      "Iteration 325, loss = 0.03213650\n",
      "Iteration 326, loss = 0.03194134\n",
      "Iteration 327, loss = 0.03174832\n",
      "Iteration 328, loss = 0.03155740\n",
      "Iteration 329, loss = 0.03136855\n",
      "Iteration 330, loss = 0.03118174\n",
      "Iteration 331, loss = 0.03099694\n",
      "Iteration 332, loss = 0.03081412\n",
      "Iteration 333, loss = 0.03063325\n",
      "Iteration 334, loss = 0.03045430\n",
      "Iteration 335, loss = 0.03027725\n",
      "Iteration 336, loss = 0.03010206\n",
      "Iteration 337, loss = 0.02992870\n",
      "Iteration 338, loss = 0.02975716\n",
      "Iteration 339, loss = 0.02958740\n",
      "Iteration 340, loss = 0.02941939\n",
      "Iteration 341, loss = 0.02925312\n",
      "Iteration 342, loss = 0.02908854\n",
      "Iteration 343, loss = 0.02892564\n",
      "Iteration 344, loss = 0.02876439\n",
      "Iteration 345, loss = 0.02860477\n",
      "Iteration 346, loss = 0.02844673\n",
      "Iteration 347, loss = 0.02829026\n",
      "Iteration 348, loss = 0.02813531\n",
      "Iteration 349, loss = 0.02798181\n",
      "Iteration 350, loss = 0.02782965\n",
      "Iteration 351, loss = 0.02767866\n",
      "Iteration 352, loss = 0.02752872\n",
      "Iteration 353, loss = 0.02738006\n",
      "Iteration 354, loss = 0.02723335\n",
      "Iteration 355, loss = 0.02708921\n",
      "Iteration 356, loss = 0.02694686\n",
      "Iteration 357, loss = 0.02680514\n",
      "Iteration 358, loss = 0.02666424\n",
      "Iteration 359, loss = 0.02652459\n",
      "Iteration 360, loss = 0.02638645\n",
      "Iteration 361, loss = 0.02624990\n",
      "Iteration 362, loss = 0.02611479\n",
      "Iteration 363, loss = 0.02598095\n",
      "Iteration 364, loss = 0.02584830\n",
      "Iteration 365, loss = 0.02571680\n",
      "Iteration 366, loss = 0.02558642\n",
      "Iteration 367, loss = 0.02545715\n",
      "Iteration 368, loss = 0.02532905\n",
      "Iteration 369, loss = 0.02520215\n",
      "Iteration 370, loss = 0.02507643\n",
      "Iteration 371, loss = 0.02495189\n",
      "Iteration 372, loss = 0.02482851\n",
      "Iteration 373, loss = 0.02470623\n",
      "Iteration 374, loss = 0.02458501\n",
      "Iteration 375, loss = 0.02446482\n",
      "Iteration 376, loss = 0.02434566\n",
      "Iteration 377, loss = 0.02422752\n",
      "Iteration 378, loss = 0.02411041\n",
      "Iteration 379, loss = 0.02399433\n",
      "Iteration 380, loss = 0.02387928\n",
      "Iteration 381, loss = 0.02376523\n",
      "Iteration 382, loss = 0.02365216\n",
      "Iteration 383, loss = 0.02354004\n",
      "Iteration 384, loss = 0.02342887\n",
      "Iteration 385, loss = 0.02331862\n",
      "Iteration 386, loss = 0.02320930\n",
      "Iteration 387, loss = 0.02310089\n",
      "Iteration 388, loss = 0.02299341\n",
      "Iteration 389, loss = 0.02288682\n",
      "Iteration 390, loss = 0.02278114\n",
      "Iteration 391, loss = 0.02267633\n",
      "Iteration 392, loss = 0.02257237\n",
      "Iteration 393, loss = 0.02246926\n",
      "Iteration 394, loss = 0.02236699\n",
      "Iteration 395, loss = 0.02226555\n",
      "Iteration 396, loss = 0.02216495\n",
      "Iteration 397, loss = 0.02206516\n",
      "Iteration 398, loss = 0.02196619\n",
      "Iteration 399, loss = 0.02186801\n",
      "Iteration 400, loss = 0.02177061\n",
      "Iteration 401, loss = 0.02167398\n",
      "Iteration 402, loss = 0.02157812\n",
      "Iteration 403, loss = 0.02148302\n",
      "Iteration 404, loss = 0.02138868\n",
      "Iteration 405, loss = 0.02129508\n",
      "Iteration 406, loss = 0.02120221\n",
      "Iteration 407, loss = 0.02111006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7634408602150538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.78      0.91      0.84       186\n",
      "      Normal       0.72      0.47      0.57        93\n",
      "\n",
      "    accuracy                           0.76       279\n",
      "   macro avg       0.75      0.69      0.70       279\n",
      "weighted avg       0.76      0.76      0.75       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.9, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "**EXPERIMENTO22**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "E agora considerando a mesma porporção de 90% e 10%, só que considerando a exclusão de 110 casos anormais."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_data3['class']\n",
    "x = new_data3.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.75072691\n",
      "Iteration 2, loss = 0.74277735\n",
      "Iteration 3, loss = 0.73527198\n",
      "Iteration 4, loss = 0.72821528\n",
      "Iteration 5, loss = 0.72161107\n",
      "Iteration 6, loss = 0.71546006\n",
      "Iteration 7, loss = 0.70975790\n",
      "Iteration 8, loss = 0.70449737\n",
      "Iteration 9, loss = 0.69967048\n",
      "Iteration 10, loss = 0.69526897\n",
      "Iteration 11, loss = 0.69128427\n",
      "Iteration 12, loss = 0.68770698\n",
      "Iteration 13, loss = 0.68452593\n",
      "Iteration 14, loss = 0.68172779\n",
      "Iteration 15, loss = 0.67929824\n",
      "Iteration 16, loss = 0.67722107\n",
      "Iteration 17, loss = 0.67547542\n",
      "Iteration 18, loss = 0.67403416\n",
      "Iteration 19, loss = 0.67286465\n",
      "Iteration 20, loss = 0.67193111\n",
      "Iteration 21, loss = 0.67119701\n",
      "Iteration 22, loss = 0.67062731\n",
      "Iteration 23, loss = 0.67018910\n",
      "Iteration 24, loss = 0.66985159\n",
      "Iteration 25, loss = 0.66958583\n",
      "Iteration 26, loss = 0.66936427\n",
      "Iteration 27, loss = 0.66916098\n",
      "Iteration 28, loss = 0.66895350\n",
      "Iteration 29, loss = 0.66872527\n",
      "Iteration 30, loss = 0.66846614\n",
      "Iteration 31, loss = 0.66817013\n",
      "Iteration 32, loss = 0.66783189\n",
      "Iteration 33, loss = 0.66744424\n",
      "Iteration 34, loss = 0.66700133\n",
      "Iteration 35, loss = 0.66650513\n",
      "Iteration 36, loss = 0.66596591\n",
      "Iteration 37, loss = 0.66539611\n",
      "Iteration 38, loss = 0.66480444\n",
      "Iteration 39, loss = 0.66419394\n",
      "Iteration 40, loss = 0.66356056\n",
      "Iteration 41, loss = 0.66289328\n",
      "Iteration 42, loss = 0.66218102\n",
      "Iteration 43, loss = 0.66142259\n",
      "Iteration 44, loss = 0.66062656\n",
      "Iteration 45, loss = 0.65979935\n",
      "Iteration 46, loss = 0.65893721\n",
      "Iteration 47, loss = 0.65803432\n",
      "Iteration 48, loss = 0.65709764\n",
      "Iteration 49, loss = 0.65614295\n",
      "Iteration 50, loss = 0.65518217\n",
      "Iteration 51, loss = 0.65421536\n",
      "Iteration 52, loss = 0.65322904\n",
      "Iteration 53, loss = 0.65220803\n",
      "Iteration 54, loss = 0.65114370\n",
      "Iteration 55, loss = 0.65002830\n",
      "Iteration 56, loss = 0.64884776\n",
      "Iteration 57, loss = 0.64759238\n",
      "Iteration 58, loss = 0.64626829\n",
      "Iteration 59, loss = 0.64489146\n",
      "Iteration 60, loss = 0.64347747\n",
      "Iteration 61, loss = 0.64203045\n",
      "Iteration 62, loss = 0.64053678\n",
      "Iteration 63, loss = 0.63897853\n",
      "Iteration 64, loss = 0.63735072\n",
      "Iteration 65, loss = 0.63566350\n",
      "Iteration 66, loss = 0.63393542\n",
      "Iteration 67, loss = 0.63217376\n",
      "Iteration 68, loss = 0.63035718\n",
      "Iteration 69, loss = 0.62845976\n",
      "Iteration 70, loss = 0.62647520\n",
      "Iteration 71, loss = 0.62440475\n",
      "Iteration 72, loss = 0.62224875\n",
      "Iteration 73, loss = 0.62001136\n",
      "Iteration 74, loss = 0.61769221\n",
      "Iteration 75, loss = 0.61528300\n",
      "Iteration 76, loss = 0.61277923\n",
      "Iteration 77, loss = 0.61018683\n",
      "Iteration 78, loss = 0.60751708\n",
      "Iteration 79, loss = 0.60476914\n",
      "Iteration 80, loss = 0.60192603\n",
      "Iteration 81, loss = 0.59897171\n",
      "Iteration 82, loss = 0.59589098\n",
      "Iteration 83, loss = 0.59267305\n",
      "Iteration 84, loss = 0.58934263\n",
      "Iteration 85, loss = 0.58595587\n",
      "Iteration 86, loss = 0.58253860\n",
      "Iteration 87, loss = 0.57904796\n",
      "Iteration 88, loss = 0.57543496\n",
      "Iteration 89, loss = 0.57167275\n",
      "Iteration 90, loss = 0.56774856\n",
      "Iteration 91, loss = 0.56366004\n",
      "Iteration 92, loss = 0.55941439\n",
      "Iteration 93, loss = 0.55502301\n",
      "Iteration 94, loss = 0.55049731\n",
      "Iteration 95, loss = 0.54585205\n",
      "Iteration 96, loss = 0.54110545\n",
      "Iteration 97, loss = 0.53625080\n",
      "Iteration 98, loss = 0.53126043\n",
      "Iteration 99, loss = 0.52609253\n",
      "Iteration 100, loss = 0.52073427\n",
      "Iteration 101, loss = 0.51519947\n",
      "Iteration 102, loss = 0.50950584\n",
      "Iteration 103, loss = 0.50365353\n",
      "Iteration 104, loss = 0.49761953\n",
      "Iteration 105, loss = 0.49138192\n",
      "Iteration 106, loss = 0.48494262\n",
      "Iteration 107, loss = 0.47831971\n",
      "Iteration 108, loss = 0.47156705\n",
      "Iteration 109, loss = 0.46469639\n",
      "Iteration 110, loss = 0.45765878\n",
      "Iteration 111, loss = 0.45043152\n",
      "Iteration 112, loss = 0.44305929\n",
      "Iteration 113, loss = 0.43560341\n",
      "Iteration 114, loss = 0.42807184\n",
      "Iteration 115, loss = 0.42043467\n",
      "Iteration 116, loss = 0.41267894\n",
      "Iteration 117, loss = 0.40482853\n",
      "Iteration 118, loss = 0.39691793\n",
      "Iteration 119, loss = 0.38896550\n",
      "Iteration 120, loss = 0.38097285\n",
      "Iteration 121, loss = 0.37292857\n",
      "Iteration 122, loss = 0.36482518\n",
      "Iteration 123, loss = 0.35668141\n",
      "Iteration 124, loss = 0.34853747\n",
      "Iteration 125, loss = 0.34041937\n",
      "Iteration 126, loss = 0.33231702\n",
      "Iteration 127, loss = 0.32421878\n",
      "Iteration 128, loss = 0.31613822\n",
      "Iteration 129, loss = 0.30810328\n",
      "Iteration 130, loss = 0.30013548\n",
      "Iteration 131, loss = 0.29224747\n",
      "Iteration 132, loss = 0.28443452\n",
      "Iteration 133, loss = 0.27670294\n",
      "Iteration 134, loss = 0.26908404\n",
      "Iteration 135, loss = 0.26159961\n",
      "Iteration 136, loss = 0.25424672\n",
      "Iteration 137, loss = 0.24701743\n",
      "Iteration 138, loss = 0.23991945\n",
      "Iteration 139, loss = 0.23297455\n",
      "Iteration 140, loss = 0.22620768\n",
      "Iteration 141, loss = 0.21963794\n",
      "Iteration 142, loss = 0.21326560\n",
      "Iteration 143, loss = 0.20707838\n",
      "Iteration 144, loss = 0.20107003\n",
      "Iteration 145, loss = 0.19524150\n",
      "Iteration 146, loss = 0.18959473\n",
      "Iteration 147, loss = 0.18413130\n",
      "Iteration 148, loss = 0.17885220\n",
      "Iteration 149, loss = 0.17375767\n",
      "Iteration 150, loss = 0.16884603\n",
      "Iteration 151, loss = 0.16411585\n",
      "Iteration 152, loss = 0.15956288\n",
      "Iteration 153, loss = 0.15517598\n",
      "Iteration 154, loss = 0.15094031\n",
      "Iteration 155, loss = 0.14684733\n",
      "Iteration 156, loss = 0.14289907\n",
      "Iteration 157, loss = 0.13910705\n",
      "Iteration 158, loss = 0.13548991\n",
      "Iteration 159, loss = 0.13205163\n",
      "Iteration 160, loss = 0.12876571\n",
      "Iteration 161, loss = 0.12561611\n",
      "Iteration 162, loss = 0.12258882\n",
      "Iteration 163, loss = 0.11966608\n",
      "Iteration 164, loss = 0.11684794\n",
      "Iteration 165, loss = 0.11414212\n",
      "Iteration 166, loss = 0.11154971\n",
      "Iteration 167, loss = 0.10906385\n",
      "Iteration 168, loss = 0.10667463\n",
      "Iteration 169, loss = 0.10437396\n",
      "Iteration 170, loss = 0.10215986\n",
      "Iteration 171, loss = 0.10003906\n",
      "Iteration 172, loss = 0.09801267\n",
      "Iteration 173, loss = 0.09605944\n",
      "Iteration 174, loss = 0.09416848\n",
      "Iteration 175, loss = 0.09234446\n",
      "Iteration 176, loss = 0.09059250\n",
      "Iteration 177, loss = 0.08891212\n",
      "Iteration 178, loss = 0.08729723\n",
      "Iteration 179, loss = 0.08573989\n",
      "Iteration 180, loss = 0.08423430\n",
      "Iteration 181, loss = 0.08277749\n",
      "Iteration 182, loss = 0.08136880\n",
      "Iteration 183, loss = 0.08000732\n",
      "Iteration 184, loss = 0.07869089\n",
      "Iteration 185, loss = 0.07741671\n",
      "Iteration 186, loss = 0.07618222\n",
      "Iteration 187, loss = 0.07498651\n",
      "Iteration 188, loss = 0.07383112\n",
      "Iteration 189, loss = 0.07271752\n",
      "Iteration 190, loss = 0.07164405\n",
      "Iteration 191, loss = 0.07060638\n",
      "Iteration 192, loss = 0.06959955\n",
      "Iteration 193, loss = 0.06861991\n",
      "Iteration 194, loss = 0.06766552\n",
      "Iteration 195, loss = 0.06673573\n",
      "Iteration 196, loss = 0.06583063\n",
      "Iteration 197, loss = 0.06495061\n",
      "Iteration 198, loss = 0.06409606\n",
      "Iteration 199, loss = 0.06326703\n",
      "Iteration 200, loss = 0.06246272\n",
      "Iteration 201, loss = 0.06168140\n",
      "Iteration 202, loss = 0.06092100\n",
      "Iteration 203, loss = 0.06017974\n",
      "Iteration 204, loss = 0.05945643\n",
      "Iteration 205, loss = 0.05875038\n",
      "Iteration 206, loss = 0.05806124\n",
      "Iteration 207, loss = 0.05738874\n",
      "Iteration 208, loss = 0.05673266\n",
      "Iteration 209, loss = 0.05609265\n",
      "Iteration 210, loss = 0.05546824\n",
      "Iteration 211, loss = 0.05485879\n",
      "Iteration 212, loss = 0.05426357\n",
      "Iteration 213, loss = 0.05368182\n",
      "Iteration 214, loss = 0.05311288\n",
      "Iteration 215, loss = 0.05255617\n",
      "Iteration 216, loss = 0.05201126\n",
      "Iteration 217, loss = 0.05147782\n",
      "Iteration 218, loss = 0.05095558\n",
      "Iteration 219, loss = 0.05044427\n",
      "Iteration 220, loss = 0.04994358\n",
      "Iteration 221, loss = 0.04945318\n",
      "Iteration 222, loss = 0.04897268\n",
      "Iteration 223, loss = 0.04850170\n",
      "Iteration 224, loss = 0.04803988\n",
      "Iteration 225, loss = 0.04758690\n",
      "Iteration 226, loss = 0.04714245\n",
      "Iteration 227, loss = 0.04670627\n",
      "Iteration 228, loss = 0.04627813\n",
      "Iteration 229, loss = 0.04585780\n",
      "Iteration 230, loss = 0.04544507\n",
      "Iteration 231, loss = 0.04503972\n",
      "Iteration 232, loss = 0.04464154\n",
      "Iteration 233, loss = 0.04425032\n",
      "Iteration 234, loss = 0.04386587\n",
      "Iteration 235, loss = 0.04348797\n",
      "Iteration 236, loss = 0.04311644\n",
      "Iteration 237, loss = 0.04275109\n",
      "Iteration 238, loss = 0.04239173\n",
      "Iteration 239, loss = 0.04203818\n",
      "Iteration 240, loss = 0.04169030\n",
      "Iteration 241, loss = 0.04134790\n",
      "Iteration 242, loss = 0.04101084\n",
      "Iteration 243, loss = 0.04067899\n",
      "Iteration 244, loss = 0.04035219\n",
      "Iteration 245, loss = 0.04003032\n",
      "Iteration 246, loss = 0.03971329\n",
      "Iteration 247, loss = 0.03940100\n",
      "Iteration 248, loss = 0.03909341\n",
      "Iteration 249, loss = 0.03879049\n",
      "Iteration 250, loss = 0.03849225\n",
      "Iteration 251, loss = 0.03819871\n",
      "Iteration 252, loss = 0.03790981\n",
      "Iteration 253, loss = 0.03762544\n",
      "Iteration 254, loss = 0.03734540\n",
      "Iteration 255, loss = 0.03706940\n",
      "Iteration 256, loss = 0.03679716\n",
      "Iteration 257, loss = 0.03652849\n",
      "Iteration 258, loss = 0.03626326\n",
      "Iteration 259, loss = 0.03600145\n",
      "Iteration 260, loss = 0.03574308\n",
      "Iteration 261, loss = 0.03548819\n",
      "Iteration 262, loss = 0.03523681\n",
      "Iteration 263, loss = 0.03498891\n",
      "Iteration 264, loss = 0.03474443\n",
      "Iteration 265, loss = 0.03450329\n",
      "Iteration 266, loss = 0.03426539\n",
      "Iteration 267, loss = 0.03403063\n",
      "Iteration 268, loss = 0.03379890\n",
      "Iteration 269, loss = 0.03357014\n",
      "Iteration 270, loss = 0.03334426\n",
      "Iteration 271, loss = 0.03312120\n",
      "Iteration 272, loss = 0.03290092\n",
      "Iteration 273, loss = 0.03268336\n",
      "Iteration 274, loss = 0.03246849\n",
      "Iteration 275, loss = 0.03225626\n",
      "Iteration 276, loss = 0.03204664\n",
      "Iteration 277, loss = 0.03183960\n",
      "Iteration 278, loss = 0.03163507\n",
      "Iteration 279, loss = 0.03143302\n",
      "Iteration 280, loss = 0.03123340\n",
      "Iteration 281, loss = 0.03103615\n",
      "Iteration 282, loss = 0.03084123\n",
      "Iteration 283, loss = 0.03064859\n",
      "Iteration 284, loss = 0.03045819\n",
      "Iteration 285, loss = 0.03026998\n",
      "Iteration 286, loss = 0.03008392\n",
      "Iteration 287, loss = 0.02989998\n",
      "Iteration 288, loss = 0.02971812\n",
      "Iteration 289, loss = 0.02953830\n",
      "Iteration 290, loss = 0.02936051\n",
      "Iteration 291, loss = 0.02918469\n",
      "Iteration 292, loss = 0.02901082\n",
      "Iteration 293, loss = 0.02883887\n",
      "Iteration 294, loss = 0.02866880\n",
      "Iteration 295, loss = 0.02850059\n",
      "Iteration 296, loss = 0.02833420\n",
      "Iteration 297, loss = 0.02816959\n",
      "Iteration 298, loss = 0.02800675\n",
      "Iteration 299, loss = 0.02784563\n",
      "Iteration 300, loss = 0.02768622\n",
      "Iteration 301, loss = 0.02752848\n",
      "Iteration 302, loss = 0.02737239\n",
      "Iteration 303, loss = 0.02721792\n",
      "Iteration 304, loss = 0.02706505\n",
      "Iteration 305, loss = 0.02691375\n",
      "Iteration 306, loss = 0.02676400\n",
      "Iteration 307, loss = 0.02661578\n",
      "Iteration 308, loss = 0.02646905\n",
      "Iteration 309, loss = 0.02632381\n",
      "Iteration 310, loss = 0.02618002\n",
      "Iteration 311, loss = 0.02603766\n",
      "Iteration 312, loss = 0.02589672\n",
      "Iteration 313, loss = 0.02575717\n",
      "Iteration 314, loss = 0.02561899\n",
      "Iteration 315, loss = 0.02548216\n",
      "Iteration 316, loss = 0.02534667\n",
      "Iteration 317, loss = 0.02521248\n",
      "Iteration 318, loss = 0.02507958\n",
      "Iteration 319, loss = 0.02494796\n",
      "Iteration 320, loss = 0.02481760\n",
      "Iteration 321, loss = 0.02468847\n",
      "Iteration 322, loss = 0.02456056\n",
      "Iteration 323, loss = 0.02443385\n",
      "Iteration 324, loss = 0.02430834\n",
      "Iteration 325, loss = 0.02418399\n",
      "Iteration 326, loss = 0.02406079\n",
      "Iteration 327, loss = 0.02393873\n",
      "Iteration 328, loss = 0.02381779\n",
      "Iteration 329, loss = 0.02369796\n",
      "Iteration 330, loss = 0.02357922\n",
      "Iteration 331, loss = 0.02346156\n",
      "Iteration 332, loss = 0.02334496\n",
      "Iteration 333, loss = 0.02322941\n",
      "Iteration 334, loss = 0.02311489\n",
      "Iteration 335, loss = 0.02300139\n",
      "Iteration 336, loss = 0.02288890\n",
      "Iteration 337, loss = 0.02277740\n",
      "Iteration 338, loss = 0.02266688\n",
      "Iteration 339, loss = 0.02255733\n",
      "Iteration 340, loss = 0.02244873\n",
      "Iteration 341, loss = 0.02234108\n",
      "Iteration 342, loss = 0.02223436\n",
      "Iteration 343, loss = 0.02212856\n",
      "Iteration 344, loss = 0.02202366\n",
      "Iteration 345, loss = 0.02191966\n",
      "Iteration 346, loss = 0.02181654\n",
      "Iteration 347, loss = 0.02171430\n",
      "Iteration 348, loss = 0.02161292\n",
      "Iteration 349, loss = 0.02151238\n",
      "Iteration 350, loss = 0.02141270\n",
      "Iteration 351, loss = 0.02131384\n",
      "Iteration 352, loss = 0.02121580\n",
      "Iteration 353, loss = 0.02111857\n",
      "Iteration 354, loss = 0.02102214\n",
      "Iteration 355, loss = 0.02092650\n",
      "Iteration 356, loss = 0.02083165\n",
      "Iteration 357, loss = 0.02073756\n",
      "Iteration 358, loss = 0.02064424\n",
      "Iteration 359, loss = 0.02055167\n",
      "Iteration 360, loss = 0.02045984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.8222222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.81      0.86      0.83        92\n",
      "      Normal       0.84      0.78      0.81        88\n",
      "\n",
      "    accuracy                           0.82       180\n",
      "   macro avg       0.82      0.82      0.82       180\n",
      "weighted avg       0.82      0.82      0.82       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.9, random_state=15)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,50,25,), activation='logistic', max_iter=500, random_state=42, verbose=True)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "source": [
    "Assim que decidimos aumentar tão consideravelmente o set_train, pensamos que teríamos uma melhora no precision, recall e accuracy em todos os três casos, porém para os dois primeiros dataSets (o dataSets original e o dataSets que excluímos as 6 primeiras variavéis), tivemos uma piora muito significativa no recall para os casos normais e na accuracy para ambos os conjuntos de dados, já para o último dataSet (o que excluímos 110 casos anormais), obtivemos um excelente resultado para ambos os casos e uma accuracy de 96%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "3ef7935b13617fabe734c4a9dee0ded502b7e7ddad9ea0181f384dd40d88ac4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}